{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMzuLDwNTKisd8Z6gwKn9Nm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Install only what's needed\n","!pip install torch-geometric -q\n","!pip install rdkit\n","import torch\n","from torch.utils.data import IterableDataset\n","from torch_geometric.datasets import MoleculeNet\n","import pandas as pd\n","import itertools\n","import sys\n","from google.colab import drive\n","import os\n","\n","gdrive_path='/content/gdrive/MyDrive/Project_HIV'\n","\n","# This will mount your google drive under 'MyDrive'\n","drive.mount('/content/gdrive', force_remount=True)\n","# In order to access the files in this notebook we have to navigate to the correct folder\n","os.chdir(gdrive_path)\n","# Check manually if all files are present\n","print(sorted(os.listdir()))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HBpIix3WkoOg","executionInfo":{"status":"ok","timestamp":1756379457566,"user_tz":-120,"elapsed":24644,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"outputId":"9dee7178-907c-42a6-e521-16c0ae49f1fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: rdkit in /usr/local/lib/python3.12/dist-packages (2025.3.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rdkit) (2.0.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rdkit) (11.3.0)\n","Mounted at /content/gdrive\n","['.git', 'HIV.ipynb', 'README.md', 'code', 'datasets', 'graph_pe_demo.py']\n"]}]},{"cell_type":"code","source":["# Add code directory to path\n","from torch_geometric.datasets import MoleculeNet\n","# sys.path.insert(0, os.path.join(os.getcwd(), 'code'))\n","\n","# # Import the Colab-friendly version\n","# from data.dataset_colab import PartialHIVDataset\n","\n","class PartialHIVDataset(IterableDataset):\n","    \"\"\"\n","    IterableDataset that loads only a small portion of the HIV dataset.\n","    \"\"\"\n","\n","    def __init__(self, root='/tmp/HIV', max_samples=10):\n","        \"\"\"\n","        Args:\n","            root: Root directory for dataset storage\n","            max_samples: Maximum number of samples to load\n","        \"\"\"\n","        self.root = root\n","        self.max_samples = max_samples\n","        self._dataset = None\n","\n","    def _lazy_load_dataset(self):\n","        \"\"\"Lazy load the dataset only when iteration begins\"\"\"\n","        if self._dataset is None:\n","            print(f\"Initializing HIV dataset (will only load {self.max_samples} samples)...\")\n","            # Import here to ensure it's available\n","\n","            self._dataset = MoleculeNet(root=self.root, name='HIV')\n","            print(f\"Dataset ready! Total size: {len(self._dataset)} molecules\")\n","            print(f\"But we'll only load {self.max_samples} of them.\\n\")\n","\n","    def parse_molecules(self):\n","        \"\"\"\n","        Parse molecules from the dataset, stopping after max_samples.\n","        \"\"\"\n","        self._lazy_load_dataset()\n","\n","        for i in range(min(self.max_samples, len(self._dataset))):\n","            data = self._dataset[i]\n","\n","            # Extract molecule information\n","            mol_info = {\n","                'source': f\"molecule_{i}\",  # Similar to your CustomIterableDataset\n","                'target': data.y.item(),    # HIV activity label\n","                'num_atoms': data.num_nodes,\n","                'num_bonds': data.num_edges // 2,  # Undirected edges\n","                'smiles': getattr(data, 'smiles', 'N/A')\n","            }\n","\n","            yield mol_info\n","\n","    def __iter__(self):\n","        \"\"\"Iterator with worker support\"\"\"\n","        iterator = self.parse_molecules()\n","        worker_info = torch.utils.data.get_worker_info()\n","\n","        if worker_info is not None:\n","            worker_total_num = worker_info.num_workers\n","            worker_id = worker_info.id\n","            return itertools.islice(iterator, worker_id, None, worker_total_num)\n","\n","        return iterator\n","\n","\n","# Your code\n","batch_list = []\n","batch_size = 40\n","\n","# Create dataset\n","dataset = PartialHIVDataset(max_samples=batch_size)\n","iterator = iter(dataset)\n","full_batch_data = list(dataset)\n","\n","df = pd.DataFrame(full_batch_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uCZHJO55qvlC","executionInfo":{"status":"ok","timestamp":1756392290037,"user_tz":-120,"elapsed":1725,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"outputId":"f0e29586-34ac-4078-ff58-e65bd0b8d55d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing HIV dataset (will only load 40 samples)...\n","Dataset ready! Total size: 41120 molecules\n","But we'll only load 40 of them.\n","\n"]}]},{"cell_type":"code","source":["import codecs\n","\n","import numpy as np\n","import torch.nn as nn\n","from transformers import AutoTokenizer\n","\n","smiles_list = []\n","batch_size = 40\n","\n","\n","# Iterate through the dataset and fill it with the SMILES strings\n","for _ in range(batch_size):\n","  mol_info = next(iterator)\n","  smiles_list.append(mol_info['smiles'])\n","\n","\n","\n","# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n","\n","# First, tokenize without padding to find the actual max length\n","tokenized_lengths = []\n","for smiles in smiles_list:\n","    tokens = tokenizer(smiles, padding=False, truncation=False)\n","    tokenized_lengths.append(len(tokens['input_ids']))\n","\n","# Determine max_length from your data\n","max_length = max(tokenized_lengths)\n","print(f\"Maximum sequence length in data: {max_length}\")\n","\n","\n","# Now tokenize with the data-driven max_length\n","encoded = tokenizer(\n","    smiles_list,\n","    padding='max_length',\n","    truncation=True,\n","    max_length=max_length,\n","    return_tensors='pt'\n",")\n","\n","# Create embedding layer\n","embedding = nn.Embedding(\n","    num_embeddings=len(tokenizer),\n","    embedding_dim=512,\n","    padding_idx=tokenizer.pad_token_id\n",")\n","\n","# Get embeddings directly\n","embeddings = embedding(encoded['input_ids'])\n","print(f\"Embeddings shape: {embeddings.shape}\")  # (batch_size, max_length, 512)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oRsTzvL0l5bx","executionInfo":{"status":"ok","timestamp":1756392295893,"user_tz":-120,"elapsed":227,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"outputId":"bf02f9d2-4012-4c86-fee5-94730c7708d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum sequence length in data: 61\n","Embeddings shape: torch.Size([40, 61, 512])\n"]}]},{"cell_type":"code","source":["from scipy.sparse.linalg import eigsh\n","from rdkit import Chem\n","def smiles_to_adjacency_matrix(smiles):\n","    \"\"\"Convert SMILES to adjacency matrix.\"\"\"\n","    mol = Chem.MolFromSmiles(smiles)\n","    if mol is None:\n","        return None\n","\n","    n_atoms = mol.GetNumAtoms()\n","    adj_matrix = np.zeros((n_atoms, n_atoms))\n","\n","    for bond in mol.GetBonds():\n","        i = bond.GetBeginAtomIdx()\n","        j = bond.GetEndAtomIdx()\n","        # Undirected graph\n","        adj_matrix[i, j] = 1\n","        adj_matrix[j, i] = 1\n","\n","    return adj_matrix\n","\n","def compute_graph_positional_encoding(adj_matrix, k=30):\n","    \"\"\"\n","    Compute graph positional encodings from eigenvectors of the\n","    symmetrically normalized graph Laplacian.\n","    \"\"\"\n","    n = adj_matrix.shape[0]\n","\n","    # Compute degree matrix\n","    degree = np.sum(adj_matrix, axis=1)\n","    degree[degree == 0] = 1\n","\n","    # D^(-1/2)\n","    d_inv_sqrt = np.diag(1.0 / np.sqrt(degree))\n","\n","    # Symmetrically normalized Laplacian\n","    identity = np.eye(n)\n","    normalized_adj = d_inv_sqrt @ adj_matrix @ d_inv_sqrt\n","    laplacian = identity - normalized_adj\n","\n","    # Compute eigenvectors\n","    if n < k:\n","        # If graph has fewer nodes than k, pad with zeros\n","        eigenvalues, eigenvectors = np.linalg.eigh(laplacian)\n","        # Pad eigenvectors to have k columns\n","        padded_eigenvectors = np.zeros((n, k))\n","        padded_eigenvectors[:, :n] = eigenvectors\n","        return padded_eigenvectors\n","    else:\n","        eigenvalues, eigenvectors = eigsh(laplacian, k=k, which='SM')\n","        return eigenvectors"],"metadata":{"id":"5VXPZypgg5_P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Process all SMILES and compute their graph PEs\n","graph_pes_list = []\n","max_nodes = 0\n","\n","for smiles in smiles_list:\n","    adj_matrix = smiles_to_adjacency_matrix(smiles)\n","    if adj_matrix is not None:\n","        pe = compute_graph_positional_encoding(adj_matrix, k=30)\n","        graph_pes_list.append(pe)\n","        max_nodes = max(max_nodes, pe.shape[0])\n","    else:\n","        graph_pes_list.append(None)\n","\n","print(f\"Maximum number of atoms in dataset: {max_nodes}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QadoBoVYgrrz","executionInfo":{"status":"ok","timestamp":1756390475241,"user_tz":-120,"elapsed":81,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"outputId":"90ac0bb2-33f7-422b-be59-5977dcbd24cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum number of atoms in dataset: 42\n"]}]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","from rdkit import Chem\n","import re\n","\n","def generate_random_orthonormal_pe(n_vectors, dim=30):\n","    \"\"\"Generate n_vectors random orthonormal vectors of dimension dim.\"\"\"\n","    if n_vectors == 0:\n","        return np.zeros((0, dim))\n","\n","    # Generate random matrix\n","    random_matrix = np.random.randn(dim, n_vectors)\n","\n","    # Use QR decomposition to get orthonormal vectors\n","    Q, _ = np.linalg.qr(random_matrix)\n","\n","    # Return first n_vectors columns (transposed to have shape (n_vectors, dim))\n","    return Q[:, :n_vectors].T\n","\n","def parse_token_components(token):\n","    \"\"\"\n","    Parse a token to identify atoms and characters.\n","    Returns: (atom_indices, n_characters)\n","    \"\"\"\n","    # Common atom patterns in SMILES\n","    atom_pattern = r'(Cl|Br|Si|Mg|Ca|Fe|Al|Na|Li|[BCNOFPSKHIV])'\n","\n","    # Find all atoms in the token\n","    atoms = re.findall(atom_pattern, token)\n","\n","    # Count non-atom characters\n","    # Remove atoms from token to count remaining characters\n","    remaining = token\n","    for atom in atoms:\n","        remaining = remaining.replace(atom, '', 1)\n","\n","    # Count actual characters (digits, +, -, =, #, etc.)\n","    n_characters = len([c for c in remaining])\n","\n","    return atoms, n_characters\n","\n","def align_graph_pe_to_tokens(smiles, graph_pe, tokenizer, max_length,random_seed=None):\n","    \"\"\"\n","    Align graph PE to tokens, handling pure characters, atoms, and mixed tokens.\n","\n","    Args:\n","        smiles: SMILES string\n","        graph_pe: Graph positional encoding for atoms (n_atoms, embedding_dim)\n","        tokenizer: Tokenizer object\n","        max_length: Maximum sequence length\n","        random_seed: Random seed for reproducible random PEs\n","    \"\"\"\n","    if random_seed is not None:\n","        np.random.seed(random_seed)\n","    # Determine embedding dimension\n","    if graph_pe is not None:\n","        embedding_dim = graph_pe.shape[1]\n","    elif embedding_dim is None:\n","        embedding_dim = 30  # Default fallback\n","\n","    mol = Chem.MolFromSmiles(smiles)\n","    if mol is None or graph_pe is None:\n","        return torch.zeros(max_length, embedding_dim)\n","\n","    # Get tokens\n","    encoding = tokenizer(smiles, padding='max_length', max_length=max_length)\n","    tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n","\n","\n","    # Initialize token PE\n","    token_pe = torch.zeros(max_length, embedding_dim)\n","\n","    # Build atom mapping from SMILES\n","    atom_symbols = [atom.GetSymbol() for atom in mol.GetAtoms()]\n","    atom_count = {symbol: 0 for symbol in set(atom_symbols)}\n","    atom_to_idx = {}\n","\n","    for idx, symbol in enumerate(atom_symbols):\n","        atom_to_idx[(symbol, atom_count[symbol])] = idx\n","        atom_count[symbol] += 1\n","\n","    # Reset atom count for tracking\n","    current_atom_count = {symbol: 0 for symbol in atom_count}\n","\n","    # Process each token\n","    for i, token in enumerate(tokens):\n","        if token in ['<s>', '</s>', '<pad>']:\n","            continue\n","\n","        # Parse token components\n","        atoms_in_token, n_characters = parse_token_components(token)\n","\n","        if len(atoms_in_token) == 0 and n_characters > 0:\n","            # Pure character token - use random orthonormal PE\n","            random_pes = generate_random_orthonormal_pe(n_characters, embedding_dim)\n","            token_pe[i] = torch.tensor(random_pes.sum(axis=0))\n","\n","        elif len(atoms_in_token) > 0 and n_characters == 0:\n","            # Pure atom token(s) - use graph PE\n","            atom_pes = []\n","            for atom_symbol in atoms_in_token:\n","                if atom_symbol in current_atom_count:\n","                    atom_key = (atom_symbol, current_atom_count[atom_symbol])\n","                    if atom_key in atom_to_idx:\n","                        atom_idx = atom_to_idx[atom_key]\n","                        atom_pes.append(graph_pe[atom_idx])\n","                        current_atom_count[atom_symbol] += 1\n","\n","            if atom_pes:\n","                # Average the PEs of all atoms in this token\n","                token_pe[i] = torch.tensor(np.sum(atom_pes, axis=0))\n","\n","        elif len(atoms_in_token) > 0 and n_characters > 0:\n","            # Mixed token - combine atom PE and character PE\n","            # Get atom PEs\n","            atom_pes = []\n","            for atom_symbol in atoms_in_token:\n","                if atom_symbol in current_atom_count:\n","                    atom_key = (atom_symbol, current_atom_count[atom_symbol])\n","                    if atom_key in atom_to_idx:\n","                        atom_idx = atom_to_idx[atom_key]\n","                        atom_pes.append(graph_pe[atom_idx])\n","                        current_atom_count[atom_symbol] += 1\n","\n","            # Get character PEs\n","            random_pes = generate_random_orthonormal_pe(n_characters, embedding_dim)\n","\n","            # Combine: sum of atoms + sum of characters\n","            combined_pe = np.zeros(embedding_dim)\n","            if atom_pes:\n","                combined_pe += np.sum(atom_pes, axis=0)\n","            combined_pe += random_pes.sum(axis=0)\n","\n","            token_pe[i] = torch.tensor(combined_pe)\n","\n","    return token_pe\n","\n"],"metadata":{"id":"_Ndd99hrDjdB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class EmbeddingWithGraphPE(nn.Module):\n","    def __init__(self, embed_dim=512, pe_dim=30):\n","        super().__init__()\n","        # One-layer projection with GeLU from graph PE to embedding dimension\n","        # Using standard Laplacian, so pe_dim is just k\n","        self.pe_projection = nn.Sequential(\n","            nn.Linear(pe_dim, embed_dim),\n","            nn.GELU()\n","        )\n","\n","    def forward(self, embeddings, graph_pes):\n","        # token_pes shape: [batch_size, seq_len, pe_dim]\n","        # embeddings shape: [batch_size, seq_len, embed_dim]\n","\n","        # Project token PEs to embedding dimension with GeLU\n","        projected_pes = self.pe_projection(graph_pes)\n","\n","        # Add to token embeddings\n","        enhanced_embeddings = embeddings + projected_pes\n","\n","        return enhanced_embeddings\n","\n","# Usage\n","model = EmbeddingWithGraphPE(embed_dim=512, pe_dim=30)\n","# graph_pes_batch shape: [40, 61, 30] (30 eigenvectors from standard Laplacian)\n","# embeddings shape: [40, 61, 512]\n","enhanced_embeddings = model(embeddings, graph_pes_batch)\n","# Output shape: [40, 61, 512]"],"metadata":{"id":"i0zuJ4bRvlJT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import pandas as pd\n","from collections import Counter\n","import random\n","\n","def check_class_balance(data, name=\"Dataset\"):\n","    \"\"\"\n","    Check the balance of classes (0's and 1's) in the dataset.\n","\n","    Args:\n","        data: List of dictionaries or DataFrame\n","        name: Name of the dataset for display\n","\n","    Returns:\n","        Dictionary with class counts and percentages\n","    \"\"\"\n","    if isinstance(data, pd.DataFrame):\n","        targets = data['target'].values\n","    else:\n","        # For list of dictionaries in LitGPT format\n","        targets = [int(item['output']) for item in data]\n","\n","    class_counts = Counter(targets)\n","    total = len(targets)\n","\n","    print(f\"\\n{name} Class Balance:\")\n","    print(f\"Total samples: {total}\")\n","    for class_label in sorted(class_counts.keys()):\n","        count = class_counts[class_label]\n","        percentage = (count / total) * 100\n","        print(f\"Class {class_label}: {count} samples ({percentage:.2f}%)\")\n","\n","    return {\n","        'total': total,\n","        'class_0': class_counts[0],\n","        'class_1': class_counts[1],\n","        'ratio_0': class_counts[0] / total,\n","        'ratio_1': class_counts[1] / total\n","    }\n","\n","\n","def stratified_train_val_split(data, train_ratio=0.8, random_seed=42):\n","    \"\"\"\n","    Create a stratified train/validation split that maintains class balance.\n","\n","    Args:\n","        data: List of dictionaries in LitGPT format\n","        train_ratio: Ratio of training data\n","        random_seed: Random seed for reproducibility\n","\n","    Returns:\n","        train_data, val_data\n","    \"\"\"\n","    random.seed(random_seed)\n","\n","    # Separate data by class\n","    class_0_data = [item for item in data if item['output'] == '0']\n","    class_1_data = [item for item in data if item['output'] == '1']\n","\n","    # Shuffle each class separately\n","    random.shuffle(class_0_data)\n","    random.shuffle(class_1_data)\n","\n","    # Calculate split indices for each class\n","    split_idx_0 = int(len(class_0_data) * train_ratio)\n","    split_idx_1 = int(len(class_1_data) * train_ratio)\n","\n","    # Split each class\n","    train_class_0 = class_0_data[:split_idx_0]\n","    val_class_0 = class_0_data[split_idx_0:]\n","\n","    train_class_1 = class_1_data[:split_idx_1]\n","    val_class_1 = class_1_data[split_idx_1:]\n","\n","    # Combine and shuffle\n","    train_data = train_class_0 + train_class_1\n","    val_data = val_class_0 + val_class_1\n","\n","    random.shuffle(train_data)\n","    random.shuffle(val_data)\n","\n","    return train_data, val_data\n","\n","\n","def convert_to_litgpt_format(df, task_type=\"classification\"):\n","    \"\"\"\n","    Convert the HIV dataset to LitGPT JSONL format with instruction/input/output fields.\n","\n","    Args:\n","        df: DataFrame containing the HIV dataset\n","        task_type: Type of task - \"classification\", \"property_prediction\", or \"generation\"\n","\n","    Returns:\n","        List of dictionaries in LitGPT format\n","    \"\"\"\n","    litgpt_data = []\n","\n","    for idx, row in df.iterrows():\n","        # Create different instruction templates based on task type\n","        if task_type == \"classification\":\n","            # Binary classification for HIV activity\n","            instruction = \"Classify the following molecule based on its HIV activity. Respond with '1' if the molecule shows HIV activity, or '0' if it does not.\"\n","            input_text = f\"SMILES: {row['smiles']}\"\n","            output_text = str(int(row['target']))\n","\n","        else:\n","            raise ValueError(f\"Unknown task_type: {task_type}\")\n","\n","        # Create the LitGPT format entry\n","        litgpt_entry = {\n","            \"instruction\": instruction,\n","            \"input\": input_text,\n","            \"output\": output_text\n","        }\n","\n","        litgpt_data.append(litgpt_entry)\n","\n","    return litgpt_data\n","\n","\n","def save_to_jsonl(data, filename):\n","    \"\"\"\n","    Save data to JSONL format (one JSON object per line).\n","\n","    Args:\n","        data: List of dictionaries\n","        filename: Output filename\n","    \"\"\"\n","    with open(filename, 'w') as f:\n","        for item in data:\n","            json_line = json.dumps(item)\n","            f.write(json_line + '\\n')\n","    print(f\"Saved {len(data)} entries to {filename}\")\n","\n","\n","# Main execution\n","if __name__ == \"__main__\":\n","    # Note: You'll need to import or define PartialHIVDataset and import pandas as pd\n","    # from your_module import PartialHIVDataset\n","    # import pandas as pd\n","\n","    # Create dataset\n","    batch_size = 1000  # Adjust based on your needs\n","    dataset = PartialHIVDataset(max_samples=batch_size)\n","\n","    # Convert to DataFrame\n","    full_batch_data = list(dataset)\n","    df = pd.DataFrame(full_batch_data)\n","\n","    print(f\"Loaded {len(df)} molecules\")\n","    print(\"\\nSample data:\")\n","    print(df.head())\n","\n","    # Check balance in the original DataFrame\n","    original_balance = check_class_balance(df, \"Original Dataset\")\n","\n","    # Convert to LitGPT format for different task types\n","    classification_data = convert_to_litgpt_format(df, task_type=\"classification\")\n","\n","    # Check balance in the converted data\n","    converted_balance = check_class_balance(classification_data, \"Converted Dataset\")\n","\n","    # Create stratified train/validation split\n","    train_data, val_data = stratified_train_val_split(classification_data, train_ratio=0.8)\n","\n","    # Check balance in train and validation sets\n","    train_balance = check_class_balance(train_data, \"Training Set\")\n","    val_balance = check_class_balance(val_data, \"Validation Set\")\n","\n","    # Compare ratios\n","    print(\"\\n\" + \"=\"*50)\n","    print(\"Class Distribution Comparison:\")\n","    print(\"=\"*50)\n","    print(f\"Original - Class 0: {original_balance['ratio_0']:.4f}, Class 1: {original_balance['ratio_1']:.4f}\")\n","    print(f\"Train    - Class 0: {train_balance['ratio_0']:.4f}, Class 1: {train_balance['ratio_1']:.4f}\")\n","    print(f\"Val      - Class 0: {val_balance['ratio_0']:.4f}, Class 1: {val_balance['ratio_1']:.4f}\")\n","\n","    # Calculate difference from original distribution\n","    train_diff_0 = abs(train_balance['ratio_0'] - original_balance['ratio_0'])\n","    train_diff_1 = abs(train_balance['ratio_1'] - original_balance['ratio_1'])\n","    val_diff_0 = abs(val_balance['ratio_0'] - original_balance['ratio_0'])\n","    val_diff_1 = abs(val_balance['ratio_1'] - original_balance['ratio_1'])\n","\n","    print(f\"\\nDifference from original distribution:\")\n","    print(f\"Train - Class 0: {train_diff_0:.4f}, Class 1: {train_diff_1:.4f}\")\n","    print(f\"Val   - Class 0: {val_diff_0:.4f}, Class 1: {val_diff_1:.4f}\")\n","\n","    # Save to JSONL files\n","    save_to_jsonl(train_data, \"hiv_train.jsonl\")\n","    save_to_jsonl(val_data, \"hiv_val.jsonl\")\n","\n","    # Print sample entries to verify format\n","    print(\"\\nSample LitGPT format entries:\")\n","    for i, entry in enumerate(classification_data[:3]):\n","        print(f\"\\nEntry {i+1}:\")\n","        print(f\"Instruction: {entry['instruction']}\")\n","        print(f\"Input: {entry['input']}\")\n","        print(f\"Output: {entry['output']}\")\n","\n","    # Verify JSONL format by reading back\n","    print(\"\\nVerifying JSONL format...\")\n","    with open(\"hiv_train.jsonl\", 'r') as f:\n","        first_line = f.readline()\n","        loaded_entry = json.loads(first_line)\n","        print(\"First entry from JSONL file:\")\n","        print(json.dumps(loaded_entry, indent=2))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lvrvcjkk1b29","executionInfo":{"status":"ok","timestamp":1756402454294,"user_tz":-120,"elapsed":1049,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"outputId":"1b5ee8bb-b958-4514-e660-ad4a60aad7c7"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing HIV dataset (will only load 1000 samples)...\n","Dataset ready! Total size: 41120 molecules\n","But we'll only load 1000 of them.\n","\n","Loaded 1000 molecules\n","\n","Sample data:\n","       source  target  num_atoms  num_bonds  \\\n","0  molecule_0     0.0         19         20   \n","1  molecule_1     0.0         39         44   \n","2  molecule_2     0.0         21         24   \n","3  molecule_3     0.0         24         25   \n","4  molecule_4     0.0         10          9   \n","\n","                                              smiles  \n","0  CCC1=[O+][Cu-3]2([O+]=C(CC)C1)[O+]=C(CC)CC(CC)...  \n","1  C(=Cc1ccccc1)C1=[O+][Cu-3]2([O+]=C(C=Cc3ccccc3...  \n","2                   CC(=O)N1c2ccccc2Sc2c1ccc1ccccc21  \n","3    Nc1ccc(C=Cc2ccc(N)cc2S(=O)(=O)O)c(S(=O)(=O)O)c1  \n","4                             O=S(=O)(O)CCS(=O)(=O)O  \n","\n","Original Dataset Class Balance:\n","Total samples: 1000\n","Class 0.0: 971 samples (97.10%)\n","Class 1.0: 29 samples (2.90%)\n","\n","Converted Dataset Class Balance:\n","Total samples: 1000\n","Class 0: 971 samples (97.10%)\n","Class 1: 29 samples (2.90%)\n","\n","Training Set Class Balance:\n","Total samples: 799\n","Class 0: 776 samples (97.12%)\n","Class 1: 23 samples (2.88%)\n","\n","Validation Set Class Balance:\n","Total samples: 201\n","Class 0: 195 samples (97.01%)\n","Class 1: 6 samples (2.99%)\n","\n","==================================================\n","Class Distribution Comparison:\n","==================================================\n","Original - Class 0: 0.9710, Class 1: 0.0290\n","Train    - Class 0: 0.9712, Class 1: 0.0288\n","Val      - Class 0: 0.9701, Class 1: 0.0299\n","\n","Difference from original distribution:\n","Train - Class 0: 0.0002, Class 1: 0.0002\n","Val   - Class 0: 0.0009, Class 1: 0.0009\n","Saved 799 entries to hiv_train.jsonl\n","Saved 201 entries to hiv_val.jsonl\n","\n","Sample LitGPT format entries:\n","\n","Entry 1:\n","Instruction: Classify the following molecule based on its HIV activity. Respond with '1' if the molecule shows HIV activity, or '0' if it does not.\n","Input: SMILES: CCC1=[O+][Cu-3]2([O+]=C(CC)C1)[O+]=C(CC)CC(CC)=[O+]2\n","Output: 0\n","\n","Entry 2:\n","Instruction: Classify the following molecule based on its HIV activity. Respond with '1' if the molecule shows HIV activity, or '0' if it does not.\n","Input: SMILES: C(=Cc1ccccc1)C1=[O+][Cu-3]2([O+]=C(C=Cc3ccccc3)CC(c3ccccc3)=[O+]2)[O+]=C(c2ccccc2)C1\n","Output: 0\n","\n","Entry 3:\n","Instruction: Classify the following molecule based on its HIV activity. Respond with '1' if the molecule shows HIV activity, or '0' if it does not.\n","Input: SMILES: CC(=O)N1c2ccccc2Sc2c1ccc1ccccc21\n","Output: 0\n","\n","Verifying JSONL format...\n","First entry from JSONL file:\n","{\n","  \"instruction\": \"Classify the following molecule based on its HIV activity. Respond with '1' if the molecule shows HIV activity, or '0' if it does not.\",\n","  \"input\": \"SMILES: O=C(O)c1cc(N=Nc2cccc([N+](=O)[O-])c2)ccc1O\",\n","  \"output\": \"0\"\n","}\n"]}]}]}