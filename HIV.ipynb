{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm","authorship_tag":"ABX9TyO5dmDc49jFI7zJlZexdm6J"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ffe3d442836d4fee9de00a5e5dcc005b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6ce085672692487281f5691231a6a143","IPY_MODEL_a2ddfa5c82d34d10b4085ce9757a29c9","IPY_MODEL_9e59931717a44b529e40b6cc5e259827"],"layout":"IPY_MODEL_fa540e8387404444a4b4488b1ef1bd8c"}},"6ce085672692487281f5691231a6a143":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9880b041901c4f6bbb763d7f5c501f50","placeholder":"​","style":"IPY_MODEL_69be98f307a9417b910df9598f133fce","value":"tokenizer_config.json: 100%"}},"a2ddfa5c82d34d10b4085ce9757a29c9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f5e4a227fe54834b75742ca7ac430bf","max":166,"min":0,"orientation":"horizontal","style":"IPY_MODEL_466e57d216dc4895956bf9acab08b153","value":166}},"9e59931717a44b529e40b6cc5e259827":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_21fc82595fc9462db8485dbd5f1347f9","placeholder":"​","style":"IPY_MODEL_833edb047093437491d8b6c4f57cf393","value":" 166/166 [00:00&lt;00:00, 16.8kB/s]"}},"fa540e8387404444a4b4488b1ef1bd8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9880b041901c4f6bbb763d7f5c501f50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69be98f307a9417b910df9598f133fce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f5e4a227fe54834b75742ca7ac430bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"466e57d216dc4895956bf9acab08b153":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"21fc82595fc9462db8485dbd5f1347f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"833edb047093437491d8b6c4f57cf393":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93aa535ba74545c1a7f238fc0a0645ab":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d925e19eb72a49efa28d86d5696e7484","IPY_MODEL_7c631fb62ea0482d81df151c2d66a73b","IPY_MODEL_ceea6a75ed854e568920957bebd4e0d5"],"layout":"IPY_MODEL_b0ae5764abaf4f3ebf93d5cdd1d8833d"}},"d925e19eb72a49efa28d86d5696e7484":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4060e76ac4b9455da2e9ff01ccbe9c06","placeholder":"​","style":"IPY_MODEL_9c21cf97c8a644dd842ae9b438291c0d","value":"config.json: 100%"}},"7c631fb62ea0482d81df151c2d66a73b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a764a4fa45f64f10a1a17478e61464fe","max":501,"min":0,"orientation":"horizontal","style":"IPY_MODEL_28827d220d87464c8e143d0e3577d894","value":501}},"ceea6a75ed854e568920957bebd4e0d5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b2aea4f22bb443bb2b0b3fd97b0d6b6","placeholder":"​","style":"IPY_MODEL_1197aa79f3814449a95894d671ca6673","value":" 501/501 [00:00&lt;00:00, 54.1kB/s]"}},"b0ae5764abaf4f3ebf93d5cdd1d8833d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4060e76ac4b9455da2e9ff01ccbe9c06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c21cf97c8a644dd842ae9b438291c0d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a764a4fa45f64f10a1a17478e61464fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28827d220d87464c8e143d0e3577d894":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8b2aea4f22bb443bb2b0b3fd97b0d6b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1197aa79f3814449a95894d671ca6673":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"39d9ff08bff045418f374da9f48d7adb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_42f65d3d224443e781c6b472f5ebee00","IPY_MODEL_5f84015b9feb4065a1a6ad49f8c69733","IPY_MODEL_a245a3defc2f41aea8ffba42b166a0a7"],"layout":"IPY_MODEL_42bd74d005f24a0990cd2e67c0a52571"}},"42f65d3d224443e781c6b472f5ebee00":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2396ada8faff40b6afd69eb5cf0a201a","placeholder":"​","style":"IPY_MODEL_16f19eee68d44cd1b03eddaafe35a8e7","value":"vocab.json: "}},"5f84015b9feb4065a1a6ad49f8c69733":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_acb1d2f7f9b74ff699b95420ab27b468","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_12de9084925e4b9e974dd77ddc547d75","value":1}},"a245a3defc2f41aea8ffba42b166a0a7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e00fdf68ca641f1b5313366908e23c8","placeholder":"​","style":"IPY_MODEL_c511bd11876d4c4ba625d99b0d8bfb2d","value":" 9.43k/? [00:00&lt;00:00, 906kB/s]"}},"42bd74d005f24a0990cd2e67c0a52571":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2396ada8faff40b6afd69eb5cf0a201a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16f19eee68d44cd1b03eddaafe35a8e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"acb1d2f7f9b74ff699b95420ab27b468":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"12de9084925e4b9e974dd77ddc547d75":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5e00fdf68ca641f1b5313366908e23c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c511bd11876d4c4ba625d99b0d8bfb2d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f367c10f11e34cfdaafb05c99c25f254":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_84b4e3f4c3b54b0d99a7989caa8fc714","IPY_MODEL_1457e3f1e843406cbef47821321af340","IPY_MODEL_ab65d3d10ddd47078875c75f02acd876"],"layout":"IPY_MODEL_8df0ed333a82467d912972ce084b3e75"}},"84b4e3f4c3b54b0d99a7989caa8fc714":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6062d09c63e4d4a9c503e4741ec424d","placeholder":"​","style":"IPY_MODEL_6e389fcc841c4e94a7502bc1e312a051","value":"merges.txt: "}},"1457e3f1e843406cbef47821321af340":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9412daddb4ff4a0db1493c72c880ae41","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d8ab34e52b4543428b7a7a818d61e64d","value":1}},"ab65d3d10ddd47078875c75f02acd876":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c729159dcd0e4783b31ade54231e3a42","placeholder":"​","style":"IPY_MODEL_c4dd178bc4684f929aaac430913e2f97","value":" 3.21k/? [00:00&lt;00:00, 269kB/s]"}},"8df0ed333a82467d912972ce084b3e75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6062d09c63e4d4a9c503e4741ec424d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e389fcc841c4e94a7502bc1e312a051":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9412daddb4ff4a0db1493c72c880ae41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"d8ab34e52b4543428b7a7a818d61e64d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c729159dcd0e4783b31ade54231e3a42":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4dd178bc4684f929aaac430913e2f97":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"27522c79826e448dae2a5ee88399c8e9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_de62e30a54d74e7aac104149038da8ae","IPY_MODEL_1132761ee1384ee19f1ee8b6bd6944ab","IPY_MODEL_931843408e6c47f58e941b15944d840c"],"layout":"IPY_MODEL_419fb3ff1b324271a0018bd374f9346e"}},"de62e30a54d74e7aac104149038da8ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc51367d1abe46fbb44ac2f8b900416c","placeholder":"​","style":"IPY_MODEL_25cc923a73e64e98b8348772ba5cac83","value":"special_tokens_map.json: 100%"}},"1132761ee1384ee19f1ee8b6bd6944ab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2f89f5fe9a6432ea0843a32e96848cb","max":150,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3e56c423b7764f4f8bc173120f5162e1","value":150}},"931843408e6c47f58e941b15944d840c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89e93a71ede64a22a7527cbb8151aac8","placeholder":"​","style":"IPY_MODEL_bc3bca42ad5549d6a362110a3ea3b44b","value":" 150/150 [00:00&lt;00:00, 14.2kB/s]"}},"419fb3ff1b324271a0018bd374f9346e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc51367d1abe46fbb44ac2f8b900416c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25cc923a73e64e98b8348772ba5cac83":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d2f89f5fe9a6432ea0843a32e96848cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e56c423b7764f4f8bc173120f5162e1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"89e93a71ede64a22a7527cbb8151aac8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc3bca42ad5549d6a362110a3ea3b44b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e65c27988e7f48199eac0d6ae1f05404":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a15715f6e63b43ea894a6b53a44504a7","IPY_MODEL_fb8031d7243a46cea6b59827dc146765","IPY_MODEL_3b330acd103949e68071691b92e3afcb"],"layout":"IPY_MODEL_7598e87f1f3f4d4c8cce818b68692755"}},"a15715f6e63b43ea894a6b53a44504a7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f36d48299714c35a7d31b5e702cc238","placeholder":"​","style":"IPY_MODEL_3e6dcda0df9343f0b161206feb201578","value":"tokenizer_config.json: "}},"fb8031d7243a46cea6b59827dc146765":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_945daae38b214cddb32f6b586ee64b90","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5bef6ef17b714b559fe107de50920adf","value":1}},"3b330acd103949e68071691b92e3afcb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ccc77060e604a41a0808bb0baa00ae1","placeholder":"​","style":"IPY_MODEL_1fa52c95709b4ff5bc8a2b7d84a5c166","value":" 7.34k/? [00:00&lt;00:00, 816kB/s]"}},"7598e87f1f3f4d4c8cce818b68692755":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f36d48299714c35a7d31b5e702cc238":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e6dcda0df9343f0b161206feb201578":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"945daae38b214cddb32f6b586ee64b90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"5bef6ef17b714b559fe107de50920adf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9ccc77060e604a41a0808bb0baa00ae1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fa52c95709b4ff5bc8a2b7d84a5c166":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c7e0b2494950481ab06aa036b84533bb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_02c14f25ccc64ae8a07d3bd83e5caadb","IPY_MODEL_a02c7919908949eda603dd1dc7226c29","IPY_MODEL_f29d4b8128be4308a42f47e6958c1b96"],"layout":"IPY_MODEL_5d6288193c9846bab82bdfbc5cb75f6c"}},"02c14f25ccc64ae8a07d3bd83e5caadb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64b96b296cb547b8be963cd8026ae77f","placeholder":"​","style":"IPY_MODEL_8d953e0271f14e0e94ae00a3c6b7ccf1","value":"vocab.json: "}},"a02c7919908949eda603dd1dc7226c29":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_95c35f3357b54707adaa3b89b8ca8faa","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a38ef3f7924c40b89bd41e6175cae1b5","value":1}},"f29d4b8128be4308a42f47e6958c1b96":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c1f03aa686944d19e0855e9e947593f","placeholder":"​","style":"IPY_MODEL_2339b0e5856a4c658c949c33b81e0eb8","value":" 798k/? [00:00&lt;00:00, 40.4MB/s]"}},"5d6288193c9846bab82bdfbc5cb75f6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64b96b296cb547b8be963cd8026ae77f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d953e0271f14e0e94ae00a3c6b7ccf1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"95c35f3357b54707adaa3b89b8ca8faa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"a38ef3f7924c40b89bd41e6175cae1b5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9c1f03aa686944d19e0855e9e947593f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2339b0e5856a4c658c949c33b81e0eb8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"61b0c94665454480a3d92e4813705bf4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e9b16f33744d4231a2ace3572cf335f1","IPY_MODEL_f0a6eba1c99d4bba939a542751a7d7a1","IPY_MODEL_39d68c36a46c4eec83fed0c7e2ec99cf"],"layout":"IPY_MODEL_db66629459c4462fb26cb9d23f9ef644"}},"e9b16f33744d4231a2ace3572cf335f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6b30ed854b44ce599384d981e71e70c","placeholder":"​","style":"IPY_MODEL_d8fd079708df4c4a8848b13af215f892","value":"merges.txt: "}},"f0a6eba1c99d4bba939a542751a7d7a1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f177fa18dce64b088e95b4b8584faf31","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3810f7e03c8a40689ecee8500d36030a","value":1}},"39d68c36a46c4eec83fed0c7e2ec99cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8a76709f59f49dba183f17a33a331eb","placeholder":"​","style":"IPY_MODEL_c59a68af4ed24b8698e81bb6cd25b5f3","value":" 456k/? [00:00&lt;00:00, 26.4MB/s]"}},"db66629459c4462fb26cb9d23f9ef644":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6b30ed854b44ce599384d981e71e70c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8fd079708df4c4a8848b13af215f892":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f177fa18dce64b088e95b4b8584faf31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"3810f7e03c8a40689ecee8500d36030a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a8a76709f59f49dba183f17a33a331eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c59a68af4ed24b8698e81bb6cd25b5f3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b10379e07a48465a966c237283429971":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_93e69106f3a147b0bd4a72a64e1dde8d","IPY_MODEL_f7bc9b1ca2574ef9981216f87c78a788","IPY_MODEL_0705b584ffe6448bbfeada31c7c83d5c"],"layout":"IPY_MODEL_25577cd262d34d089c094ab6c88fc481"}},"93e69106f3a147b0bd4a72a64e1dde8d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_14fa6fd1b3ee4cceae285f4aa3772a90","placeholder":"​","style":"IPY_MODEL_5a560f16f92e484694aa4646282590f0","value":"tokenizer.json: "}},"f7bc9b1ca2574ef9981216f87c78a788":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5793fb10e73342819feb6c037d08bcd5","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_70b399b2bdd94643b2957ecf68a8ff0d","value":1}},"0705b584ffe6448bbfeada31c7c83d5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_164e2961d4354374ac9dc7c610f53dea","placeholder":"​","style":"IPY_MODEL_71231c11be1a4d4c8ad46d3757214b2e","value":" 2.11M/? [00:00&lt;00:00, 90.2MB/s]"}},"25577cd262d34d089c094ab6c88fc481":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14fa6fd1b3ee4cceae285f4aa3772a90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a560f16f92e484694aa4646282590f0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5793fb10e73342819feb6c037d08bcd5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"70b399b2bdd94643b2957ecf68a8ff0d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"164e2961d4354374ac9dc7c610f53dea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71231c11be1a4d4c8ad46d3757214b2e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9026e23a49504f3e8223341f55ddbf63":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9d4bb8fb6845492d998797914d6e8683","IPY_MODEL_c307187b2c00455281c46a5fe6ca76ce","IPY_MODEL_a6c22c602a4c43aa9ce23eaaddb2b65c"],"layout":"IPY_MODEL_5a89bcbe91b740dfab106f005a9f8446"}},"9d4bb8fb6845492d998797914d6e8683":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b89f563a66dc43bc9d2ff21473ca6cd7","placeholder":"​","style":"IPY_MODEL_2ce338b426b845a392d9066b7a8261bd","value":"added_tokens.json: "}},"c307187b2c00455281c46a5fe6ca76ce":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_95704dadeff648a0b13bf949c29511d6","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8a524041456f4a538b374deff567d865","value":1}},"a6c22c602a4c43aa9ce23eaaddb2b65c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b08634d7f214f1dbb0dadc2eee88aab","placeholder":"​","style":"IPY_MODEL_6f6c7dd030a9452cbe4deab01845b510","value":" 1.08k/? [00:00&lt;00:00, 96.6kB/s]"}},"5a89bcbe91b740dfab106f005a9f8446":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b89f563a66dc43bc9d2ff21473ca6cd7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ce338b426b845a392d9066b7a8261bd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"95704dadeff648a0b13bf949c29511d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"8a524041456f4a538b374deff567d865":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5b08634d7f214f1dbb0dadc2eee88aab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f6c7dd030a9452cbe4deab01845b510":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"af6723deff6c42e4a7a2897f4a298b4d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a5c8b4d1ee834573b1aec71e3dbd60d9","IPY_MODEL_e762fe10e3524bbc8846e76643ee879b","IPY_MODEL_984effe7e62a49b0818f6de916ba5a95"],"layout":"IPY_MODEL_a92ea24879c14a11b1791b37151a2dfd"}},"a5c8b4d1ee834573b1aec71e3dbd60d9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3df264139ab4f3d9f29e3b542a915df","placeholder":"​","style":"IPY_MODEL_1ffa12fc6deb4609985cfabb5b0ad610","value":"special_tokens_map.json: 100%"}},"e762fe10e3524bbc8846e76643ee879b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a58e0aca91c4f289d4d0d16a81de169","max":99,"min":0,"orientation":"horizontal","style":"IPY_MODEL_43792898c9894cbf942fa612f128df6c","value":99}},"984effe7e62a49b0818f6de916ba5a95":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f329d214e4a345a48fa5b7a8e5206cdb","placeholder":"​","style":"IPY_MODEL_182e07be04ee4a86991687040519cffc","value":" 99.0/99.0 [00:00&lt;00:00, 11.3kB/s]"}},"a92ea24879c14a11b1791b37151a2dfd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3df264139ab4f3d9f29e3b542a915df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ffa12fc6deb4609985cfabb5b0ad610":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a58e0aca91c4f289d4d0d16a81de169":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43792898c9894cbf942fa612f128df6c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f329d214e4a345a48fa5b7a8e5206cdb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"182e07be04ee4a86991687040519cffc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"09eceb7f9c0d46f495e5cb60df32fdfd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_982b57dba0e14e82a88797b01c5c0d20","IPY_MODEL_f32fa6a287fa4354a69bc0be94f5e1bb","IPY_MODEL_b3517739baf24a39bc4a8dc71477f69e"],"layout":"IPY_MODEL_b517737b018444a898d1a2d9a5cbde9d"}},"982b57dba0e14e82a88797b01c5c0d20":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_253983c792c542559040f99cb548bb28","placeholder":"​","style":"IPY_MODEL_680a0a888adf4d78bbc64ade6bdd219a","value":"tokenizer_config.json: 100%"}},"f32fa6a287fa4354a69bc0be94f5e1bb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee57996e05154635a5dde08bf366a0b5","max":396,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7c70a42efa2c4d099b44a70e24e34f6c","value":396}},"b3517739baf24a39bc4a8dc71477f69e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_334a4aa96bf1413cbf626b92d569833e","placeholder":"​","style":"IPY_MODEL_d702cc90cc544e88a4d65ee38cc41c75","value":" 396/396 [00:00&lt;00:00, 52.2kB/s]"}},"b517737b018444a898d1a2d9a5cbde9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"253983c792c542559040f99cb548bb28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"680a0a888adf4d78bbc64ade6bdd219a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee57996e05154635a5dde08bf366a0b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c70a42efa2c4d099b44a70e24e34f6c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"334a4aa96bf1413cbf626b92d569833e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d702cc90cc544e88a4d65ee38cc41c75":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d8e0c673366c4edeab792ec2199ddd2d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_58a2a2dc1e3847c1a992a9f2f2537394","IPY_MODEL_7478c8f9f1cc42b6a226bb066b22a179","IPY_MODEL_c3560de112f741e8bdf34f7d892fd94a"],"layout":"IPY_MODEL_b9644d0a27d2440bb2b7981274607b20"}},"58a2a2dc1e3847c1a992a9f2f2537394":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b9c9b952af64b24b6edcbdc30a96208","placeholder":"​","style":"IPY_MODEL_cb048f63bc6244e9911b942b107fb197","value":"tokenizer.json: "}},"7478c8f9f1cc42b6a226bb066b22a179":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8165a03e5eb548f99a1f0b63f835d69a","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_68f72991fbb94f55aa2394027144b6dd","value":1}},"c3560de112f741e8bdf34f7d892fd94a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cbe321fbd28144b3a97269c3c33291ec","placeholder":"​","style":"IPY_MODEL_e9ddd04158b045479eeb7bfff0e6965b","value":" 2.11M/? [00:00&lt;00:00, 88.4MB/s]"}},"b9644d0a27d2440bb2b7981274607b20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b9c9b952af64b24b6edcbdc30a96208":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb048f63bc6244e9911b942b107fb197":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8165a03e5eb548f99a1f0b63f835d69a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"68f72991fbb94f55aa2394027144b6dd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cbe321fbd28144b3a97269c3c33291ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9ddd04158b045479eeb7bfff0e6965b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c52f88651c7146cab7c49ccf8b0351bd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_275ed6a1687f4f8e8a1284d87db98bb6","IPY_MODEL_5200d2a6ccb949119542b84583ede8f9","IPY_MODEL_0ab27d0284dd48fd89b44ea7699f1067"],"layout":"IPY_MODEL_3aa05d2c980d493986f925d2820cb0bb"}},"275ed6a1687f4f8e8a1284d87db98bb6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f3505f0eebb482b952ad2329cf06a4a","placeholder":"​","style":"IPY_MODEL_8ea9faed1e5048268c54dc3dfce158b5","value":"special_tokens_map.json: 100%"}},"5200d2a6ccb949119542b84583ede8f9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c04ca3ea83cc433ca0a26adb46848dba","max":99,"min":0,"orientation":"horizontal","style":"IPY_MODEL_190fb865c2294e578c8cbdb7311c4af2","value":99}},"0ab27d0284dd48fd89b44ea7699f1067":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9c970fd2f8f49a1a74bcc044b3825cd","placeholder":"​","style":"IPY_MODEL_0842da4cdc1145ea96867869eec1fccd","value":" 99.0/99.0 [00:00&lt;00:00, 12.0kB/s]"}},"3aa05d2c980d493986f925d2820cb0bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f3505f0eebb482b952ad2329cf06a4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ea9faed1e5048268c54dc3dfce158b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c04ca3ea83cc433ca0a26adb46848dba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"190fb865c2294e578c8cbdb7311c4af2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e9c970fd2f8f49a1a74bcc044b3825cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0842da4cdc1145ea96867869eec1fccd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# Install only what's needed\n","!pip install torch-geometric -q\n","!pip install rdkit\n","import torch\n","from torch.utils.data import IterableDataset\n","from torch_geometric.datasets import MoleculeNet\n","import pandas as pd\n","import itertools\n","import sys\n","from google.colab import drive\n","import os\n","\n","gdrive_path='/content/gdrive/MyDrive/Project_HIV'\n","\n","# This will mount your google drive under 'MyDrive'\n","drive.mount('/content/gdrive', force_remount=True)\n","# In order to access the files in this notebook we have to navigate to the correct folder\n","os.chdir(gdrive_path)\n","# Check manually if all files are present\n","print(sorted(os.listdir()))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HBpIix3WkoOg","executionInfo":{"status":"ok","timestamp":1756551556878,"user_tz":-120,"elapsed":52689,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"outputId":"e9bd9759-b05c-434e-aad9-7914713e9907"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting rdkit\n","  Downloading rdkit-2025.3.5-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rdkit) (2.0.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rdkit) (11.3.0)\n","Downloading rdkit-2025.3.5-cp312-cp312-manylinux_2_28_x86_64.whl (36.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.2/36.2 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: rdkit\n","Successfully installed rdkit-2025.3.5\n","Mounted at /content/gdrive\n","['.git', 'HIV.ipynb', 'README.md', '__pycache__', 'checkpoint_batch_2000.pt', 'code', 'data_import.py', 'datasets', 'hiv_train.jsonl', 'hiv_val.jsonl', 'pythia_pe_peft.py', 'utils.py']\n"]}]},{"cell_type":"code","source":["# Add code directory to path\n","from torch_geometric.datasets import MoleculeNet\n","# sys.path.insert(0, os.path.join(os.getcwd(), 'code'))\n","\n","# # Import the Colab-friendly version\n","# from data.dataset_colab import PartialHIVDataset\n","\n","class PartialHIVDataset(IterableDataset):\n","    \"\"\"\n","    IterableDataset that loads only a small portion of the HIV dataset.\n","    \"\"\"\n","\n","    def __init__(self, root='/tmp/HIV', max_samples=10):\n","        \"\"\"\n","        Args:\n","            root: Root directory for dataset storage\n","            max_samples: Maximum number of samples to load\n","        \"\"\"\n","        self.root = root\n","        self.max_samples = max_samples\n","        self._dataset = None\n","\n","    def _lazy_load_dataset(self):\n","        \"\"\"Lazy load the dataset only when iteration begins\"\"\"\n","        if self._dataset is None:\n","            print(f\"Initializing HIV dataset (will only load {self.max_samples} samples)...\")\n","            # Import here to ensure it's available\n","\n","            self._dataset = MoleculeNet(root=self.root, name='HIV')\n","            print(f\"Dataset ready! Total size: {len(self._dataset)} molecules\")\n","            print(f\"But we'll only load {self.max_samples} of them.\\n\")\n","\n","    def parse_molecules(self):\n","        \"\"\"\n","        Parse molecules from the dataset, stopping after max_samples.\n","        \"\"\"\n","        self._lazy_load_dataset()\n","\n","        for i in range(min(self.max_samples, len(self._dataset))):\n","            data = self._dataset[i]\n","\n","            # Extract molecule information\n","            mol_info = {\n","                'source': f\"molecule_{i}\",  # Similar to your CustomIterableDataset\n","                'target': data.y.item(),    # HIV activity label\n","                'num_atoms': data.num_nodes,\n","                'num_bonds': data.num_edges // 2,  # Undirected edges\n","                'smiles': getattr(data, 'smiles', 'N/A')\n","            }\n","\n","            yield mol_info\n","\n","    def __iter__(self):\n","        \"\"\"Iterator with worker support\"\"\"\n","        iterator = self.parse_molecules()\n","        worker_info = torch.utils.data.get_worker_info()\n","\n","        if worker_info is not None:\n","            worker_total_num = worker_info.num_workers\n","            worker_id = worker_info.id\n","            return itertools.islice(iterator, worker_id, None, worker_total_num)\n","\n","        return iterator\n","\n","\n","# Your code\n","batch_list = []\n","batch_size = 40\n","\n","# Create dataset\n","dataset = PartialHIVDataset(max_samples=batch_size)\n","iterator = iter(dataset)\n","full_batch_data = list(dataset)\n","\n","df = pd.DataFrame(full_batch_data)"],"metadata":{"id":"uCZHJO55qvlC","executionInfo":{"status":"ok","timestamp":1756551604104,"user_tz":-120,"elapsed":47222,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9a4dce9f-224c-4f41-dbcb-2783beead79f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing HIV dataset (will only load 40 samples)...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/HIV.csv\n","Processing...\n","/usr/local/lib/python3.12/dist-packages/torch_geometric/datasets/molecule_net.py:213: UserWarning: Skipping molecule 'O=C1O[Al]23(OC1=O)(OC(=O)C(=O)O2)OC(=O)C(=O)O3' since it resulted in zero atoms\n","  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n","/usr/local/lib/python3.12/dist-packages/torch_geometric/datasets/molecule_net.py:213: UserWarning: Skipping molecule 'Cc1ccc([B-2]2(c3ccc(C)cc3)=NCCO2)cc1' since it resulted in zero atoms\n","  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n","/usr/local/lib/python3.12/dist-packages/torch_geometric/datasets/molecule_net.py:213: UserWarning: Skipping molecule 'Oc1ccc(C2Oc3cc(O)cc4c3C(=[O+][AlH3-3]35([O+]=C6c7c(cc(O)cc7[OH+]3)OC(c3ccc(O)cc3O)C6O)([O+]=C3c6c(cc(O)cc6[OH+]5)OC(c5ccc(O)cc5O)C3O)[OH+]4)C2O)c(O)c1' since it resulted in zero atoms\n","  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n","/usr/local/lib/python3.12/dist-packages/torch_geometric/datasets/molecule_net.py:213: UserWarning: Skipping molecule 'CC1=C2[OH+][AlH3-3]34([O+]=C2C=CN1C)([O+]=C1C=CN(C)C(C)=C1[OH+]3)[O+]=C1C=CN(C)C(C)=C1[OH+]4' since it resulted in zero atoms\n","  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n","/usr/local/lib/python3.12/dist-packages/torch_geometric/datasets/molecule_net.py:213: UserWarning: Skipping molecule 'CC(c1cccs1)=[N+]1[N-]C(N)=[S+][AlH3-]12[OH+]B(c1ccccc1)[OH+]2' since it resulted in zero atoms\n","  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n","/usr/local/lib/python3.12/dist-packages/torch_geometric/datasets/molecule_net.py:213: UserWarning: Skipping molecule 'CC(c1ccccn1)=[N+]1[N-]C(N)=[S+][AlH3-]12[OH+]B(c1ccccc1)[OH+]2' since it resulted in zero atoms\n","  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n","/usr/local/lib/python3.12/dist-packages/torch_geometric/datasets/molecule_net.py:213: UserWarning: Skipping molecule '[Na+].c1ccc([SH+][GeH2+]2[SH+]c3ccccc3[SH+]2)c([SH+][GeH2+]2[SH+]c3ccccc3[SH+]2)c1' since it resulted in zero atoms\n","  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n","Done!\n"]},{"output_type":"stream","name":"stdout","text":["Dataset ready! Total size: 41120 molecules\n","But we'll only load 40 of them.\n","\n"]}]},{"cell_type":"code","source":["import codecs\n","\n","import numpy as np\n","import torch.nn as nn\n","from transformers import AutoTokenizer\n","\n","smiles_list = []\n","batch_size = 40\n","\n","\n","# Iterate through the dataset and fill it with the SMILES strings\n","for _ in range(batch_size):\n","  mol_info = next(iterator)\n","  smiles_list.append(mol_info['smiles'])\n","\n","\n","\n","# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n","\n","# First, tokenize without padding to find the actual max length\n","tokenized_lengths = []\n","for smiles in smiles_list:\n","    tokens = tokenizer(smiles, padding=False, truncation=False)\n","    tokenized_lengths.append(len(tokens['input_ids']))\n","\n","# Determine max_length from your data\n","max_length = max(tokenized_lengths)\n","print(f\"Maximum sequence length in data: {max_length}\")\n","\n","\n","# Now tokenize with the data-driven max_length\n","encoded = tokenizer(\n","    smiles_list,\n","    padding='max_length',\n","    truncation=True,\n","    max_length=max_length,\n","    return_tensors='pt'\n",")\n","\n","# Create embedding layer\n","embedding = nn.Embedding(\n","    num_embeddings=len(tokenizer),\n","    embedding_dim=512,\n","    padding_idx=tokenizer.pad_token_id\n",")\n","\n","# Get embeddings directly\n","embeddings = embedding(encoded['input_ids'])\n","print(f\"Embeddings shape: {embeddings.shape}\")  # (batch_size, max_length, 512)\n","\n"],"metadata":{"id":"oRsTzvL0l5bx","executionInfo":{"status":"ok","timestamp":1756551609220,"user_tz":-120,"elapsed":5112,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"colab":{"base_uri":"https://localhost:8080/","height":340,"referenced_widgets":["ffe3d442836d4fee9de00a5e5dcc005b","6ce085672692487281f5691231a6a143","a2ddfa5c82d34d10b4085ce9757a29c9","9e59931717a44b529e40b6cc5e259827","fa540e8387404444a4b4488b1ef1bd8c","9880b041901c4f6bbb763d7f5c501f50","69be98f307a9417b910df9598f133fce","2f5e4a227fe54834b75742ca7ac430bf","466e57d216dc4895956bf9acab08b153","21fc82595fc9462db8485dbd5f1347f9","833edb047093437491d8b6c4f57cf393","93aa535ba74545c1a7f238fc0a0645ab","d925e19eb72a49efa28d86d5696e7484","7c631fb62ea0482d81df151c2d66a73b","ceea6a75ed854e568920957bebd4e0d5","b0ae5764abaf4f3ebf93d5cdd1d8833d","4060e76ac4b9455da2e9ff01ccbe9c06","9c21cf97c8a644dd842ae9b438291c0d","a764a4fa45f64f10a1a17478e61464fe","28827d220d87464c8e143d0e3577d894","8b2aea4f22bb443bb2b0b3fd97b0d6b6","1197aa79f3814449a95894d671ca6673","39d9ff08bff045418f374da9f48d7adb","42f65d3d224443e781c6b472f5ebee00","5f84015b9feb4065a1a6ad49f8c69733","a245a3defc2f41aea8ffba42b166a0a7","42bd74d005f24a0990cd2e67c0a52571","2396ada8faff40b6afd69eb5cf0a201a","16f19eee68d44cd1b03eddaafe35a8e7","acb1d2f7f9b74ff699b95420ab27b468","12de9084925e4b9e974dd77ddc547d75","5e00fdf68ca641f1b5313366908e23c8","c511bd11876d4c4ba625d99b0d8bfb2d","f367c10f11e34cfdaafb05c99c25f254","84b4e3f4c3b54b0d99a7989caa8fc714","1457e3f1e843406cbef47821321af340","ab65d3d10ddd47078875c75f02acd876","8df0ed333a82467d912972ce084b3e75","e6062d09c63e4d4a9c503e4741ec424d","6e389fcc841c4e94a7502bc1e312a051","9412daddb4ff4a0db1493c72c880ae41","d8ab34e52b4543428b7a7a818d61e64d","c729159dcd0e4783b31ade54231e3a42","c4dd178bc4684f929aaac430913e2f97","27522c79826e448dae2a5ee88399c8e9","de62e30a54d74e7aac104149038da8ae","1132761ee1384ee19f1ee8b6bd6944ab","931843408e6c47f58e941b15944d840c","419fb3ff1b324271a0018bd374f9346e","fc51367d1abe46fbb44ac2f8b900416c","25cc923a73e64e98b8348772ba5cac83","d2f89f5fe9a6432ea0843a32e96848cb","3e56c423b7764f4f8bc173120f5162e1","89e93a71ede64a22a7527cbb8151aac8","bc3bca42ad5549d6a362110a3ea3b44b"]},"outputId":"347e7d92-b4eb-4e3b-e3c4-3261bd4a2d7b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/166 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffe3d442836d4fee9de00a5e5dcc005b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/501 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93aa535ba74545c1a7f238fc0a0645ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39d9ff08bff045418f374da9f48d7adb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f367c10f11e34cfdaafb05c99c25f254"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27522c79826e448dae2a5ee88399c8e9"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Maximum sequence length in data: 61\n","Embeddings shape: torch.Size([40, 61, 512])\n"]}]},{"cell_type":"code","source":["from scipy.sparse.linalg import eigsh\n","from rdkit import Chem\n","def smiles_to_adjacency_matrix(smiles):\n","    \"\"\"Convert SMILES to adjacency matrix.\"\"\"\n","    mol = Chem.MolFromSmiles(smiles)\n","    if mol is None:\n","        return None\n","\n","    n_atoms = mol.GetNumAtoms()\n","    adj_matrix = np.zeros((n_atoms, n_atoms))\n","\n","    for bond in mol.GetBonds():\n","        i = bond.GetBeginAtomIdx()\n","        j = bond.GetEndAtomIdx()\n","        # Undirected graph\n","        adj_matrix[i, j] = 1\n","        adj_matrix[j, i] = 1\n","\n","    return adj_matrix\n","\n","def compute_graph_positional_encoding(adj_matrix, k=30):\n","    \"\"\"\n","    Compute graph positional encodings from eigenvectors of the\n","    symmetrically normalized graph Laplacian.\n","    \"\"\"\n","    n = adj_matrix.shape[0]\n","\n","    # Compute degree matrix\n","    degree = np.sum(adj_matrix, axis=1)\n","    degree[degree == 0] = 1\n","\n","    # D^(-1/2)\n","    d_inv_sqrt = np.diag(1.0 / np.sqrt(degree))\n","\n","    # Symmetrically normalized Laplacian\n","    identity = np.eye(n)\n","    normalized_adj = d_inv_sqrt @ adj_matrix @ d_inv_sqrt\n","    laplacian = identity - normalized_adj\n","\n","    # Compute eigenvectors\n","    if n < k:\n","        # If graph has fewer nodes than k, pad with zeros\n","        eigenvalues, eigenvectors = np.linalg.eigh(laplacian)\n","        # Pad eigenvectors to have k columns\n","        padded_eigenvectors = np.zeros((n, k))\n","        padded_eigenvectors[:, :n] = eigenvectors\n","        return padded_eigenvectors\n","    else:\n","        eigenvalues, eigenvectors = eigsh(laplacian, k=k, which='SM')\n","        return eigenvectors"],"metadata":{"id":"5VXPZypgg5_P","executionInfo":{"status":"ok","timestamp":1756551609236,"user_tz":-120,"elapsed":13,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Process all SMILES and compute their graph PEs\n","graph_pes_list = []\n","max_nodes = 0\n","\n","for smiles in smiles_list:\n","    adj_matrix = smiles_to_adjacency_matrix(smiles)\n","    if adj_matrix is not None:\n","        pe = compute_graph_positional_encoding(adj_matrix, k=30)\n","        graph_pes_list.append(pe)\n","        max_nodes = max(max_nodes, pe.shape[0])\n","    else:\n","        graph_pes_list.append(None)\n","\n","print(f\"Maximum number of atoms in dataset: {max_nodes}\")\n"],"metadata":{"id":"QadoBoVYgrrz","executionInfo":{"status":"ok","timestamp":1756551609292,"user_tz":-120,"elapsed":41,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"593bffb5-13ce-4646-c5b2-ae8fa50f0b60"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum number of atoms in dataset: 42\n"]}]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","from rdkit import Chem\n","import re\n","\n","# def generate_random_orthonormal_pe(n_vectors, dim=30):\n","#     \"\"\"Generate n_vectors random orthonormal vectors of dimension dim.\"\"\"\n","#     if n_vectors == 0:\n","#         return np.zeros((0, dim))\n","\n","#     # Generate random matrix\n","#     random_matrix = np.random.randn(dim, n_vectors)\n","\n","#     # Use QR decomposition to get orthonormal vectors\n","#     Q, _ = np.linalg.qr(random_matrix)\n","\n","#     # Return first n_vectors columns (transposed to have shape (n_vectors, dim))\n","#     return Q[:, :n_vectors].T\n","\n","def generate_zero_pe(n_vectors, dim=30):\n","    \"\"\"Generate n_vectors zero vectors of dimension dim.\"\"\"\n","    return np.zeros((n_vectors, dim))\n","\n","def parse_token_components(token):\n","    \"\"\"\n","    Parse a token to identify atoms and characters.\n","    Returns: (atom_indices, n_characters)\n","    \"\"\"\n","    # Common atom patterns in SMILES\n","    atom_pattern = r'(Cl|Br|Si|Mg|Ca|Fe|Al|Na|Li|[BCNOFPSKHIV])'\n","\n","    # Find all atoms in the token\n","    atoms = re.findall(atom_pattern, token)\n","\n","    # Count non-atom characters\n","    # Remove atoms from token to count remaining characters\n","    remaining = token\n","    for atom in atoms:\n","        remaining = remaining.replace(atom, '', 1)\n","\n","    # Count actual characters (digits, +, -, =, #, etc.)\n","    n_characters = len([c for c in remaining])\n","\n","    return atoms, n_characters\n","\n","def align_graph_pe_to_tokens(smiles, graph_pe, tokenizer, max_length,random_seed=None):\n","    \"\"\"\n","    Align graph PE to tokens, handling pure characters, atoms, and mixed tokens.\n","\n","    Args:\n","        smiles: SMILES string\n","        graph_pe: Graph positional encoding for atoms (n_atoms, embedding_dim)\n","        tokenizer: Tokenizer object\n","        max_length: Maximum sequence length\n","        random_seed: Random seed for reproducible random PEs\n","    \"\"\"\n","    if random_seed is not None:\n","        np.random.seed(random_seed)\n","    # Determine embedding dimension\n","    if graph_pe is not None:\n","        embedding_dim = graph_pe.shape[1]\n","    elif embedding_dim is None:\n","        embedding_dim = 30  # Default fallback\n","\n","    mol = Chem.MolFromSmiles(smiles)\n","    if mol is None or graph_pe is None:\n","        return torch.zeros(max_length, embedding_dim)\n","\n","    # Get tokens\n","    encoding = tokenizer(smiles, padding='max_length', max_length=max_length)\n","    tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n","\n","\n","    # Initialize token PE\n","    token_pe = torch.zeros(max_length, embedding_dim)\n","\n","    # Build atom mapping from SMILES\n","    atom_symbols = [atom.GetSymbol() for atom in mol.GetAtoms()]\n","    atom_count = {symbol: 0 for symbol in set(atom_symbols)}\n","    atom_to_idx = {}\n","\n","    for idx, symbol in enumerate(atom_symbols):\n","        atom_to_idx[(symbol, atom_count[symbol])] = idx\n","        atom_count[symbol] += 1\n","\n","    # Reset atom count for tracking\n","    current_atom_count = {symbol: 0 for symbol in atom_count}\n","\n","    # Process each token\n","    for i, token in enumerate(tokens):\n","        if token in ['<s>', '</s>', '<pad>']:\n","            continue\n","\n","        # Parse token components\n","        atoms_in_token, n_characters = parse_token_components(token)\n","\n","        if len(atoms_in_token) == 0 and n_characters > 0:\n","            # Pure character token - use random orthonormal PE\n","            random_pes = generate_zero_pe(n_characters, embedding_dim)\n","            token_pe[i] = torch.tensor(random_pes.sum(axis=0))\n","\n","        elif len(atoms_in_token) > 0 and n_characters == 0:\n","            # Pure atom token(s) - use graph PE\n","            atom_pes = []\n","            for atom_symbol in atoms_in_token:\n","                if atom_symbol in current_atom_count:\n","                    atom_key = (atom_symbol, current_atom_count[atom_symbol])\n","                    if atom_key in atom_to_idx:\n","                        atom_idx = atom_to_idx[atom_key]\n","                        atom_pes.append(graph_pe[atom_idx])\n","                        current_atom_count[atom_symbol] += 1\n","\n","            if atom_pes:\n","                # sum the PEs of all atoms in this token\n","                token_pe[i] = torch.tensor(np.sum(atom_pes, axis=0))\n","\n","        elif len(atoms_in_token) > 0 and n_characters > 0:\n","            # Mixed token - combine atom PE and character PE\n","            # Get atom PEs\n","            atom_pes = []\n","            for atom_symbol in atoms_in_token:\n","                if atom_symbol in current_atom_count:\n","                    atom_key = (atom_symbol, current_atom_count[atom_symbol])\n","                    if atom_key in atom_to_idx:\n","                        atom_idx = atom_to_idx[atom_key]\n","                        atom_pes.append(graph_pe[atom_idx])\n","                        current_atom_count[atom_symbol] += 1\n","\n","            # Get character PEs\n","            random_pes = generate_zero_pe(n_characters, embedding_dim)\n","\n","            # Combine: sum of atoms + sum of characters\n","            combined_pe = np.zeros(embedding_dim)\n","            if atom_pes:\n","                combined_pe += np.sum(atom_pes, axis=0)\n","            combined_pe += random_pes.sum(axis=0)\n","\n","            token_pe[i] = torch.tensor(combined_pe)\n","\n","    return token_pe\n","\n"],"metadata":{"id":"_Ndd99hrDjdB","executionInfo":{"status":"ok","timestamp":1756551609344,"user_tz":-120,"elapsed":47,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["token_pes_list = []\n","\n","for smiles, graph_pe in zip(smiles_list, graph_pes_list):\n","    # Align graph PE to tokens using the provided function\n","    token_pe = align_graph_pe_to_tokens(\n","        smiles=smiles,\n","        graph_pe=graph_pe,\n","        tokenizer=tokenizer,\n","        max_length=max_length,\n","        random_seed=42\n","    )\n","    token_pes_list.append(token_pe)\n","\n","# Stack into batch tensor\n","token_pes_batch = torch.stack(token_pes_list)"],"metadata":{"id":"Ahf_qjEtXHz3","executionInfo":{"status":"ok","timestamp":1756551609437,"user_tz":-120,"elapsed":91,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class EmbeddingWithGraphPE(nn.Module):\n","    def __init__(self, embed_dim=512, pe_dim=30):\n","        super().__init__()\n","        # One-layer projection with GeLU from graph PE to embedding dimension\n","        # Using standard Laplacian, so pe_dim is just k\n","        self.pe_projection = nn.Sequential(\n","            nn.Linear(pe_dim, embed_dim),\n","            nn.GELU()\n","        )\n","\n","    def forward(self, embeddings, token_pes):\n","        # token_pes shape: [batch_size, seq_len, pe_dim]\n","        # embeddings shape: [batch_size, seq_len, embed_dim]\n","\n","        # Project token PEs to embedding dimension with GeLU\n","        projected_pes = self.pe_projection(token_pes)\n","\n","        # Add to token embeddings\n","        enhanced_embeddings = embeddings + projected_pes\n","\n","        return enhanced_embeddings\n","\n","# Usage\n","model = EmbeddingWithGraphPE(embed_dim=512, pe_dim=30)\n","# graph_pes_batch shape: [40, 61, 30] (30 eigenvectors from standard Laplacian)\n","# embeddings shape: [40, 61, 512]\n","enhanced_embeddings = model(embeddings, token_pes_batch)\n","# Output shape: [40, 61, 512]"],"metadata":{"id":"i0zuJ4bRvlJT","executionInfo":{"status":"ok","timestamp":1756551609490,"user_tz":-120,"elapsed":50,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["enhanced_embeddings.shape"],"metadata":{"id":"LYwFIBucX0hF","executionInfo":{"status":"ok","timestamp":1756551609543,"user_tz":-120,"elapsed":51,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3a506b78-19b7-438d-81c7-7479ebe9bc5a"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([40, 61, 512])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["import json\n","from data_import import check_class_balance, stratified_train_val_split, convert_to_litgpt_format, save_to_jsonl\n","\n","# Create dataset\n","batch_size = 41120  # Adjust based on your needs\n","dataset = PartialHIVDataset(max_samples=batch_size)\n","\n","# Convert to DataFrame\n","full_batch_data = list(dataset)\n","df = pd.DataFrame(full_batch_data)\n","\n","\n","# Check balance in the original DataFrame\n","original_balance = check_class_balance(df, \"Original Dataset\")\n","\n","# Convert to LitGPT format for different task types\n","classification_data = convert_to_litgpt_format(df, task_type=\"classification\")\n","\n","# Check balance in the converted data\n","converted_balance = check_class_balance(classification_data, \"Converted Dataset\")\n","\n","# Create stratified train/validation split\n","train_data, val_data = stratified_train_val_split(classification_data, train_ratio=0.8)\n","\n","# Check balance in train and validation sets\n","train_balance = check_class_balance(train_data, \"Training Set\")\n","val_balance = check_class_balance(val_data, \"Validation Set\")\n","\n","# Calculate difference from original distribution\n","train_diff_0 = abs(train_balance['ratio_0'] - original_balance['ratio_0'])\n","train_diff_1 = abs(train_balance['ratio_1'] - original_balance['ratio_1'])\n","val_diff_0 = abs(val_balance['ratio_0'] - original_balance['ratio_0'])\n","val_diff_1 = abs(val_balance['ratio_1'] - original_balance['ratio_1'])\n","\n","\n","# Save to JSONL files\n","save_to_jsonl(train_data, \"hiv_train.jsonl\")\n","save_to_jsonl(val_data, \"hiv_val.jsonl\")\n"],"metadata":{"id":"0i30OS9CSDBm","executionInfo":{"status":"ok","timestamp":1756551620520,"user_tz":-120,"elapsed":10975,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0c00c56b-bf22-45b1-eb70-8398deb6df15"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing HIV dataset (will only load 41120 samples)...\n","Dataset ready! Total size: 41120 molecules\n","But we'll only load 41120 of them.\n","\n","\n","Original Dataset Class Balance:\n","Total samples: 41120\n","Class 0.0: 39677 samples (96.49%)\n","Class 1.0: 1443 samples (3.51%)\n","\n","Converted Dataset Class Balance:\n","Total samples: 41120\n","Class 0: 39677 samples (96.49%)\n","Class 1: 1443 samples (3.51%)\n","\n","Training Set Class Balance:\n","Total samples: 32895\n","Class 0: 31741 samples (96.49%)\n","Class 1: 1154 samples (3.51%)\n","\n","Validation Set Class Balance:\n","Total samples: 8225\n","Class 0: 7936 samples (96.49%)\n","Class 1: 289 samples (3.51%)\n","Saved 32895 entries to hiv_train.jsonl\n","Saved 8225 entries to hiv_val.jsonl\n"]}]},{"cell_type":"code","source":["import json\n","import torch\n","from utils import calculate_max_lengths\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer\n","\n","def load_jsonl(filepath):\n","    \"\"\"Load data from JSONL file\"\"\"\n","    data = []\n","    with open(filepath, 'r') as f:\n","        for line in f:\n","            data.append(json.loads(line))\n","    return data\n","\n","class HIVDataset(Dataset):\n","    def __init__(self, jsonl_filepath, tokenizer, max_full_length):\n","        self.data = load_jsonl(jsonl_filepath)\n","        self.tokenizer = tokenizer\n","        self.max_full_length = max_full_length\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        item = self.data[idx]\n","\n","        # Extract SMILES\n","        smiles = item['input'].replace(\"SMILES: \", \"\")\n","\n","        # Create full prompt\n","        full_prompt = f\"{item['instruction']}\\n{item['input']}\\nAnswer:\"\n","        # Compute graph PE with actual SMILES length\n","        smiles_tokens = self.tokenizer(smiles, add_special_tokens=False)\n","        actual_smiles_length = len(smiles_tokens['input_ids'])\n","        # Get prompt length for label masking\n","        prompt_encoding = self.tokenizer(full_prompt, add_special_tokens=True, return_tensors='pt')\n","        prompt_length = prompt_encoding['input_ids'].shape[1]\n","\n","        # Full sequence for training\n","        target_text = f\"{full_prompt} {item['output']}\"\n","        full_encoding = self.tokenizer(\n","            target_text,\n","            padding='max_length',\n","            max_length=self.max_full_length,\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","\n","        # Compute graph PE for SMILES only\n","        adj_matrix = smiles_to_adjacency_matrix(smiles)\n","        if adj_matrix is not None:\n","            graph_pe = compute_graph_positional_encoding(adj_matrix, k=30)\n","            smiles_token_pe = align_graph_pe_to_tokens(\n","                smiles, graph_pe, self.tokenizer, actual_smiles_length, random_seed=42\n","            )\n","        else:\n","            smiles_token_pe = torch.zeros(actual_smiles_length, 30)\n","\n","        # Create full-length PE tensor\n","        full_token_pe = torch.zeros(self.max_full_length, 30)\n","\n","        # Find where SMILES tokens are and place PEs there\n","        # This is approximate - you might need better alignment\n","        instruction_text = f\"{item['instruction']}\\nSMILES: \"\n","        instruction_tokens = self.tokenizer(instruction_text, add_special_tokens=True)\n","        smiles_start_idx = len(instruction_tokens['input_ids'])\n","\n","        # Copy SMILES PEs to correct position\n","        pe_end_idx = min(smiles_start_idx + actual_smiles_length, self.max_full_length)\n","\n","\n","        pe_length = min(smiles_token_pe.shape[0], pe_end_idx - smiles_start_idx)\n","        full_token_pe[smiles_start_idx:smiles_start_idx + pe_length] = smiles_token_pe[:pe_length]\n","        # Create labels with prompt masked\n","        labels = full_encoding['input_ids'][0].clone()\n","        labels[:prompt_length] = -100  # Mask prompt tokens\n","\n","        return {\n","            'input_ids': full_encoding['input_ids'][0],\n","            'graph_pes': full_token_pe,\n","            'labels': labels,\n","        }\n","\n","# First calculate max lengths\n","max_full_length = calculate_max_lengths('hiv_train.jsonl')\n","\n","# Then create dataloaders with those lengths\n","def create_dataloaders(train_path='hiv_train.jsonl', val_path='hiv_val.jsonl',\n","                      batch_size=8, tokenizer=None, max_full_length=max_full_length):\n","    if tokenizer is None:\n","        tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-160m\")\n","        if tokenizer.pad_token is None:\n","            tokenizer.pad_token = tokenizer.eos_token\n","\n","    train_dataset = HIVDataset(train_path, tokenizer, max_full_length)\n","    val_dataset = HIVDataset(val_path, tokenizer, max_full_length)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","    return train_loader, val_loader, tokenizer\n","\n","# Usage\n","train_loader, val_loader, tokenizer = create_dataloaders(batch_size=8)\n","\n","# Test one batch\n","sample_batch = next(iter(train_loader))\n","print(f\"Input IDs shape: {sample_batch['input_ids'].shape}\")\n","print(f\"Graph PEs shape: {sample_batch['graph_pes'].shape}\")\n","print(f\"Labels shape: {sample_batch['labels'].shape}\")"],"metadata":{"id":"41yISTqnPEyP","executionInfo":{"status":"ok","timestamp":1756551635375,"user_tz":-120,"elapsed":14851,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"colab":{"base_uri":"https://localhost:8080/","height":359,"referenced_widgets":["e65c27988e7f48199eac0d6ae1f05404","a15715f6e63b43ea894a6b53a44504a7","fb8031d7243a46cea6b59827dc146765","3b330acd103949e68071691b92e3afcb","7598e87f1f3f4d4c8cce818b68692755","2f36d48299714c35a7d31b5e702cc238","3e6dcda0df9343f0b161206feb201578","945daae38b214cddb32f6b586ee64b90","5bef6ef17b714b559fe107de50920adf","9ccc77060e604a41a0808bb0baa00ae1","1fa52c95709b4ff5bc8a2b7d84a5c166","c7e0b2494950481ab06aa036b84533bb","02c14f25ccc64ae8a07d3bd83e5caadb","a02c7919908949eda603dd1dc7226c29","f29d4b8128be4308a42f47e6958c1b96","5d6288193c9846bab82bdfbc5cb75f6c","64b96b296cb547b8be963cd8026ae77f","8d953e0271f14e0e94ae00a3c6b7ccf1","95c35f3357b54707adaa3b89b8ca8faa","a38ef3f7924c40b89bd41e6175cae1b5","9c1f03aa686944d19e0855e9e947593f","2339b0e5856a4c658c949c33b81e0eb8","61b0c94665454480a3d92e4813705bf4","e9b16f33744d4231a2ace3572cf335f1","f0a6eba1c99d4bba939a542751a7d7a1","39d68c36a46c4eec83fed0c7e2ec99cf","db66629459c4462fb26cb9d23f9ef644","b6b30ed854b44ce599384d981e71e70c","d8fd079708df4c4a8848b13af215f892","f177fa18dce64b088e95b4b8584faf31","3810f7e03c8a40689ecee8500d36030a","a8a76709f59f49dba183f17a33a331eb","c59a68af4ed24b8698e81bb6cd25b5f3","b10379e07a48465a966c237283429971","93e69106f3a147b0bd4a72a64e1dde8d","f7bc9b1ca2574ef9981216f87c78a788","0705b584ffe6448bbfeada31c7c83d5c","25577cd262d34d089c094ab6c88fc481","14fa6fd1b3ee4cceae285f4aa3772a90","5a560f16f92e484694aa4646282590f0","5793fb10e73342819feb6c037d08bcd5","70b399b2bdd94643b2957ecf68a8ff0d","164e2961d4354374ac9dc7c610f53dea","71231c11be1a4d4c8ad46d3757214b2e","9026e23a49504f3e8223341f55ddbf63","9d4bb8fb6845492d998797914d6e8683","c307187b2c00455281c46a5fe6ca76ce","a6c22c602a4c43aa9ce23eaaddb2b65c","5a89bcbe91b740dfab106f005a9f8446","b89f563a66dc43bc9d2ff21473ca6cd7","2ce338b426b845a392d9066b7a8261bd","95704dadeff648a0b13bf949c29511d6","8a524041456f4a538b374deff567d865","5b08634d7f214f1dbb0dadc2eee88aab","6f6c7dd030a9452cbe4deab01845b510","af6723deff6c42e4a7a2897f4a298b4d","a5c8b4d1ee834573b1aec71e3dbd60d9","e762fe10e3524bbc8846e76643ee879b","984effe7e62a49b0818f6de916ba5a95","a92ea24879c14a11b1791b37151a2dfd","f3df264139ab4f3d9f29e3b542a915df","1ffa12fc6deb4609985cfabb5b0ad610","5a58e0aca91c4f289d4d0d16a81de169","43792898c9894cbf942fa612f128df6c","f329d214e4a345a48fa5b7a8e5206cdb","182e07be04ee4a86991687040519cffc","09eceb7f9c0d46f495e5cb60df32fdfd","982b57dba0e14e82a88797b01c5c0d20","f32fa6a287fa4354a69bc0be94f5e1bb","b3517739baf24a39bc4a8dc71477f69e","b517737b018444a898d1a2d9a5cbde9d","253983c792c542559040f99cb548bb28","680a0a888adf4d78bbc64ade6bdd219a","ee57996e05154635a5dde08bf366a0b5","7c70a42efa2c4d099b44a70e24e34f6c","334a4aa96bf1413cbf626b92d569833e","d702cc90cc544e88a4d65ee38cc41c75","d8e0c673366c4edeab792ec2199ddd2d","58a2a2dc1e3847c1a992a9f2f2537394","7478c8f9f1cc42b6a226bb066b22a179","c3560de112f741e8bdf34f7d892fd94a","b9644d0a27d2440bb2b7981274607b20","5b9c9b952af64b24b6edcbdc30a96208","cb048f63bc6244e9911b942b107fb197","8165a03e5eb548f99a1f0b63f835d69a","68f72991fbb94f55aa2394027144b6dd","cbe321fbd28144b3a97269c3c33291ec","e9ddd04158b045479eeb7bfff0e6965b","c52f88651c7146cab7c49ccf8b0351bd","275ed6a1687f4f8e8a1284d87db98bb6","5200d2a6ccb949119542b84583ede8f9","0ab27d0284dd48fd89b44ea7699f1067","3aa05d2c980d493986f925d2820cb0bb","7f3505f0eebb482b952ad2329cf06a4a","8ea9faed1e5048268c54dc3dfce158b5","c04ca3ea83cc433ca0a26adb46848dba","190fb865c2294e578c8cbdb7311c4af2","e9c970fd2f8f49a1a74bcc044b3825cd","0842da4cdc1145ea96867869eec1fccd"]},"outputId":"5eed65ed-7af6-4b12-de98-5e0a6885e0a8"},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e65c27988e7f48199eac0d6ae1f05404"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7e0b2494950481ab06aa036b84533bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61b0c94665454480a3d92e4813705bf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b10379e07a48465a966c237283429971"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["added_tokens.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9026e23a49504f3e8223341f55ddbf63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af6723deff6c42e4a7a2897f4a298b4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09eceb7f9c0d46f495e5cb60df32fdfd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8e0c673366c4edeab792ec2199ddd2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c52f88651c7146cab7c49ccf8b0351bd"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Input IDs shape: torch.Size([8, 498])\n","Graph PEs shape: torch.Size([8, 498, 30])\n","Labels shape: torch.Size([8, 498])\n"]}]},{"cell_type":"code","source":["# Look at first example in batch\n","i = 0\n","print(f\"Input text (decoded): {tokenizer.decode(sample_batch['input_ids'][i])}\")\n","print(f\"\\nNon-zero PE positions: {torch.nonzero(sample_batch['graph_pes'][i].sum(dim=1)).squeeze().tolist()}\")\n","print(f\"\\nLabel tokens that aren't -100: {sample_batch['labels'][i][sample_batch['labels'][i] != -100].tolist()}\")"],"metadata":{"id":"CycDQXtCRHqC","executionInfo":{"status":"ok","timestamp":1756551635414,"user_tz":-120,"elapsed":36,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0d671f8a-d123-4373-850c-8eecf3e38da5"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Input text (decoded): Classify the following molecule based on its HIV activity. Respond with '1' if the molecule shows HIV activity, or '0' if it does not.\n","SMILES: Cc1c2c(n(CCc3ccccc3)c1C)NN=C(C#N)S2(=O)=O\n","Answer: 0<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n","\n","Non-zero PE positions: [38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]\n","\n","Label tokens that aren't -100: [470, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"]}]},{"cell_type":"code","source":["# %pip install peft"],"metadata":{"id":"XnBZRRW3lIzU","executionInfo":{"status":"ok","timestamp":1756551635418,"user_tz":-120,"elapsed":3,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from transformers import AutoModelForCausalLM, AutoTokenizer,get_linear_schedule_with_warmup\n","from peft import LoraConfig, get_peft_model, TaskType\n","from typing import Optional\n","\n","class HIVPEModule(nn.Module):\n","    def __init__(self, pe_dim=30, embed_dim=768):\n","        super().__init__()\n","        self.pe_projection = nn.Linear(pe_dim, embed_dim)\n","        self.activation = nn.GELU()\n","        self.dropout = nn.Dropout(0.7)\n","        # Better initialization for stability\n","        nn.init.xavier_uniform_(self.pe_projection.weight, gain=0.1)\n","        nn.init.zeros_(self.pe_projection.bias)\n","\n","        # Use a more reasonable scale that works with fp16\n","        self.scale = 0.1  #\n","\n","    def forward(self, embeddings, graph_pes):\n","        if graph_pes is not None:\n","            # Ensure proper dtype\n","            graph_pes = graph_pes.to(embeddings.dtype)\n","            projected_pes = self.activation(self.pe_projection(graph_pes))\n","            projected_pes = self.dropout(projected_pes)\n","            # Use learnable scale parameter for better gradient flow\n","            return embeddings + self.scale * projected_pes\n","        return embeddings\n","\n","class PythiaWithPE(nn.Module):\n","    \"\"\"Wrapper that adds PE to Pythia model\"\"\"\n","    def __init__(self, base_model, pe_dim=30):\n","        super().__init__()\n","        self.base_model = base_model\n","        self.embed_dim = base_model.gpt_neox.embed_in.embedding_dim\n","        self.pe_module = HIVPEModule(pe_dim=pe_dim, embed_dim=self.embed_dim)\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        graph_pes=None,\n","        attention_mask=None,\n","        labels=None,\n","        **kwargs\n","    ):\n","        # Get embeddings\n","        inputs_embeds = self.base_model.gpt_neox.embed_in(input_ids)\n","\n","        # Add PE with proper dtype handling\n","        if graph_pes is not None:\n","            inputs_embeds = self.pe_module(inputs_embeds, graph_pes)\n","\n","        # Forward through the model with embeddings\n","        outputs = self.base_model(\n","            inputs_embeds=inputs_embeds,\n","            attention_mask=attention_mask,\n","            labels=labels,\n","            **kwargs\n","        )\n","\n","        return outputs\n","\n","    def generate(self, input_ids, graph_pes=None, **kwargs):\n","        \"\"\"Generate with PE support\"\"\"\n","        # Get embeddings with PE for the prompt\n","        inputs_embeds = self.base_model.gpt_neox.embed_in(input_ids)\n","        if graph_pes is not None:\n","            inputs_embeds = self.pe_module(inputs_embeds, graph_pes)\n","\n","        # Generate using embeddings\n","        return self.base_model.generate(\n","            inputs_embeds=inputs_embeds,\n","            **kwargs\n","        )\n","\n","def setup_model_with_pe_and_lora(model_name=\"EleutherAI/pythia-160m\", pe_dim=30):\n","    \"\"\"\n","    Setup Pythia with custom PE and LoRA using PEFT\n","    \"\"\"\n","    print(f\"Loading {model_name}...\")\n","\n","    # Load base model - use float32 for training stability\n","    base_model = AutoModelForCausalLM.from_pretrained(\n","        model_name,\n","        torch_dtype=torch.float32,  # Use float32 for stability\n","        device_map=\"auto\"\n","    )\n","\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    if tokenizer.pad_token is None:\n","        tokenizer.pad_token = tokenizer.eos_token\n","\n","    # Wrap with PE\n","    model = PythiaWithPE(base_model, pe_dim=pe_dim)\n","\n","    # Configure LoRA with conservative settings\n","    lora_config = LoraConfig(\n","        task_type=TaskType.CAUSAL_LM,\n","        r=4,\n","        lora_alpha=8,\n","        lora_dropout=0.3,  # Increased dropout\n","        target_modules=[\n","            \"query_key_value\",\n","            \"dense_h_to_4h\",\n","            \"dense_4h_to_h\"\n","        ],\n","        # Initialize LoRA weights with smaller values\n","        init_lora_weights=\"gaussian\"\n","    )\n","\n","    # Apply LoRA to the base model\n","    model.base_model = get_peft_model(model.base_model, lora_config)\n","\n","    # Print trainable parameters\n","    total_params = sum(p.numel() for p in model.parameters())\n","    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(f\"✓ Total parameters: {total_params:,}\")\n","    print(f\"✓ Trainable parameters: {trainable_params:,}\")\n","    print(f\"✓ Trainable %: {100 * trainable_params / total_params:.2f}%\")\n","\n","    return model, tokenizer\n","\n","def train_with_focal_loss(\n","    model,\n","    train_loader,\n","    val_loader=None,\n","    epochs=3,\n","    learning_rate=1e-5,\n","    device=\"cuda\",\n","    tokenizer=None,save_checkpoint_at_batch=2000,  # Add this parameter\n","    checkpoint_path=\"checkpoint_batch_2000.pt\"  # Add this parameter\n","):\n","    \"\"\"Training loop with Focal Loss for extreme class imbalance\"\"\"\n","\n","    model = model.to(device)\n","\n","    # Focal loss parameters\n","    alpha = 0.965  # Weight for class 0 (should roughly match class distribution)\n","    gamma = 5.0   # Focusing parameter - increase this for more extreme imbalance\n","\n","    gradient_accumulation_steps = 2\n","    validation_frequency = 100  # Check validation every 2000 batches\n","\n","    optimizer = torch.optim.AdamW(\n","        model.parameters(),\n","        lr=learning_rate,\n","        weight_decay=0.3,\n","        eps=1e-6\n","    )\n","    num_training_steps = (len(train_loader) * epochs) // gradient_accumulation_steps\n","    scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=100,\n","    num_training_steps=num_training_steps)\n","    token_0 = 470  # \" 0\"\n","    token_1 = 337  # \" 1\"\n","\n","    scaler = torch.amp.GradScaler('cuda') if device == \"cuda\" else None\n","\n","    def focal_loss(logits, labels, alpha=0.1, gamma=5.0):\n","        \"\"\"\n","        Focal loss implementation\n","        alpha: weight for class 0 (majority class)\n","        1-alpha: weight for class 1 (minority class)\n","        \"\"\"\n","        # Get class probabilities\n","        probs = torch.softmax(logits, dim=-1)\n","\n","        # Get probability of true class\n","        ce_loss = nn.functional.cross_entropy(logits, labels, reduction='none')\n","        pt = torch.exp(-ce_loss)\n","\n","        # Apply class weights\n","        # For token_0 (majority), use alpha; for token_1 (minority), use 1-alpha\n","        alpha_t = torch.where(labels == token_0,  1 - alpha,alpha)\n","\n","        # Focal term: (1 - pt)^gamma reduces loss for well-classified examples\n","        focal_weight = alpha_t * (1 - pt) ** gamma\n","\n","        # Final focal loss\n","        focal_loss = focal_weight * ce_loss\n","\n","        return focal_loss.mean()\n","\n","    def run_validation(model, val_loader, max_batches=None):\n","        \"\"\"Run validation and return metrics\"\"\"\n","        model.eval()\n","        val_loss = 0\n","        val_class_0_correct = 0\n","        val_class_1_correct = 0\n","        val_class_0_total = 0\n","        val_class_1_total = 0\n","        val_batches = 0\n","\n","        with torch.no_grad():\n","            for i, batch in enumerate(val_loader):\n","                if max_batches and i >= max_batches:\n","                    break\n","\n","                input_ids = batch['input_ids'].to(device)\n","                graph_pes = batch['graph_pes'].to(device)\n","                labels = batch['labels'].to(device)\n","\n","                outputs = model(input_ids=input_ids, graph_pes=graph_pes)\n","                logits = outputs.logits\n","\n","                mask = ((labels == token_0) | (labels == token_1)) & (labels != -100)\n","\n","                if mask.any():\n","                    logits_flat = logits.view(-1, logits.size(-1))\n","                    labels_flat = labels.view(-1)\n","                    mask_flat = mask.view(-1)\n","\n","                    relevant_logits = logits_flat[mask_flat]\n","                    relevant_labels = labels_flat[mask_flat]\n","\n","                    loss = focal_loss(relevant_logits, relevant_labels, alpha=alpha, gamma=gamma)\n","\n","                    if not torch.isnan(loss):\n","                        val_loss += loss.item()\n","                        val_batches += 1\n","\n","                        preds = relevant_logits.argmax(dim=-1)\n","                        class_0_mask = relevant_labels == token_0\n","                        class_1_mask = relevant_labels == token_1\n","\n","                        if class_0_mask.any():\n","                            val_class_0_correct += (preds[class_0_mask] == relevant_labels[class_0_mask]).sum().item()\n","                            val_class_0_total += class_0_mask.sum().item()\n","\n","                        if class_1_mask.any():\n","                            val_class_1_correct += (preds[class_1_mask] == relevant_labels[class_1_mask]).sum().item()\n","                            val_class_1_total += class_1_mask.sum().item()\n","\n","        model.train()\n","        return {\n","            'loss': val_loss / val_batches if val_batches > 0 else float('inf'),\n","            'class_0_acc': val_class_0_correct / val_class_0_total if val_class_0_total > 0 else 0,\n","            'class_1_acc': val_class_1_correct / val_class_1_total if val_class_1_total > 0 else 0,\n","            'class_0_total': val_class_0_total,\n","            'class_1_total': val_class_1_total\n","        }\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        total_loss = 0\n","        valid_batches = 0\n","        class_0_correct = 0\n","        class_1_correct = 0\n","        class_0_total = 0\n","        class_1_total = 0\n","\n","        # Track predictions distribution\n","        pred_0_count = 0\n","        pred_1_count = 0\n","\n","        for batch_idx, batch in enumerate(train_loader):\n","            input_ids = batch['input_ids'].to(device)\n","            graph_pes = batch['graph_pes'].to(device)\n","            labels = batch['labels'].to(device)\n","            if batch_idx == save_checkpoint_at_batch:\n","                print(f\"\\nSaving checkpoint at batch {batch_idx}...\")\n","                checkpoint = {\n","                    'batch': batch_idx,\n","                    'epoch': epoch,\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'scheduler_state_dict': scheduler.state_dict(),\n","                    'scaler_state_dict': scaler.state_dict() if scaler else None,\n","                    'train_loss': total_loss / valid_batches if valid_batches > 0 else 0,\n","                    'class_0_acc': class_0_correct / class_0_total if class_0_total > 0 else 0,\n","                    'class_1_acc': class_1_correct / class_1_total if class_1_total > 0 else 0,\n","                }\n","                torch.save(checkpoint, checkpoint_path)\n","                print(f\"Checkpoint saved to {checkpoint_path}\")\n","            if scaler is not None:\n","                with torch.amp.autocast('cuda'):\n","                    outputs = model(input_ids=input_ids, graph_pes=graph_pes)\n","                    logits = outputs.logits\n","\n","                    if torch.isnan(logits).any():\n","                        print(f\"NaN detected at batch {batch_idx}, skipping...\")\n","                        optimizer.zero_grad()\n","                        continue\n","\n","                    # Your mask for 0/1 tokens\n","                    mask = ((labels == token_0) | (labels == token_1)) & (labels != -100)\n","\n","                    if mask.any():\n","                        logits_flat = logits.view(-1, logits.size(-1))\n","                        labels_flat = labels.view(-1)\n","                        mask_flat = mask.view(-1)\n","\n","                        relevant_logits = logits_flat[mask_flat]\n","                        relevant_labels = labels_flat[mask_flat]\n","\n","                        # Apply focal loss\n","                        loss = focal_loss(relevant_logits, relevant_labels, alpha=alpha, gamma=gamma)\n","                        loss = loss / gradient_accumulation_steps\n","\n","                        # Add L2 regularization on logits to prevent extreme predictions\n","                        logit_reg = 0.01 * (relevant_logits ** 2).mean()\n","                        loss = loss + logit_reg / gradient_accumulation_steps\n","\n","                        # Track predictions for monitoring\n","                        with torch.no_grad():\n","                            preds = relevant_logits.argmax(dim=-1)\n","                            pred_0_count += (preds == token_0).sum().item()\n","                            pred_1_count += (preds == token_1).sum().item()\n","\n","                            class_0_mask = relevant_labels == token_0\n","                            class_1_mask = relevant_labels == token_1\n","\n","                            if class_0_mask.any():\n","                                class_0_correct += (preds[class_0_mask] == relevant_labels[class_0_mask]).sum().item()\n","                                class_0_total += class_0_mask.sum().item()\n","\n","                            if class_1_mask.any():\n","                                class_1_correct += (preds[class_1_mask] == relevant_labels[class_1_mask]).sum().item()\n","                                class_1_total += class_1_mask.sum().item()\n","                    else:\n","                        continue\n","\n","                scaler.scale(loss).backward()\n","\n","                if (batch_idx + 1) % gradient_accumulation_steps == 0:\n","                    scaler.unscale_(optimizer)\n","\n","                    # Log gradient norm\n","                    if batch_idx % 50 == 0:\n","                        total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","                        print(f\"  Gradient norm: {total_norm:.4f}\")\n","                    else:\n","                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","                    scaler.step(optimizer)\n","                    scaler.update()\n","                    optimizer.zero_grad()\n","                    scheduler.step()\n","\n","            if mask.any() and not torch.isnan(loss):\n","                total_loss += loss.item() * gradient_accumulation_steps\n","                valid_batches += 1\n","\n","                if batch_idx % 10 == 0:\n","                    print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, \"\n","                          f\"Loss: {loss.item() * gradient_accumulation_steps:.4f}\")\n","\n","                    # Print detailed statistics every 50 batches\n","                    if batch_idx > 0 and batch_idx % 50 == 0:\n","                        print(f\"  Class 0 accuracy: {class_0_correct/class_0_total:.2%} ({class_0_correct}/{class_0_total})\")\n","                        if class_1_total > 0:\n","                            print(f\"  Class 1 accuracy: {class_1_correct/class_1_total:.2%} ({class_1_correct}/{class_1_total})\")\n","                        else:\n","                            print(f\"  Class 1 accuracy: No class 1 samples yet\")\n","                        print(f\"  Predictions distribution: {pred_0_count} zeros, {pred_1_count} ones\")\n","\n","                        # Reset prediction counters\n","                        pred_0_count = 0\n","                        pred_1_count = 0\n","\n","                # Periodic validation check\n","                if val_loader is not None and batch_idx > 0 and batch_idx % validation_frequency == 0:\n","                    print(f\"\\n--- Validation Check at Batch {batch_idx} ---\")\n","                    val_metrics = run_validation(model, val_loader, max_batches=100)  # Quick check on 100 batches\n","                    print(f\"Validation Loss: {val_metrics['loss']:.4f}\")\n","                    print(f\"Validation Class 0 accuracy: {val_metrics['class_0_acc']:.2%} \"\n","                          f\"({int(val_metrics['class_0_acc'] * val_metrics['class_0_total'])}/{val_metrics['class_0_total']})\")\n","                    if val_metrics['class_1_total'] > 0:\n","                        print(f\"Validation Class 1 accuracy: {val_metrics['class_1_acc']:.2%} \"\n","                              f\"({int(val_metrics['class_1_acc'] * val_metrics['class_1_total'])}/{val_metrics['class_1_total']})\")\n","                    print(\"--- End Validation Check ---\\n\")\n","\n","        # End of epoch summary\n","        print(f\"\\n{'='*50}\")\n","        print(f\"Epoch {epoch+1} Summary:\")\n","        print(f\"Average Loss: {total_loss/valid_batches:.4f}\")\n","        if class_0_total > 0:\n","            print(f\"Class 0 accuracy: {class_0_correct/class_0_total:.2%} ({class_0_correct}/{class_0_total})\")\n","        if class_1_total > 0:\n","            print(f\"Class 1 accuracy: {class_1_correct/class_1_total:.2%} ({class_1_correct}/{class_1_total})\")\n","        else:\n","            print(\"No class 1 samples in this epoch!\")\n","        print(f\"{'='*50}\\n\")\n","\n","        # Full validation at epoch end\n","        if val_loader is not None:\n","            print(\"Running full validation...\")\n","            val_metrics = run_validation(model, val_loader)  # Full validation\n","            print(f\"Full Validation Results:\")\n","            print(f\"  Loss: {val_metrics['loss']:.4f}\")\n","            print(f\"  Class 0 accuracy: {val_metrics['class_0_acc']:.2%}\")\n","            if val_metrics['class_1_total'] > 0:\n","                print(f\"  Class 1 accuracy: {val_metrics['class_1_acc']:.2%}\")\n","            print()\n","\n","    return model\n","\n","# Updated usage code\n","if __name__ == \"__main__\":\n","    # Setup model and tokenizer\n","    model, tokenizer = setup_model_with_pe_and_lora()\n","\n","    # Use the correct device and dtype setup\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    model = model.to(device)\n","\n","    # Don't convert to half precision manually - let autocast handle it\n","    # model.pe_module = model.pe_module.half()  # Remove this line\n","\n","    print(\"Model setup complete and ready for training!\")"],"metadata":{"id":"-vjqCGzkrLHn","executionInfo":{"status":"ok","timestamp":1756554044818,"user_tz":-120,"elapsed":1621,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"547ba67e-0ab9-41f6-efd1-d328c09ed8c2"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading EleutherAI/pythia-160m...\n","✓ Total parameters: 162,862,848\n","✓ Trainable parameters: 539,904\n","✓ Trainable %: 0.33%\n","Model setup complete and ready for training!\n"]}]},{"cell_type":"code","source":["import gc\n","\n","# Clear GPU memory\n","torch.cuda.empty_cache()\n","gc.collect()\n","# Setup model and tokenizer\n","model, tokenizer = setup_model_with_pe_and_lora()\n","\n","# Convert model to device\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = model.to(device)\n","\n","\n","# Use smaller batch size to ensure minority class representation\n","train_loader, val_loader, _ = create_dataloaders(\n","    batch_size=10,  # Reduced to ensure we see minority class\n","    tokenizer=tokenizer\n",")\n","\n","# Check if we actually see class 1 in the data\n","print(\"\\nChecking class 1 presence in first 10 batches:\")\n","class_1_count = 0\n","for i, batch in enumerate(train_loader):\n","    if i >= 10:\n","        break\n","    labels = batch['labels']\n","    if (labels == 337).any():\n","        class_1_count += 1\n","print(f\"Found class 1 in {class_1_count}/10 batches\")\n","\n","# Train with focal loss (use the new function)\n","trained_model = train_with_focal_loss(  # Changed from train_with_pe\n","    model,\n","    train_loader,\n","    val_loader,\n","    epochs=3,\n","    learning_rate=1e-4,\n","    device=device,\n","    tokenizer=tokenizer,save_checkpoint_at_batch=2000,checkpoint_path=\"checkpoint_batch_2000.pt\"\n",")"],"metadata":{"id":"l_BbuEEEtfPa","executionInfo":{"status":"error","timestamp":1756554208430,"user_tz":-120,"elapsed":158510,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"b58e4b07-0c4d-46aa-a136-feaa69663ca4"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading EleutherAI/pythia-160m...\n","✓ Total parameters: 162,862,848\n","✓ Trainable parameters: 539,904\n","✓ Trainable %: 0.33%\n","\n","Checking class 1 presence in first 10 batches:\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1890492291.py:49: RuntimeWarning: k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.\n","  eigenvalues, eigenvectors = eigsh(laplacian, k=k, which='SM')\n"]},{"output_type":"stream","name":"stdout","text":["Found class 1 in 2/10 batches\n","Epoch 1/3, Batch 0/3290, Loss: 6617.5273\n","Epoch 1/3, Batch 10/3290, Loss: 6616.7808\n","Epoch 1/3, Batch 20/3290, Loss: 6619.5366\n","Epoch 1/3, Batch 30/3290, Loss: 6546.2256\n","Epoch 1/3, Batch 40/3290, Loss: 6390.8159\n","Epoch 1/3, Batch 50/3290, Loss: 6177.0127\n","  Class 0 accuracy: 0.00% (0/488)\n","  Class 1 accuracy: 0.00% (0/22)\n","  Predictions distribution: 0 zeros, 0 ones\n","Epoch 1/3, Batch 60/3290, Loss: 5345.1816\n","Epoch 1/3, Batch 70/3290, Loss: 1916.9429\n","Epoch 1/3, Batch 80/3290, Loss: 4.5985\n","Epoch 1/3, Batch 90/3290, Loss: 7.0761\n","Epoch 1/3, Batch 100/3290, Loss: 8.0037\n","  Class 0 accuracy: 0.00% (0/973)\n","  Class 1 accuracy: 0.00% (0/37)\n","  Predictions distribution: 0 zeros, 0 ones\n","\n","--- Validation Check at Batch 100 ---\n","Validation Loss: 1.1850\n","Validation Class 0 accuracy: 0.00% (0/967)\n","Validation Class 1 accuracy: 0.00% (0/33)\n","--- End Validation Check ---\n","\n","Epoch 1/3, Batch 110/3290, Loss: 1.4510\n","Epoch 1/3, Batch 120/3290, Loss: 5.5908\n","Epoch 1/3, Batch 130/3290, Loss: 3.8695\n","Epoch 1/3, Batch 140/3290, Loss: 3.1389\n","Epoch 1/3, Batch 150/3290, Loss: 1.6456\n","  Class 0 accuracy: 5.78% (84/1453)\n","  Class 1 accuracy: 0.00% (0/57)\n","  Predictions distribution: 85 zeros, 0 ones\n","Epoch 1/3, Batch 160/3290, Loss: 1.7606\n","Epoch 1/3, Batch 170/3290, Loss: 1.4001\n","Epoch 1/3, Batch 180/3290, Loss: 3.5409\n","Epoch 1/3, Batch 190/3290, Loss: 0.8630\n","Epoch 1/3, Batch 200/3290, Loss: 0.9859\n","  Class 0 accuracy: 25.81% (500/1937)\n","  Class 1 accuracy: 9.59% (7/73)\n","  Predictions distribution: 424 zeros, 62 ones\n","\n","--- Validation Check at Batch 200 ---\n","Validation Loss: 0.0008\n","Validation Class 0 accuracy: 81.70% (790/967)\n","Validation Class 1 accuracy: 100.00% (33/33)\n","--- End Validation Check ---\n","\n","Epoch 1/3, Batch 210/3290, Loss: 1.0398\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-296110432.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Train with focal loss (use the new function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m trained_model = train_with_focal_loss(  # Changed from train_with_pe\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-752376880.py\u001b[0m in \u001b[0;36mtrain_with_focal_loss\u001b[0;34m(model, train_loader, val_loader, epochs, learning_rate, device, tokenizer, save_checkpoint_at_batch, checkpoint_path)\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_pes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph_pes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m                     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-752376880.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, graph_pes, attention_mask, labels, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Forward through the model with embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         outputs = self.base_model(\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1848\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1850\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1851\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m         ```\"\"\"\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         outputs: BaseModelOutputWithPast = self.gpt_neox(\n\u001b[0m\u001b[1;32m    578\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m                         \u001b[0mmonkey_patched_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Restore original forward methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_forward\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmonkey_patched_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_cache\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpast_key_values\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0mpast_key_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDynamicCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcache_position\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/cache_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ddp_cache_data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;31m# Specialized constructor for DDP cache data, needed for BC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddp_cache_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDynamicLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m         \u001b[0;31m# `ddp_cache_data` was originally added for compatibility with `torch.distributed` (DDP). See #36212\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m         \u001b[0;31m# and #36373 for more information. In a nutshell, it is `map(gather_map, zip(*caches))`, i.e. each item in the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/cache_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layer_classes, config, cache_processor, max_batch_size, max_cache_len, device, dtype, layer_device_map, tp_size, **kwargs)\u001b[0m\n\u001b[1;32m   1105\u001b[0m         \u001b[0mprocessor_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_processor_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_init_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_layer_args_from_model_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1108\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"num_hidden_layers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/cache_utils.py\u001b[0m in \u001b[0;36mparse_layer_args_from_model_config\u001b[0;34m(config, batch_size, max_cache_len, device, dtype, layer_device_map, tp_size, max_batch_size)\u001b[0m\n\u001b[1;32m   1906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1908\u001b[0;31m def parse_layer_args_from_model_config(\n\u001b[0m\u001b[1;32m   1909\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}