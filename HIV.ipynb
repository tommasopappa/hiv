{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm","authorship_tag":"ABX9TyOwUK9L7vDEeiapnz2hC1Og"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"8952b851d8664ef1a4e522a9420c24e2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_856ff9f3159c4d2bb76b031662b7221c","IPY_MODEL_f573b06deaae49bda6a6ae5d520cb587","IPY_MODEL_c1a0ea07f6624d808f263c4a02110f91"],"layout":"IPY_MODEL_4c6ef2b3493348119ac63023a6ca96fa"}},"856ff9f3159c4d2bb76b031662b7221c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7859906d7cb048dc87e90bd6660e83e5","placeholder":"​","style":"IPY_MODEL_e76e1fe7eae240b88f8602f089f41c77","value":"tokenizer_config.json: 100%"}},"f573b06deaae49bda6a6ae5d520cb587":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_df61e5dd0e124434a8f369a30c6be32b","max":166,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0845f73653314c0ba803e39f42837ee8","value":166}},"c1a0ea07f6624d808f263c4a02110f91":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f2eb751b4b34a0f80c53b67f78a1496","placeholder":"​","style":"IPY_MODEL_327faf2943874b18bd08790f4b443e8c","value":" 166/166 [00:00&lt;00:00, 15.4kB/s]"}},"4c6ef2b3493348119ac63023a6ca96fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7859906d7cb048dc87e90bd6660e83e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e76e1fe7eae240b88f8602f089f41c77":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df61e5dd0e124434a8f369a30c6be32b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0845f73653314c0ba803e39f42837ee8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5f2eb751b4b34a0f80c53b67f78a1496":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"327faf2943874b18bd08790f4b443e8c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c893b9181894dbb832d6d749f50ce1d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a8f2e1d4fc2d4ab2a0a12f7c66d6aea2","IPY_MODEL_2685a0a6b210419caf4d28a6172cb454","IPY_MODEL_3dbb03b3fb434a5d8775a600124896fa"],"layout":"IPY_MODEL_76d8f5d2724b4b929c053d35ca0fe200"}},"a8f2e1d4fc2d4ab2a0a12f7c66d6aea2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_289ee9d38a8747e98472cebdcbff5b07","placeholder":"​","style":"IPY_MODEL_99896e8174cb4a1cb4a84efd196b607b","value":"config.json: 100%"}},"2685a0a6b210419caf4d28a6172cb454":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_41bc2b0682a14a43aceb1a6e5f537d9a","max":501,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cfebcfa5435349f5b83ad02b6401a647","value":501}},"3dbb03b3fb434a5d8775a600124896fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_213c00383fd44221a2d235901d75d743","placeholder":"​","style":"IPY_MODEL_30c41bac74974e2e8a49b6cd56584ef2","value":" 501/501 [00:00&lt;00:00, 53.9kB/s]"}},"76d8f5d2724b4b929c053d35ca0fe200":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"289ee9d38a8747e98472cebdcbff5b07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99896e8174cb4a1cb4a84efd196b607b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41bc2b0682a14a43aceb1a6e5f537d9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfebcfa5435349f5b83ad02b6401a647":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"213c00383fd44221a2d235901d75d743":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30c41bac74974e2e8a49b6cd56584ef2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c0d9314b8c4e4edf9aca2dafab56a071":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0fdd35deba5649b2a6fa13aa530f7135","IPY_MODEL_25926a84ec4447258735488cdef946c8","IPY_MODEL_5d83c8abf66e45ffae3479a2a4356ea9"],"layout":"IPY_MODEL_74343dd711bf402d8dc97cff42bdc5c2"}},"0fdd35deba5649b2a6fa13aa530f7135":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_779cd53035274f9f9713b0e94a8f5dfe","placeholder":"​","style":"IPY_MODEL_d8a9981d12224b9a8a3f5840ca4e644e","value":"vocab.json: "}},"25926a84ec4447258735488cdef946c8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2c2b3df225e4046b164c8504c398440","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3b31134d499c4a198f8d7ff20cc46e11","value":1}},"5d83c8abf66e45ffae3479a2a4356ea9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a6e0c37dad242b89f645cf446e23f5a","placeholder":"​","style":"IPY_MODEL_3e9041f8cd324923a0aabe1f852d834b","value":" 9.43k/? [00:00&lt;00:00, 1.04MB/s]"}},"74343dd711bf402d8dc97cff42bdc5c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"779cd53035274f9f9713b0e94a8f5dfe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8a9981d12224b9a8a3f5840ca4e644e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2c2b3df225e4046b164c8504c398440":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"3b31134d499c4a198f8d7ff20cc46e11":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a6e0c37dad242b89f645cf446e23f5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e9041f8cd324923a0aabe1f852d834b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae0a255fd11e43848daac4f5a99a7960":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_664d5b44ccea42389399bcb4a507205d","IPY_MODEL_5af3c7ac3cc144eda0d02005515231dd","IPY_MODEL_99fbdcdcadd347aba753d63f59131b44"],"layout":"IPY_MODEL_ef8aaa0c1af441c48b7767d3282ff111"}},"664d5b44ccea42389399bcb4a507205d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_626858613c3c4cd88a140df12c583562","placeholder":"​","style":"IPY_MODEL_5f6e67c39a364c9083cd36503d98fbc3","value":"merges.txt: "}},"5af3c7ac3cc144eda0d02005515231dd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a105951151548cfb1498c6d773a1d28","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a989cf3bf1a14dc0877d7265365841c7","value":1}},"99fbdcdcadd347aba753d63f59131b44":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dcbea6841eb6476c8a0e712da7a381bd","placeholder":"​","style":"IPY_MODEL_6ddb90bd4fcd43669d40c4e45c9dca43","value":" 3.21k/? [00:00&lt;00:00, 256kB/s]"}},"ef8aaa0c1af441c48b7767d3282ff111":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"626858613c3c4cd88a140df12c583562":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f6e67c39a364c9083cd36503d98fbc3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9a105951151548cfb1498c6d773a1d28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"a989cf3bf1a14dc0877d7265365841c7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dcbea6841eb6476c8a0e712da7a381bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ddb90bd4fcd43669d40c4e45c9dca43":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a89ec5b742784c4db148d9d1dd61a027":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7bc3f037740f43ccb8e40a0920c08fab","IPY_MODEL_7cedddde89eb42f5a780e9a852b17caf","IPY_MODEL_99773cff7ef642ab87e81a5b7dc7111e"],"layout":"IPY_MODEL_c8ba4efb9c654504827661a99771218b"}},"7bc3f037740f43ccb8e40a0920c08fab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ad526b8d06a414fab6eb200cb421023","placeholder":"​","style":"IPY_MODEL_762f70a7c9a340d1a023836ca2befc60","value":"special_tokens_map.json: 100%"}},"7cedddde89eb42f5a780e9a852b17caf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_870f22529a064c78b467cdee16ad57e9","max":150,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0b8539fe78844f8abc58386ac0a57818","value":150}},"99773cff7ef642ab87e81a5b7dc7111e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7cd599f5743d44d3a640502467c512f6","placeholder":"​","style":"IPY_MODEL_0ff1d96ebf0d428fb93dda056131a870","value":" 150/150 [00:00&lt;00:00, 13.9kB/s]"}},"c8ba4efb9c654504827661a99771218b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ad526b8d06a414fab6eb200cb421023":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"762f70a7c9a340d1a023836ca2befc60":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"870f22529a064c78b467cdee16ad57e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b8539fe78844f8abc58386ac0a57818":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7cd599f5743d44d3a640502467c512f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ff1d96ebf0d428fb93dda056131a870":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2c4a386b3fdd4e0e96ced62caad356dc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ef08e4118d8f4f4a80eb865191151537","IPY_MODEL_1a2e8b0b4ff24a61892eb0b5c09f840d","IPY_MODEL_6b2170d1305b46ce83edaf4fb4f6b825"],"layout":"IPY_MODEL_1df8e225dc86445185e5da39782d417a"}},"ef08e4118d8f4f4a80eb865191151537":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0674e3002f5484e8ff980d049de601b","placeholder":"​","style":"IPY_MODEL_67e89d7b27fe448c961e08d19f90879e","value":"tokenizer_config.json: "}},"1a2e8b0b4ff24a61892eb0b5c09f840d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_56ade494d0624fe0b9790c1c3ee50f74","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fcd735d4fdf047fcba5f00764bbb7f00","value":1}},"6b2170d1305b46ce83edaf4fb4f6b825":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_37c9d67dc069402eb24b9c3755ac9491","placeholder":"​","style":"IPY_MODEL_1e52206cfa3941e2abb2e6283353e9ba","value":" 7.34k/? [00:00&lt;00:00, 587kB/s]"}},"1df8e225dc86445185e5da39782d417a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0674e3002f5484e8ff980d049de601b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67e89d7b27fe448c961e08d19f90879e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"56ade494d0624fe0b9790c1c3ee50f74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"fcd735d4fdf047fcba5f00764bbb7f00":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"37c9d67dc069402eb24b9c3755ac9491":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e52206cfa3941e2abb2e6283353e9ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3379ded465c844b69d0b409ac2d95400":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_47247f0f67444c92927a9fd9349b6a07","IPY_MODEL_0da396ab54734a2899a9c12999abb3e4","IPY_MODEL_e885f87f22f043b4833c3075ffe3f241"],"layout":"IPY_MODEL_de98f4ed9b3a4406a75dacb5c83e6f34"}},"47247f0f67444c92927a9fd9349b6a07":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1964a9d785a04e3b8ecd23a2bce88421","placeholder":"​","style":"IPY_MODEL_f9bd8bdb936045e482c0b77d9677dbde","value":"vocab.json: "}},"0da396ab54734a2899a9c12999abb3e4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_69cad0a5b34249d4b9006f7a91c718db","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c8e0b86ba052422bb7fd739a1125aa23","value":1}},"e885f87f22f043b4833c3075ffe3f241":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c4625ed99b5400bb338dbbda2bed025","placeholder":"​","style":"IPY_MODEL_dcfacb2bc3ca42fda2b3ca939c664de3","value":" 798k/? [00:00&lt;00:00, 22.0MB/s]"}},"de98f4ed9b3a4406a75dacb5c83e6f34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1964a9d785a04e3b8ecd23a2bce88421":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9bd8bdb936045e482c0b77d9677dbde":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"69cad0a5b34249d4b9006f7a91c718db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"c8e0b86ba052422bb7fd739a1125aa23":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5c4625ed99b5400bb338dbbda2bed025":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dcfacb2bc3ca42fda2b3ca939c664de3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cfe94a706481451ba2f48a27e987596c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_77be68c0fde642f89b2eb37b3f3d84db","IPY_MODEL_9d844edafb204257a28668f51bbfecb4","IPY_MODEL_3927b477bc23424bbd83443de35704f4"],"layout":"IPY_MODEL_32ee3fd575184ddda4e97cec4cea2181"}},"77be68c0fde642f89b2eb37b3f3d84db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c964c9322ab45e6b9e318b53a94bbde","placeholder":"​","style":"IPY_MODEL_e110988db12746bc9f5c613a7e9b728a","value":"merges.txt: "}},"9d844edafb204257a28668f51bbfecb4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d2d5403cf1b4e1b94c30620bd8bedd1","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f0aedbe13d064e0985ea8a682cb0672a","value":1}},"3927b477bc23424bbd83443de35704f4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf376c379d2545b0be5559b1227b0756","placeholder":"​","style":"IPY_MODEL_91cc3c31331a4cfea8466886a957c843","value":" 456k/? [00:00&lt;00:00, 24.5MB/s]"}},"32ee3fd575184ddda4e97cec4cea2181":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c964c9322ab45e6b9e318b53a94bbde":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e110988db12746bc9f5c613a7e9b728a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7d2d5403cf1b4e1b94c30620bd8bedd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"f0aedbe13d064e0985ea8a682cb0672a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cf376c379d2545b0be5559b1227b0756":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91cc3c31331a4cfea8466886a957c843":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb572a327cf94ed48288177d6d6dc793":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cb3518665f594fa8839b707e6dbda4bf","IPY_MODEL_761a64dfe0fd41e18a25dd662c98c66e","IPY_MODEL_d21b5f0a02d649b7bbd092516288267a"],"layout":"IPY_MODEL_36c8837e1c404ebc8eb94b7a75ed0d64"}},"cb3518665f594fa8839b707e6dbda4bf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8d70e970d2447868f9d999e1fabcf78","placeholder":"​","style":"IPY_MODEL_e9ddf17414ae403f9cbc5c572f7efc2f","value":"tokenizer.json: "}},"761a64dfe0fd41e18a25dd662c98c66e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd5c74a6a21f4d22aa39fd66e23109d0","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_adb315cb20c64b4d9370986d6319dea1","value":1}},"d21b5f0a02d649b7bbd092516288267a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d67edab1bb114adfbf5892d68992c5b7","placeholder":"​","style":"IPY_MODEL_6b1e505566084f3cae169b4bf31b4211","value":" 2.11M/? [00:00&lt;00:00, 51.4MB/s]"}},"36c8837e1c404ebc8eb94b7a75ed0d64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8d70e970d2447868f9d999e1fabcf78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9ddf17414ae403f9cbc5c572f7efc2f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dd5c74a6a21f4d22aa39fd66e23109d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"adb315cb20c64b4d9370986d6319dea1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d67edab1bb114adfbf5892d68992c5b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b1e505566084f3cae169b4bf31b4211":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"97798f3df77f498caeffb04baecd907d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2f45df2c132741a2b7cb6e935e80cfbf","IPY_MODEL_9eadee8d697643fdb0f1a138f4d4a9ee","IPY_MODEL_62fbea52da1b4194aedeba2b885d257e"],"layout":"IPY_MODEL_88e93e60ca564dd6a3327b37dd289ef1"}},"2f45df2c132741a2b7cb6e935e80cfbf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e291eb0497974b13a36cfb9fd3f9734c","placeholder":"​","style":"IPY_MODEL_0e4607cccdc1458a81b1c230496e6afb","value":"added_tokens.json: "}},"9eadee8d697643fdb0f1a138f4d4a9ee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_08fdee2f337b4d2c9c76bc97ae15e579","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_833bd5439a794713bf6f2ada4b258178","value":1}},"62fbea52da1b4194aedeba2b885d257e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_476cec65d01841c7b0782bd641b2127e","placeholder":"​","style":"IPY_MODEL_c8f5f8400ac948189c02a007131c2ed3","value":" 1.08k/? [00:00&lt;00:00, 49.3kB/s]"}},"88e93e60ca564dd6a3327b37dd289ef1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e291eb0497974b13a36cfb9fd3f9734c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e4607cccdc1458a81b1c230496e6afb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"08fdee2f337b4d2c9c76bc97ae15e579":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"833bd5439a794713bf6f2ada4b258178":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"476cec65d01841c7b0782bd641b2127e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8f5f8400ac948189c02a007131c2ed3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"84d2292eeac645ba9cb6cdeb5ad499f7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2229b27249e44d69a3555904fdaadad6","IPY_MODEL_636c434f17ca4ed3aad330acc94ee08d","IPY_MODEL_c37b264aeab94f429636c4be5ae632f6"],"layout":"IPY_MODEL_729e53e4b1194e9c991e8cbb7fbd1606"}},"2229b27249e44d69a3555904fdaadad6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d8ce0691b33c412ca43b77bf9dc0096e","placeholder":"​","style":"IPY_MODEL_44a0b9b2d76b4e77bd030c9c14581e5c","value":"special_tokens_map.json: 100%"}},"636c434f17ca4ed3aad330acc94ee08d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa87151117ac4f059b1af9ef125afb51","max":99,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7e05a3c7e9bb452881934ab6f4b65938","value":99}},"c37b264aeab94f429636c4be5ae632f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_15c7bf3b0c844f9a90ab29459e143917","placeholder":"​","style":"IPY_MODEL_0b41575685d54d16b1c7643e582c3721","value":" 99.0/99.0 [00:00&lt;00:00, 12.1kB/s]"}},"729e53e4b1194e9c991e8cbb7fbd1606":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8ce0691b33c412ca43b77bf9dc0096e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44a0b9b2d76b4e77bd030c9c14581e5c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fa87151117ac4f059b1af9ef125afb51":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e05a3c7e9bb452881934ab6f4b65938":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"15c7bf3b0c844f9a90ab29459e143917":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b41575685d54d16b1c7643e582c3721":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4db68a98e96943d5ae8a2485a9a7c21b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f282fe3b73b744e181bb1059c7688455","IPY_MODEL_e36d18229e204e3e8e72f5a6b283e0ab","IPY_MODEL_d7b2bd03c0524231b3d909724506a87c"],"layout":"IPY_MODEL_638636edcc6740f2b52b9978b380d373"}},"f282fe3b73b744e181bb1059c7688455":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e62aa1ed7a7488ea41e2cce432f83c2","placeholder":"​","style":"IPY_MODEL_b1af74e606754e37912396145f5cd735","value":"tokenizer_config.json: 100%"}},"e36d18229e204e3e8e72f5a6b283e0ab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ebb0c795a2d14138b7458c47f1facf5b","max":396,"min":0,"orientation":"horizontal","style":"IPY_MODEL_00e5344d07cb44ac93b30b0874787b6f","value":396}},"d7b2bd03c0524231b3d909724506a87c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0682dff539584904a9dc521827463614","placeholder":"​","style":"IPY_MODEL_ebc9bb49c41d484daff8ee3a105f703f","value":" 396/396 [00:00&lt;00:00, 47.7kB/s]"}},"638636edcc6740f2b52b9978b380d373":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e62aa1ed7a7488ea41e2cce432f83c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1af74e606754e37912396145f5cd735":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ebb0c795a2d14138b7458c47f1facf5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00e5344d07cb44ac93b30b0874787b6f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0682dff539584904a9dc521827463614":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebc9bb49c41d484daff8ee3a105f703f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc55225cfe8742c4a93969b0a6f502c5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2c64796d3a634632a5d1bc62c2765f41","IPY_MODEL_aaf204c345504a08aa412fafaf02d4a9","IPY_MODEL_db6547b999e8420cb543e20a401553c0"],"layout":"IPY_MODEL_0538ceb6e7e8441fa4a57ba64595bcd5"}},"2c64796d3a634632a5d1bc62c2765f41":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c89eea7e4884844a5abe163a8427e13","placeholder":"​","style":"IPY_MODEL_e76b5cb37f504129ab942d79348bfc22","value":"tokenizer.json: "}},"aaf204c345504a08aa412fafaf02d4a9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbc95009f9de4cc8bece8372ae83de01","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cce1a720619e41dd948d4256d87b4790","value":1}},"db6547b999e8420cb543e20a401553c0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_475e2b5d699d4d6c956f0d15f99b79aa","placeholder":"​","style":"IPY_MODEL_62f2ae59c0d54e61b43b998cca321444","value":" 2.11M/? [00:00&lt;00:00, 92.0MB/s]"}},"0538ceb6e7e8441fa4a57ba64595bcd5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c89eea7e4884844a5abe163a8427e13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e76b5cb37f504129ab942d79348bfc22":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dbc95009f9de4cc8bece8372ae83de01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"cce1a720619e41dd948d4256d87b4790":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"475e2b5d699d4d6c956f0d15f99b79aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62f2ae59c0d54e61b43b998cca321444":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"597ac6888e2d4e058a00f8c10966c1d6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bd2e9aff41e74cd8b99a5171889fc565","IPY_MODEL_5f0e24d7ba9547d1acbe5fdfb9fedf74","IPY_MODEL_6e05bb59f5f5429f8d0fefaac3f0695c"],"layout":"IPY_MODEL_f3906a775de54098a5aec3031a66fa31"}},"bd2e9aff41e74cd8b99a5171889fc565":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18328c65634a4e45994b89413043c9a4","placeholder":"​","style":"IPY_MODEL_30bcdf620ef04e398e069f0876ab68dd","value":"special_tokens_map.json: 100%"}},"5f0e24d7ba9547d1acbe5fdfb9fedf74":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba24f20a44e2499bad5d9d547000e807","max":99,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f145bec92b90461c94d08fc12de7a9c5","value":99}},"6e05bb59f5f5429f8d0fefaac3f0695c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d32f0fcb16254c029d112a6821cb6d2d","placeholder":"​","style":"IPY_MODEL_df5bc498d73e41b8a4e6366421798fb0","value":" 99.0/99.0 [00:00&lt;00:00, 13.4kB/s]"}},"f3906a775de54098a5aec3031a66fa31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18328c65634a4e45994b89413043c9a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30bcdf620ef04e398e069f0876ab68dd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ba24f20a44e2499bad5d9d547000e807":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f145bec92b90461c94d08fc12de7a9c5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d32f0fcb16254c029d112a6821cb6d2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df5bc498d73e41b8a4e6366421798fb0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# Install only what's needed\n","!pip install torch-geometric -q\n","!pip install rdkit\n","import torch\n","from torch.utils.data import IterableDataset\n","from torch_geometric.datasets import MoleculeNet\n","import pandas as pd\n","import itertools\n","import sys\n","from google.colab import drive\n","import os\n","\n","gdrive_path='/content/gdrive/MyDrive/Project_HIV'\n","\n","# This will mount your google drive under 'MyDrive'\n","drive.mount('/content/gdrive', force_remount=True)\n","# In order to access the files in this notebook we have to navigate to the correct folder\n","os.chdir(gdrive_path)\n","# Check manually if all files are present\n","print(sorted(os.listdir()))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HBpIix3WkoOg","executionInfo":{"status":"ok","timestamp":1756562884405,"user_tz":-120,"elapsed":104803,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"outputId":"49e4a5a7-6773-4f16-d374-d34869bffb1c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting rdkit\n","  Downloading rdkit-2025.3.5-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rdkit) (2.0.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rdkit) (11.3.0)\n","Downloading rdkit-2025.3.5-cp312-cp312-manylinux_2_28_x86_64.whl (36.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.2/36.2 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: rdkit\n","Successfully installed rdkit-2025.3.5\n","Mounted at /content/gdrive\n","['.git', '.gitignore', 'HIV.ipynb', 'README.md', '__pycache__', 'checkpoint_batch_2000.pt', 'code', 'data_import.py', 'datasets', 'hiv_train.jsonl', 'hiv_val.jsonl', 'pythia_pe_peft.py', 'utils.py']\n"]}]},{"cell_type":"code","source":["# Add code directory to path\n","from torch_geometric.datasets import MoleculeNet\n","# sys.path.insert(0, os.path.join(os.getcwd(), 'code'))\n","\n","# # Import the Colab-friendly version\n","# from data.dataset_colab import PartialHIVDataset\n","\n","class PartialHIVDataset(IterableDataset):\n","    \"\"\"\n","    IterableDataset that loads only a small portion of the HIV dataset.\n","    \"\"\"\n","\n","    def __init__(self, root='/tmp/HIV', max_samples=10):\n","        \"\"\"\n","        Args:\n","            root: Root directory for dataset storage\n","            max_samples: Maximum number of samples to load\n","        \"\"\"\n","        self.root = root\n","        self.max_samples = max_samples\n","        self._dataset = None\n","\n","    def _lazy_load_dataset(self):\n","        \"\"\"Lazy load the dataset only when iteration begins\"\"\"\n","        if self._dataset is None:\n","            print(f\"Initializing HIV dataset (will only load {self.max_samples} samples)...\")\n","            # Import here to ensure it's available\n","\n","            self._dataset = MoleculeNet(root=self.root, name='HIV')\n","            print(f\"Dataset ready! Total size: {len(self._dataset)} molecules\")\n","            print(f\"But we'll only load {self.max_samples} of them.\\n\")\n","\n","    def parse_molecules(self):\n","        \"\"\"\n","        Parse molecules from the dataset, stopping after max_samples.\n","        \"\"\"\n","        self._lazy_load_dataset()\n","\n","        for i in range(min(self.max_samples, len(self._dataset))):\n","            data = self._dataset[i]\n","\n","            # Extract molecule information\n","            mol_info = {\n","                'source': f\"molecule_{i}\",  # Similar to your CustomIterableDataset\n","                'target': data.y.item(),    # HIV activity label\n","                'num_atoms': data.num_nodes,\n","                'num_bonds': data.num_edges // 2,  # Undirected edges\n","                'smiles': getattr(data, 'smiles', 'N/A')\n","            }\n","\n","            yield mol_info\n","\n","    def __iter__(self):\n","        \"\"\"Iterator with worker support\"\"\"\n","        iterator = self.parse_molecules()\n","        worker_info = torch.utils.data.get_worker_info()\n","\n","        if worker_info is not None:\n","            worker_total_num = worker_info.num_workers\n","            worker_id = worker_info.id\n","            return itertools.islice(iterator, worker_id, None, worker_total_num)\n","\n","        return iterator\n","\n","\n","# Your code\n","batch_list = []\n","batch_size = 40\n","\n","# Create dataset\n","dataset = PartialHIVDataset(max_samples=batch_size)\n","iterator = iter(dataset)\n","full_batch_data = list(dataset)\n","\n","df = pd.DataFrame(full_batch_data)"],"metadata":{"id":"uCZHJO55qvlC","executionInfo":{"status":"ok","timestamp":1756562928710,"user_tz":-120,"elapsed":44291,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"cb84bc88-f7cf-42c1-cb8c-d0bbed487cdb"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing HIV dataset (will only load 40 samples)...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/HIV.csv\n","Processing...\n","/usr/local/lib/python3.12/dist-packages/torch_geometric/datasets/molecule_net.py:213: UserWarning: Skipping molecule 'O=C1O[Al]23(OC1=O)(OC(=O)C(=O)O2)OC(=O)C(=O)O3' since it resulted in zero atoms\n","  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n","/usr/local/lib/python3.12/dist-packages/torch_geometric/datasets/molecule_net.py:213: UserWarning: Skipping molecule 'Cc1ccc([B-2]2(c3ccc(C)cc3)=NCCO2)cc1' since it resulted in zero atoms\n","  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n","/usr/local/lib/python3.12/dist-packages/torch_geometric/datasets/molecule_net.py:213: UserWarning: Skipping molecule 'Oc1ccc(C2Oc3cc(O)cc4c3C(=[O+][AlH3-3]35([O+]=C6c7c(cc(O)cc7[OH+]3)OC(c3ccc(O)cc3O)C6O)([O+]=C3c6c(cc(O)cc6[OH+]5)OC(c5ccc(O)cc5O)C3O)[OH+]4)C2O)c(O)c1' since it resulted in zero atoms\n","  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n","/usr/local/lib/python3.12/dist-packages/torch_geometric/datasets/molecule_net.py:213: UserWarning: Skipping molecule 'CC1=C2[OH+][AlH3-3]34([O+]=C2C=CN1C)([O+]=C1C=CN(C)C(C)=C1[OH+]3)[O+]=C1C=CN(C)C(C)=C1[OH+]4' since it resulted in zero atoms\n","  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n","/usr/local/lib/python3.12/dist-packages/torch_geometric/datasets/molecule_net.py:213: UserWarning: Skipping molecule 'CC(c1cccs1)=[N+]1[N-]C(N)=[S+][AlH3-]12[OH+]B(c1ccccc1)[OH+]2' since it resulted in zero atoms\n","  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n","/usr/local/lib/python3.12/dist-packages/torch_geometric/datasets/molecule_net.py:213: UserWarning: Skipping molecule 'CC(c1ccccn1)=[N+]1[N-]C(N)=[S+][AlH3-]12[OH+]B(c1ccccc1)[OH+]2' since it resulted in zero atoms\n","  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n","/usr/local/lib/python3.12/dist-packages/torch_geometric/datasets/molecule_net.py:213: UserWarning: Skipping molecule '[Na+].c1ccc([SH+][GeH2+]2[SH+]c3ccccc3[SH+]2)c([SH+][GeH2+]2[SH+]c3ccccc3[SH+]2)c1' since it resulted in zero atoms\n","  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n","Done!\n"]},{"output_type":"stream","name":"stdout","text":["Dataset ready! Total size: 41120 molecules\n","But we'll only load 40 of them.\n","\n"]}]},{"cell_type":"code","source":["import codecs\n","\n","import numpy as np\n","import torch.nn as nn\n","from transformers import AutoTokenizer\n","\n","smiles_list = []\n","batch_size = 40\n","\n","\n","# Iterate through the dataset and fill it with the SMILES strings\n","for _ in range(batch_size):\n","  mol_info = next(iterator)\n","  smiles_list.append(mol_info['smiles'])\n","\n","\n","\n","# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n","\n","# First, tokenize without padding to find the actual max length\n","tokenized_lengths = []\n","for smiles in smiles_list:\n","    tokens = tokenizer(smiles, padding=False, truncation=False)\n","    tokenized_lengths.append(len(tokens['input_ids']))\n","\n","# Determine max_length from your data\n","max_length = max(tokenized_lengths)\n","print(f\"Maximum sequence length in data: {max_length}\")\n","\n","\n","# Now tokenize with the data-driven max_length\n","encoded = tokenizer(\n","    smiles_list,\n","    padding='max_length',\n","    truncation=True,\n","    max_length=max_length,\n","    return_tensors='pt'\n",")\n","\n","# Create embedding layer\n","embedding = nn.Embedding(\n","    num_embeddings=len(tokenizer),\n","    embedding_dim=512,\n","    padding_idx=tokenizer.pad_token_id\n",")\n","\n","# Get embeddings directly\n","embeddings = embedding(encoded['input_ids'])\n","print(f\"Embeddings shape: {embeddings.shape}\")  # (batch_size, max_length, 512)\n","\n"],"metadata":{"id":"oRsTzvL0l5bx","executionInfo":{"status":"ok","timestamp":1756562932686,"user_tz":-120,"elapsed":3975,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"colab":{"base_uri":"https://localhost:8080/","height":340,"referenced_widgets":["8952b851d8664ef1a4e522a9420c24e2","856ff9f3159c4d2bb76b031662b7221c","f573b06deaae49bda6a6ae5d520cb587","c1a0ea07f6624d808f263c4a02110f91","4c6ef2b3493348119ac63023a6ca96fa","7859906d7cb048dc87e90bd6660e83e5","e76e1fe7eae240b88f8602f089f41c77","df61e5dd0e124434a8f369a30c6be32b","0845f73653314c0ba803e39f42837ee8","5f2eb751b4b34a0f80c53b67f78a1496","327faf2943874b18bd08790f4b443e8c","6c893b9181894dbb832d6d749f50ce1d","a8f2e1d4fc2d4ab2a0a12f7c66d6aea2","2685a0a6b210419caf4d28a6172cb454","3dbb03b3fb434a5d8775a600124896fa","76d8f5d2724b4b929c053d35ca0fe200","289ee9d38a8747e98472cebdcbff5b07","99896e8174cb4a1cb4a84efd196b607b","41bc2b0682a14a43aceb1a6e5f537d9a","cfebcfa5435349f5b83ad02b6401a647","213c00383fd44221a2d235901d75d743","30c41bac74974e2e8a49b6cd56584ef2","c0d9314b8c4e4edf9aca2dafab56a071","0fdd35deba5649b2a6fa13aa530f7135","25926a84ec4447258735488cdef946c8","5d83c8abf66e45ffae3479a2a4356ea9","74343dd711bf402d8dc97cff42bdc5c2","779cd53035274f9f9713b0e94a8f5dfe","d8a9981d12224b9a8a3f5840ca4e644e","b2c2b3df225e4046b164c8504c398440","3b31134d499c4a198f8d7ff20cc46e11","4a6e0c37dad242b89f645cf446e23f5a","3e9041f8cd324923a0aabe1f852d834b","ae0a255fd11e43848daac4f5a99a7960","664d5b44ccea42389399bcb4a507205d","5af3c7ac3cc144eda0d02005515231dd","99fbdcdcadd347aba753d63f59131b44","ef8aaa0c1af441c48b7767d3282ff111","626858613c3c4cd88a140df12c583562","5f6e67c39a364c9083cd36503d98fbc3","9a105951151548cfb1498c6d773a1d28","a989cf3bf1a14dc0877d7265365841c7","dcbea6841eb6476c8a0e712da7a381bd","6ddb90bd4fcd43669d40c4e45c9dca43","a89ec5b742784c4db148d9d1dd61a027","7bc3f037740f43ccb8e40a0920c08fab","7cedddde89eb42f5a780e9a852b17caf","99773cff7ef642ab87e81a5b7dc7111e","c8ba4efb9c654504827661a99771218b","4ad526b8d06a414fab6eb200cb421023","762f70a7c9a340d1a023836ca2befc60","870f22529a064c78b467cdee16ad57e9","0b8539fe78844f8abc58386ac0a57818","7cd599f5743d44d3a640502467c512f6","0ff1d96ebf0d428fb93dda056131a870"]},"outputId":"6e181dad-fc5d-4e85-f767-ccf9bd070598"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/166 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8952b851d8664ef1a4e522a9420c24e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/501 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c893b9181894dbb832d6d749f50ce1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0d9314b8c4e4edf9aca2dafab56a071"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae0a255fd11e43848daac4f5a99a7960"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a89ec5b742784c4db148d9d1dd61a027"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Maximum sequence length in data: 61\n","Embeddings shape: torch.Size([40, 61, 512])\n"]}]},{"cell_type":"code","source":["from scipy.sparse.linalg import eigsh\n","from rdkit import Chem\n","def smiles_to_adjacency_matrix(smiles):\n","    \"\"\"Convert SMILES to adjacency matrix.\"\"\"\n","    mol = Chem.MolFromSmiles(smiles)\n","    if mol is None:\n","        return None\n","\n","    n_atoms = mol.GetNumAtoms()\n","    adj_matrix = np.zeros((n_atoms, n_atoms))\n","\n","    for bond in mol.GetBonds():\n","        i = bond.GetBeginAtomIdx()\n","        j = bond.GetEndAtomIdx()\n","        # Undirected graph\n","        adj_matrix[i, j] = 1\n","        adj_matrix[j, i] = 1\n","\n","    return adj_matrix\n","\n","def compute_graph_positional_encoding(adj_matrix, k=30):\n","    \"\"\"\n","    Compute graph positional encodings from eigenvectors of the\n","    symmetrically normalized graph Laplacian.\n","    \"\"\"\n","    n = adj_matrix.shape[0]\n","\n","    # Compute degree matrix\n","    degree = np.sum(adj_matrix, axis=1)\n","    degree[degree == 0] = 1\n","\n","    # D^(-1/2)\n","    d_inv_sqrt = np.diag(1.0 / np.sqrt(degree))\n","\n","    # Symmetrically normalized Laplacian\n","    identity = np.eye(n)\n","    normalized_adj = d_inv_sqrt @ adj_matrix @ d_inv_sqrt\n","    laplacian = identity - normalized_adj\n","\n","    # Compute eigenvectors\n","    if n < k:\n","        # If graph has fewer nodes than k, pad with zeros\n","        eigenvalues, eigenvectors = np.linalg.eigh(laplacian)\n","        # Pad eigenvectors to have k columns\n","        padded_eigenvectors = np.zeros((n, k))\n","        padded_eigenvectors[:, :n] = eigenvectors\n","        return padded_eigenvectors\n","    else:\n","        eigenvalues, eigenvectors = eigsh(laplacian, k=k, which='SM')\n","        return eigenvectors"],"metadata":{"id":"5VXPZypgg5_P","executionInfo":{"status":"ok","timestamp":1756562932731,"user_tz":-120,"elapsed":21,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Process all SMILES and compute their graph PEs\n","graph_pes_list = []\n","max_nodes = 0\n","\n","for smiles in smiles_list:\n","    adj_matrix = smiles_to_adjacency_matrix(smiles)\n","    if adj_matrix is not None:\n","        pe = compute_graph_positional_encoding(adj_matrix, k=30)\n","        graph_pes_list.append(pe)\n","        max_nodes = max(max_nodes, pe.shape[0])\n","    else:\n","        graph_pes_list.append(None)\n","\n","print(f\"Maximum number of atoms in dataset: {max_nodes}\")\n"],"metadata":{"id":"QadoBoVYgrrz","executionInfo":{"status":"ok","timestamp":1756562932794,"user_tz":-120,"elapsed":62,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"39769334-6c7d-4709-81ca-9a4c636e0862"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum number of atoms in dataset: 42\n"]}]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","from rdkit import Chem\n","import re\n","\n","# def generate_random_orthonormal_pe(n_vectors, dim=30):\n","#     \"\"\"Generate n_vectors random orthonormal vectors of dimension dim.\"\"\"\n","#     if n_vectors == 0:\n","#         return np.zeros((0, dim))\n","\n","#     # Generate random matrix\n","#     random_matrix = np.random.randn(dim, n_vectors)\n","\n","#     # Use QR decomposition to get orthonormal vectors\n","#     Q, _ = np.linalg.qr(random_matrix)\n","\n","#     # Return first n_vectors columns (transposed to have shape (n_vectors, dim))\n","#     return Q[:, :n_vectors].T\n","\n","def generate_zero_pe(n_vectors, dim=30):\n","    \"\"\"Generate n_vectors zero vectors of dimension dim for non-atom characters.\"\"\"\n","    return np.zeros((n_vectors, dim))\n","\n","def parse_token_components(token):\n","    \"\"\"\n","    Parse a token to identify atoms and characters.\n","    Returns: (atom_indices, n_characters)\n","    \"\"\"\n","    # Common atom patterns in SMILES\n","    atom_pattern = r'(Cl|Br|Si|Mg|Ca|Fe|Al|Na|Li|[BCNOFPSKHIV])'\n","\n","    # Find all atoms in the token\n","    atoms = re.findall(atom_pattern, token)\n","\n","    # Count non-atom characters\n","    # Remove atoms from token to count remaining characters\n","    remaining = token\n","    for atom in atoms:\n","        remaining = remaining.replace(atom, '', 1)\n","\n","    # Count actual characters (digits, +, -, =, #, etc.)\n","    n_characters = len([c for c in remaining])\n","\n","    return atoms, n_characters\n","\n","def align_graph_pe_to_tokens(smiles, graph_pe, tokenizer, max_length,random_seed=None):\n","    \"\"\"\n","    Align graph PE to tokens, handling pure characters, atoms, and mixed tokens.\n","\n","    Args:\n","        smiles: SMILES string\n","        graph_pe: Graph positional encoding for atoms (n_atoms, embedding_dim)\n","        tokenizer: Tokenizer object\n","        max_length: Maximum sequence length\n","        random_seed: Random seed for reproducible random PEs\n","    \"\"\"\n","    if random_seed is not None:\n","        np.random.seed(random_seed)\n","    # Determine embedding dimension\n","    if graph_pe is not None:\n","        embedding_dim = graph_pe.shape[1]\n","    elif embedding_dim is None:\n","        embedding_dim = 30  # Default fallback\n","\n","    mol = Chem.MolFromSmiles(smiles)\n","    if mol is None or graph_pe is None:\n","        return torch.zeros(max_length, embedding_dim)\n","\n","    # Get tokens\n","    encoding = tokenizer(smiles, padding='max_length', max_length=max_length)\n","    tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n","\n","\n","    # Initialize token PE\n","    token_pe = torch.zeros(max_length, embedding_dim)\n","\n","    # Build atom mapping from SMILES\n","    atom_symbols = [atom.GetSymbol() for atom in mol.GetAtoms()]\n","    atom_count = {symbol: 0 for symbol in set(atom_symbols)}\n","    atom_to_idx = {}\n","\n","    for idx, symbol in enumerate(atom_symbols):\n","        atom_to_idx[(symbol, atom_count[symbol])] = idx\n","        atom_count[symbol] += 1\n","\n","    # Reset atom count for tracking\n","    current_atom_count = {symbol: 0 for symbol in atom_count}\n","\n","    # Process each token\n","    for i, token in enumerate(tokens):\n","        if token in ['<s>', '</s>', '<pad>']:\n","            continue\n","\n","        # Parse token components\n","        atoms_in_token, n_characters = parse_token_components(token)\n","\n","        if len(atoms_in_token) == 0 and n_characters > 0:\n","            # Pure character token - use random orthonormal PE\n","            random_pes = generate_zero_pe(n_characters, embedding_dim)\n","            token_pe[i] = torch.tensor(random_pes.sum(axis=0))\n","\n","        elif len(atoms_in_token) > 0 and n_characters == 0:\n","            # Pure atom token(s) - use graph PE\n","            atom_pes = []\n","            for atom_symbol in atoms_in_token:\n","                if atom_symbol in current_atom_count:\n","                    atom_key = (atom_symbol, current_atom_count[atom_symbol])\n","                    if atom_key in atom_to_idx:\n","                        atom_idx = atom_to_idx[atom_key]\n","                        atom_pes.append(graph_pe[atom_idx])\n","                        current_atom_count[atom_symbol] += 1\n","\n","            if atom_pes:\n","                # sum the PEs of all atoms in this token\n","                token_pe[i] = torch.tensor(np.sum(atom_pes, axis=0))\n","\n","        elif len(atoms_in_token) > 0 and n_characters > 0:\n","            # Mixed token - combine atom PE and character PE\n","            # Get atom PEs\n","            atom_pes = []\n","            for atom_symbol in atoms_in_token:\n","                if atom_symbol in current_atom_count:\n","                    atom_key = (atom_symbol, current_atom_count[atom_symbol])\n","                    if atom_key in atom_to_idx:\n","                        atom_idx = atom_to_idx[atom_key]\n","                        atom_pes.append(graph_pe[atom_idx])\n","                        current_atom_count[atom_symbol] += 1\n","\n","            # Get character PEs\n","            random_pes = generate_zero_pe(n_characters, embedding_dim)\n","\n","            # Combine: sum of atoms + sum of characters\n","            combined_pe = np.zeros(embedding_dim)\n","            if atom_pes:\n","                combined_pe += np.sum(atom_pes, axis=0)\n","            combined_pe += random_pes.sum(axis=0)\n","\n","            token_pe[i] = torch.tensor(combined_pe)\n","\n","    return token_pe\n","\n"],"metadata":{"id":"_Ndd99hrDjdB","executionInfo":{"status":"ok","timestamp":1756562932815,"user_tz":-120,"elapsed":20,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["token_pes_list = []\n","\n","for smiles, graph_pe in zip(smiles_list, graph_pes_list):\n","    # Align graph PE to tokens using the provided function\n","    token_pe = align_graph_pe_to_tokens(\n","        smiles=smiles,\n","        graph_pe=graph_pe,\n","        tokenizer=tokenizer,\n","        max_length=max_length,\n","        random_seed=42\n","    )\n","    token_pes_list.append(token_pe)\n","\n","# Stack into batch tensor\n","token_pes_batch = torch.stack(token_pes_list)"],"metadata":{"id":"Ahf_qjEtXHz3","executionInfo":{"status":"ok","timestamp":1756562932852,"user_tz":-120,"elapsed":35,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class EmbeddingWithGraphPE(nn.Module):\n","    def __init__(self, embed_dim=512, pe_dim=30):\n","        super().__init__()\n","        # One-layer projection with GeLU from graph PE to embedding dimension\n","        # Using standard Laplacian, so pe_dim is just k\n","        self.pe_projection = nn.Sequential(\n","            nn.Linear(pe_dim, embed_dim),\n","            nn.GELU()\n","        )\n","\n","    def forward(self, embeddings, token_pes):\n","        # token_pes shape: [batch_size, seq_len, pe_dim]\n","        # embeddings shape: [batch_size, seq_len, embed_dim]\n","\n","        # Project token PEs to embedding dimension with GeLU\n","        projected_pes = self.pe_projection(token_pes)\n","\n","        # Add to token embeddings\n","        enhanced_embeddings = embeddings + projected_pes\n","\n","        return enhanced_embeddings\n","\n","# Usage\n","model = EmbeddingWithGraphPE(embed_dim=512, pe_dim=30)\n","# graph_pes_batch shape: [40, 61, 30] (30 eigenvectors from standard Laplacian)\n","# embeddings shape: [40, 61, 512]\n","enhanced_embeddings = model(embeddings, token_pes_batch)\n","# Output shape: [40, 61, 512]"],"metadata":{"id":"i0zuJ4bRvlJT","executionInfo":{"status":"ok","timestamp":1756562932943,"user_tz":-120,"elapsed":88,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["enhanced_embeddings.shape"],"metadata":{"id":"LYwFIBucX0hF","executionInfo":{"status":"ok","timestamp":1756562932961,"user_tz":-120,"elapsed":20,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c2a97feb-0f4a-4ef4-eacb-3a17743c4670"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([40, 61, 512])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["import json\n","from data_import import check_class_balance, stratified_train_val_split, convert_to_litgpt_format, save_to_jsonl\n","\n","# Create dataset\n","batch_size = 41120  # Adjust based on your needs\n","dataset = PartialHIVDataset(max_samples=batch_size)\n","\n","# Convert to DataFrame\n","full_batch_data = list(dataset)\n","df = pd.DataFrame(full_batch_data)\n","\n","\n","# Check balance in the original DataFrame\n","original_balance = check_class_balance(df, \"Original Dataset\")\n","\n","# Convert to LitGPT format for different task types\n","classification_data = convert_to_litgpt_format(df, task_type=\"classification\")\n","\n","# Check balance in the converted data\n","converted_balance = check_class_balance(classification_data, \"Converted Dataset\")\n","\n","# Create stratified train/validation split\n","train_data, val_data = stratified_train_val_split(classification_data, train_ratio=0.8)\n","\n","# Check balance in train and validation sets\n","train_balance = check_class_balance(train_data, \"Training Set\")\n","val_balance = check_class_balance(val_data, \"Validation Set\")\n","\n","# Calculate difference from original distribution\n","train_diff_0 = abs(train_balance['ratio_0'] - original_balance['ratio_0'])\n","train_diff_1 = abs(train_balance['ratio_1'] - original_balance['ratio_1'])\n","val_diff_0 = abs(val_balance['ratio_0'] - original_balance['ratio_0'])\n","val_diff_1 = abs(val_balance['ratio_1'] - original_balance['ratio_1'])\n","\n","\n","# Save to JSONL files\n","save_to_jsonl(train_data, \"hiv_train.jsonl\")\n","save_to_jsonl(val_data, \"hiv_val.jsonl\")\n"],"metadata":{"id":"0i30OS9CSDBm","executionInfo":{"status":"ok","timestamp":1756562942111,"user_tz":-120,"elapsed":9148,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"bd78376c-1028-4ef4-bf8a-51080963eba3"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing HIV dataset (will only load 41120 samples)...\n","Dataset ready! Total size: 41120 molecules\n","But we'll only load 41120 of them.\n","\n","\n","Original Dataset Class Balance:\n","Total samples: 41120\n","Class 0.0: 39677 samples (96.49%)\n","Class 1.0: 1443 samples (3.51%)\n","\n","Converted Dataset Class Balance:\n","Total samples: 41120\n","Class 0: 39677 samples (96.49%)\n","Class 1: 1443 samples (3.51%)\n","\n","Training Set Class Balance:\n","Total samples: 32895\n","Class 0: 31741 samples (96.49%)\n","Class 1: 1154 samples (3.51%)\n","\n","Validation Set Class Balance:\n","Total samples: 8225\n","Class 0: 7936 samples (96.49%)\n","Class 1: 289 samples (3.51%)\n","Saved 32895 entries to hiv_train.jsonl\n","Saved 8225 entries to hiv_val.jsonl\n"]}]},{"cell_type":"code","source":["import json\n","import torch\n","from utils import calculate_max_lengths\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer\n","\n","def load_jsonl(filepath):\n","    \"\"\"Load data from JSONL file\"\"\"\n","    data = []\n","    with open(filepath, 'r') as f:\n","        for line in f:\n","            data.append(json.loads(line))\n","    return data\n","\n","class HIVDataset(Dataset):\n","    def __init__(self, jsonl_filepath, tokenizer, max_full_length):\n","        self.data = load_jsonl(jsonl_filepath)\n","        self.tokenizer = tokenizer\n","        self.max_full_length = max_full_length\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        item = self.data[idx]\n","\n","        # Extract SMILES\n","        smiles = item['input'].replace(\"SMILES: \", \"\")\n","\n","        # Create full prompt\n","        full_prompt = f\"{item['instruction']}\\n{item['input']}\\nAnswer:\"\n","        # Compute graph PE with actual SMILES length\n","        smiles_tokens = self.tokenizer(smiles, add_special_tokens=False)\n","        actual_smiles_length = len(smiles_tokens['input_ids'])\n","        # Get prompt length for label masking\n","        prompt_encoding = self.tokenizer(full_prompt, add_special_tokens=True, return_tensors='pt')\n","        prompt_length = prompt_encoding['input_ids'].shape[1]\n","\n","        # Full sequence for training\n","        target_text = f\"{full_prompt} {item['output']}\"\n","        full_encoding = self.tokenizer(\n","            target_text,\n","            padding='max_length',\n","            max_length=self.max_full_length,\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","\n","        # Compute graph PE for SMILES only\n","        adj_matrix = smiles_to_adjacency_matrix(smiles)\n","        if adj_matrix is not None:\n","            graph_pe = compute_graph_positional_encoding(adj_matrix, k=30)\n","            smiles_token_pe = align_graph_pe_to_tokens(\n","                smiles, graph_pe, self.tokenizer, actual_smiles_length, random_seed=42\n","            )\n","        else:\n","            smiles_token_pe = torch.zeros(actual_smiles_length, 30)\n","\n","        # Create full-length PE tensor\n","        full_token_pe = torch.zeros(self.max_full_length, 30)\n","\n","        # Find where SMILES tokens are and place PEs there\n","        # This is approximate - you might need better alignment\n","        instruction_text = f\"{item['instruction']}\\nSMILES: \"\n","        instruction_tokens = self.tokenizer(instruction_text, add_special_tokens=True)\n","        smiles_start_idx = len(instruction_tokens['input_ids'])\n","\n","        # Copy SMILES PEs to correct position\n","        pe_end_idx = min(smiles_start_idx + actual_smiles_length, self.max_full_length)\n","\n","\n","        pe_length = min(smiles_token_pe.shape[0], pe_end_idx - smiles_start_idx)\n","        full_token_pe[smiles_start_idx:smiles_start_idx + pe_length] = smiles_token_pe[:pe_length]\n","        # Create labels with prompt masked\n","        labels = full_encoding['input_ids'][0].clone()\n","        labels[:prompt_length] = -100  # Mask prompt tokens\n","\n","        return {\n","            'input_ids': full_encoding['input_ids'][0],\n","            'graph_pes': full_token_pe,\n","            'labels': labels,\n","        }\n","\n","# First calculate max lengths\n","max_full_length = calculate_max_lengths('hiv_train.jsonl')\n","\n","# Then create dataloaders with those lengths\n","def create_dataloaders(train_path='hiv_train.jsonl', val_path='hiv_val.jsonl',\n","                      batch_size=8, tokenizer=None, max_full_length=max_full_length):\n","    if tokenizer is None:\n","        tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-160m\")\n","        if tokenizer.pad_token is None:\n","            tokenizer.pad_token = tokenizer.eos_token\n","\n","    train_dataset = HIVDataset(train_path, tokenizer, max_full_length)\n","    val_dataset = HIVDataset(val_path, tokenizer, max_full_length)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","    return train_loader, val_loader, tokenizer\n","\n","# Usage\n","train_loader, val_loader, tokenizer = create_dataloaders(batch_size=8)\n","\n","# Test one batch\n","sample_batch = next(iter(train_loader))\n","print(f\"Input IDs shape: {sample_batch['input_ids'].shape}\")\n","print(f\"Graph PEs shape: {sample_batch['graph_pes'].shape}\")\n","print(f\"Labels shape: {sample_batch['labels'].shape}\")"],"metadata":{"id":"41yISTqnPEyP","executionInfo":{"status":"ok","timestamp":1756562954348,"user_tz":-120,"elapsed":12205,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"colab":{"base_uri":"https://localhost:8080/","height":359,"referenced_widgets":["2c4a386b3fdd4e0e96ced62caad356dc","ef08e4118d8f4f4a80eb865191151537","1a2e8b0b4ff24a61892eb0b5c09f840d","6b2170d1305b46ce83edaf4fb4f6b825","1df8e225dc86445185e5da39782d417a","d0674e3002f5484e8ff980d049de601b","67e89d7b27fe448c961e08d19f90879e","56ade494d0624fe0b9790c1c3ee50f74","fcd735d4fdf047fcba5f00764bbb7f00","37c9d67dc069402eb24b9c3755ac9491","1e52206cfa3941e2abb2e6283353e9ba","3379ded465c844b69d0b409ac2d95400","47247f0f67444c92927a9fd9349b6a07","0da396ab54734a2899a9c12999abb3e4","e885f87f22f043b4833c3075ffe3f241","de98f4ed9b3a4406a75dacb5c83e6f34","1964a9d785a04e3b8ecd23a2bce88421","f9bd8bdb936045e482c0b77d9677dbde","69cad0a5b34249d4b9006f7a91c718db","c8e0b86ba052422bb7fd739a1125aa23","5c4625ed99b5400bb338dbbda2bed025","dcfacb2bc3ca42fda2b3ca939c664de3","cfe94a706481451ba2f48a27e987596c","77be68c0fde642f89b2eb37b3f3d84db","9d844edafb204257a28668f51bbfecb4","3927b477bc23424bbd83443de35704f4","32ee3fd575184ddda4e97cec4cea2181","5c964c9322ab45e6b9e318b53a94bbde","e110988db12746bc9f5c613a7e9b728a","7d2d5403cf1b4e1b94c30620bd8bedd1","f0aedbe13d064e0985ea8a682cb0672a","cf376c379d2545b0be5559b1227b0756","91cc3c31331a4cfea8466886a957c843","eb572a327cf94ed48288177d6d6dc793","cb3518665f594fa8839b707e6dbda4bf","761a64dfe0fd41e18a25dd662c98c66e","d21b5f0a02d649b7bbd092516288267a","36c8837e1c404ebc8eb94b7a75ed0d64","f8d70e970d2447868f9d999e1fabcf78","e9ddf17414ae403f9cbc5c572f7efc2f","dd5c74a6a21f4d22aa39fd66e23109d0","adb315cb20c64b4d9370986d6319dea1","d67edab1bb114adfbf5892d68992c5b7","6b1e505566084f3cae169b4bf31b4211","97798f3df77f498caeffb04baecd907d","2f45df2c132741a2b7cb6e935e80cfbf","9eadee8d697643fdb0f1a138f4d4a9ee","62fbea52da1b4194aedeba2b885d257e","88e93e60ca564dd6a3327b37dd289ef1","e291eb0497974b13a36cfb9fd3f9734c","0e4607cccdc1458a81b1c230496e6afb","08fdee2f337b4d2c9c76bc97ae15e579","833bd5439a794713bf6f2ada4b258178","476cec65d01841c7b0782bd641b2127e","c8f5f8400ac948189c02a007131c2ed3","84d2292eeac645ba9cb6cdeb5ad499f7","2229b27249e44d69a3555904fdaadad6","636c434f17ca4ed3aad330acc94ee08d","c37b264aeab94f429636c4be5ae632f6","729e53e4b1194e9c991e8cbb7fbd1606","d8ce0691b33c412ca43b77bf9dc0096e","44a0b9b2d76b4e77bd030c9c14581e5c","fa87151117ac4f059b1af9ef125afb51","7e05a3c7e9bb452881934ab6f4b65938","15c7bf3b0c844f9a90ab29459e143917","0b41575685d54d16b1c7643e582c3721","4db68a98e96943d5ae8a2485a9a7c21b","f282fe3b73b744e181bb1059c7688455","e36d18229e204e3e8e72f5a6b283e0ab","d7b2bd03c0524231b3d909724506a87c","638636edcc6740f2b52b9978b380d373","1e62aa1ed7a7488ea41e2cce432f83c2","b1af74e606754e37912396145f5cd735","ebb0c795a2d14138b7458c47f1facf5b","00e5344d07cb44ac93b30b0874787b6f","0682dff539584904a9dc521827463614","ebc9bb49c41d484daff8ee3a105f703f","bc55225cfe8742c4a93969b0a6f502c5","2c64796d3a634632a5d1bc62c2765f41","aaf204c345504a08aa412fafaf02d4a9","db6547b999e8420cb543e20a401553c0","0538ceb6e7e8441fa4a57ba64595bcd5","0c89eea7e4884844a5abe163a8427e13","e76b5cb37f504129ab942d79348bfc22","dbc95009f9de4cc8bece8372ae83de01","cce1a720619e41dd948d4256d87b4790","475e2b5d699d4d6c956f0d15f99b79aa","62f2ae59c0d54e61b43b998cca321444","597ac6888e2d4e058a00f8c10966c1d6","bd2e9aff41e74cd8b99a5171889fc565","5f0e24d7ba9547d1acbe5fdfb9fedf74","6e05bb59f5f5429f8d0fefaac3f0695c","f3906a775de54098a5aec3031a66fa31","18328c65634a4e45994b89413043c9a4","30bcdf620ef04e398e069f0876ab68dd","ba24f20a44e2499bad5d9d547000e807","f145bec92b90461c94d08fc12de7a9c5","d32f0fcb16254c029d112a6821cb6d2d","df5bc498d73e41b8a4e6366421798fb0"]},"outputId":"f7b9e01e-62c5-4092-fe86-2002d702480b"},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c4a386b3fdd4e0e96ced62caad356dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3379ded465c844b69d0b409ac2d95400"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfe94a706481451ba2f48a27e987596c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb572a327cf94ed48288177d6d6dc793"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["added_tokens.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97798f3df77f498caeffb04baecd907d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84d2292eeac645ba9cb6cdeb5ad499f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4db68a98e96943d5ae8a2485a9a7c21b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc55225cfe8742c4a93969b0a6f502c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"597ac6888e2d4e058a00f8c10966c1d6"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Input IDs shape: torch.Size([8, 498])\n","Graph PEs shape: torch.Size([8, 498, 30])\n","Labels shape: torch.Size([8, 498])\n"]}]},{"cell_type":"code","source":["# Look at first example in batch\n","i = 0\n","print(f\"Input text (decoded): {tokenizer.decode(sample_batch['input_ids'][i])}\")\n","print(f\"\\nNon-zero PE positions: {torch.nonzero(sample_batch['graph_pes'][i].sum(dim=1)).squeeze().tolist()}\")\n","print(f\"\\nLabel tokens that aren't -100: {sample_batch['labels'][i][sample_batch['labels'][i] != -100].tolist()}\")"],"metadata":{"id":"CycDQXtCRHqC","executionInfo":{"status":"ok","timestamp":1756562954373,"user_tz":-120,"elapsed":15,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"bf7a3867-66b3-4386-c67b-d38b7ca2751f"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Input text (decoded): Classify the following molecule based on its HIV activity. Respond with '1' if the molecule shows HIV activity, or '0' if it does not.\n","SMILES: COc1cc2c(cc1O)C1(C(=O)c3ccc4c(c3C1O)OCO4)N(C)CC2\n","Answer: 0<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n","\n","Non-zero PE positions: [38, 47, 49, 52, 55, 65, 67, 69, 70, 73, 75, 77]\n","\n","Label tokens that aren't -100: [470, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"]}]},{"cell_type":"code","source":["# %pip install peft"],"metadata":{"id":"XnBZRRW3lIzU","executionInfo":{"status":"ok","timestamp":1756562954375,"user_tz":-120,"elapsed":1,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from transformers import AutoModelForCausalLM, AutoTokenizer,get_linear_schedule_with_warmup\n","from peft import LoraConfig, get_peft_model, TaskType\n","from typing import Optional\n","\n","class HIVPEModule(nn.Module):\n","    def __init__(self, pe_dim=30, embed_dim=768):\n","        super().__init__()\n","        self.pe_projection = nn.Linear(pe_dim, embed_dim)\n","        self.activation = nn.GELU()\n","        self.dropout = nn.Dropout(0.7)\n","        # Better initialization for stability\n","        nn.init.xavier_uniform_(self.pe_projection.weight, gain=0.1)\n","        nn.init.zeros_(self.pe_projection.bias)\n","\n","        # Use a more reasonable scale that works with fp16\n","        self.scale = 0.1  #\n","\n","    def forward(self, embeddings, graph_pes):\n","        if graph_pes is not None:\n","            # Ensure proper dtype\n","            graph_pes = graph_pes.to(embeddings.dtype)\n","            projected_pes = self.activation(self.pe_projection(graph_pes))\n","            projected_pes = self.dropout(projected_pes)\n","            # Use learnable scale parameter for better gradient flow\n","            return embeddings + self.scale * projected_pes\n","        return embeddings\n","\n","class PythiaWithPE(nn.Module):\n","    \"\"\"Wrapper that adds PE to Pythia model\"\"\"\n","    def __init__(self, base_model, pe_dim=30):\n","        super().__init__()\n","        self.base_model = base_model\n","        self.embed_dim = base_model.gpt_neox.embed_in.embedding_dim\n","        self.pe_module = HIVPEModule(pe_dim=pe_dim, embed_dim=self.embed_dim)\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        graph_pes=None,\n","        attention_mask=None,\n","        labels=None,\n","        **kwargs\n","    ):\n","        # Get embeddings\n","        inputs_embeds = self.base_model.gpt_neox.embed_in(input_ids)\n","\n","        # Add PE with proper dtype handling\n","        if graph_pes is not None:\n","            inputs_embeds = self.pe_module(inputs_embeds, graph_pes)\n","\n","        # Forward through the model with embeddings\n","        outputs = self.base_model(\n","            inputs_embeds=inputs_embeds,\n","            attention_mask=attention_mask,\n","            labels=labels,\n","            **kwargs\n","        )\n","\n","        return outputs\n","\n","    def generate(self, input_ids, graph_pes=None, **kwargs):\n","        \"\"\"Generate with PE support\"\"\"\n","        # Get embeddings with PE for the prompt\n","        inputs_embeds = self.base_model.gpt_neox.embed_in(input_ids)\n","        if graph_pes is not None:\n","            inputs_embeds = self.pe_module(inputs_embeds, graph_pes)\n","\n","        # Generate using embeddings\n","        return self.base_model.generate(\n","            inputs_embeds=inputs_embeds,\n","            **kwargs\n","        )\n","\n","def setup_model_with_pe_and_lora(model_name=\"EleutherAI/pythia-160m\", pe_dim=30):\n","    \"\"\"\n","    Setup Pythia with custom PE and LoRA using PEFT\n","    \"\"\"\n","    print(f\"Loading {model_name}...\")\n","\n","    # Load base model - use float32 for training stability\n","    base_model = AutoModelForCausalLM.from_pretrained(\n","        model_name,\n","        torch_dtype=torch.float32,  # Use float32 for stability\n","        device_map=\"auto\"\n","    )\n","\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    if tokenizer.pad_token is None:\n","        tokenizer.pad_token = tokenizer.eos_token\n","\n","    # Wrap with PE\n","    model = PythiaWithPE(base_model, pe_dim=pe_dim)\n","\n","    # Configure LoRA with conservative settings\n","    lora_config = LoraConfig(\n","        task_type=TaskType.CAUSAL_LM,\n","        r=4,\n","        lora_alpha=8,\n","        lora_dropout=0.3,  # Increased dropout\n","        target_modules=[\n","            \"query_key_value\",\n","            \"dense_h_to_4h\",\n","            \"dense_4h_to_h\"\n","        ],\n","        # Initialize LoRA weights with smaller values\n","        init_lora_weights=\"gaussian\"\n","    )\n","\n","    # Apply LoRA to the base model\n","    model.base_model = get_peft_model(model.base_model, lora_config)\n","\n","    # Print trainable parameters\n","    total_params = sum(p.numel() for p in model.parameters())\n","    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(f\"✓ Total parameters: {total_params:,}\")\n","    print(f\"✓ Trainable parameters: {trainable_params:,}\")\n","    print(f\"✓ Trainable %: {100 * trainable_params / total_params:.2f}%\")\n","\n","    return model, tokenizer\n","\n","def train_with_focal_loss(\n","    model,\n","    train_loader,\n","    val_loader=None,\n","    epochs=3,\n","    learning_rate=1e-5,\n","    device=\"cuda\",\n","    tokenizer=None,save_checkpoint_at_batch=2000,  # Add this parameter\n","    checkpoint_path=\"checkpoint_batch_2000.pt\"  # Add this parameter\n","):\n","    \"\"\"Training loop with Focal Loss for extreme class imbalance\"\"\"\n","\n","    model = model.to(device)\n","\n","    # Focal loss parameters\n","    alpha = 0.965  # Weight for class 0 (should roughly match class distribution)\n","    gamma = 5.0   # Focusing parameter - increase this for more extreme imbalance\n","\n","    gradient_accumulation_steps = 2\n","    validation_frequency = 100  # Check validation every 2000 batches\n","\n","    optimizer = torch.optim.AdamW(\n","        model.parameters(),\n","        lr=learning_rate,\n","        weight_decay=0.3,\n","        eps=1e-6\n","    )\n","    num_training_steps = (len(train_loader) * epochs) // gradient_accumulation_steps\n","    scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=100,\n","    num_training_steps=num_training_steps)\n","    token_0 = 470  # \" 0\"\n","    token_1 = 337  # \" 1\"\n","\n","    scaler = torch.amp.GradScaler('cuda') if device == \"cuda\" else None\n","\n","    def focal_loss(logits, labels, alpha=0.1, gamma=5.0):\n","        \"\"\"\n","        Focal loss implementation\n","        alpha: weight for class 0 (majority class)\n","        1-alpha: weight for class 1 (minority class)\n","        \"\"\"\n","        # Get class probabilities\n","        probs = torch.softmax(logits, dim=-1)\n","\n","        # Get probability of true class\n","        ce_loss = nn.functional.cross_entropy(logits, labels, reduction='none')\n","        pt = torch.exp(-ce_loss)\n","\n","        # Apply class weights\n","        # For token_0 (majority), use alpha; for token_1 (minority), use 1-alpha\n","        alpha_t = torch.where(labels == token_0,  1 - alpha,alpha)\n","\n","        # Focal term: (1 - pt)^gamma reduces loss for well-classified examples\n","        focal_weight = alpha_t * (1 - pt) ** gamma\n","\n","        # Final focal loss\n","        focal_loss = focal_weight * ce_loss\n","\n","        return focal_loss.mean()\n","\n","    def weighted_cross_entropy(logits, labels):\n","      \"\"\"Simple weighted CE for extreme class imbalance\"\"\"\n","      # Create class weights\n","      class_weights = torch.zeros(logits.size(-1), device=logits.device)\n","      class_weights[token_0] = 0.001  # Weight for majority class (3.5%)\n","      class_weights[token_1] = 0.999  # Weight for minority class (96.5%)\n","\n","      return F.cross_entropy(logits, labels, weight=class_weights)\n","\n","    def run_validation(model, val_loader, max_batches=None):\n","        \"\"\"Run validation and return metrics\"\"\"\n","        model.eval()\n","        val_loss = 0\n","        val_class_0_correct = 0\n","        val_class_1_correct = 0\n","        val_class_0_total = 0\n","        val_class_1_total = 0\n","        val_batches = 0\n","\n","        with torch.no_grad():\n","            for i, batch in enumerate(val_loader):\n","                if max_batches and i >= max_batches:\n","                    break\n","\n","                input_ids = batch['input_ids'].to(device)\n","                graph_pes = batch['graph_pes'].to(device)\n","                labels = batch['labels'].to(device)\n","\n","                outputs = model(input_ids=input_ids, graph_pes=graph_pes)\n","                logits = outputs.logits\n","\n","                mask = ((labels == token_0) | (labels == token_1)) & (labels != -100)\n","\n","                if mask.any():\n","                    logits_flat = logits.view(-1, logits.size(-1))\n","                    labels_flat = labels.view(-1)\n","                    mask_flat = mask.view(-1)\n","\n","                    relevant_logits = logits_flat[mask_flat]\n","                    relevant_labels = labels_flat[mask_flat]\n","\n","                    #loss = focal_loss(relevant_logits, relevant_labels, alpha=alpha, gamma=gamma)\n","                    loss = weighted_cross_entropy(relevant_logits, relevant_labels)\n","                    loss = torch.clamp(loss, min=0.001)  # Prevent loss from going below 0.001\n","                    loss = loss / gradient_accumulation_steps\n","                    if not torch.isnan(loss):\n","                        val_loss += loss.item()\n","                        val_batches += 1\n","\n","                        preds = relevant_logits.argmax(dim=-1)\n","                        class_0_mask = relevant_labels == token_0\n","                        class_1_mask = relevant_labels == token_1\n","\n","                        if class_0_mask.any():\n","                            val_class_0_correct += (preds[class_0_mask] == relevant_labels[class_0_mask]).sum().item()\n","                            val_class_0_total += class_0_mask.sum().item()\n","\n","                        if class_1_mask.any():\n","                            val_class_1_correct += (preds[class_1_mask] == relevant_labels[class_1_mask]).sum().item()\n","                            val_class_1_total += class_1_mask.sum().item()\n","\n","        model.train()\n","        return {\n","            'loss': val_loss / val_batches if val_batches > 0 else float('inf'),\n","            'class_0_acc': val_class_0_correct / val_class_0_total if val_class_0_total > 0 else 0,\n","            'class_1_acc': val_class_1_correct / val_class_1_total if val_class_1_total > 0 else 0,\n","            'class_0_total': val_class_0_total,\n","            'class_1_total': val_class_1_total\n","        }\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        total_loss = 0\n","        valid_batches = 0\n","        class_0_correct = 0\n","        class_1_correct = 0\n","        class_0_total = 0\n","        class_1_total = 0\n","\n","        # Track predictions distribution\n","        pred_0_count = 0\n","        pred_1_count = 0\n","\n","        for batch_idx, batch in enumerate(train_loader):\n","            input_ids = batch['input_ids'].to(device)\n","            graph_pes = batch['graph_pes'].to(device)\n","            labels = batch['labels'].to(device)\n","            if batch_idx == save_checkpoint_at_batch:\n","                print(f\"\\nSaving checkpoint at batch {batch_idx}...\")\n","                checkpoint = {\n","                    'batch': batch_idx,\n","                    'epoch': epoch,\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'scheduler_state_dict': scheduler.state_dict(),\n","                    'scaler_state_dict': scaler.state_dict() if scaler else None,\n","                    'train_loss': total_loss / valid_batches if valid_batches > 0 else 0,\n","                    'class_0_acc': class_0_correct / class_0_total if class_0_total > 0 else 0,\n","                    'class_1_acc': class_1_correct / class_1_total if class_1_total > 0 else 0,\n","                }\n","                torch.save(checkpoint, checkpoint_path)\n","                print(f\"Checkpoint saved to {checkpoint_path}\")\n","            if scaler is not None:\n","                with torch.amp.autocast('cuda'):\n","                    outputs = model(input_ids=input_ids, graph_pes=graph_pes)\n","                    logits = outputs.logits\n","\n","                    if torch.isnan(logits).any():\n","                        print(f\"NaN detected at batch {batch_idx}, skipping...\")\n","                        optimizer.zero_grad()\n","                        continue\n","\n","                    # Your mask for 0/1 tokens\n","                    mask = ((labels == token_0) | (labels == token_1)) & (labels != -100)\n","\n","                    if mask.any():\n","                        logits_flat = logits.view(-1, logits.size(-1))\n","                        labels_flat = labels.view(-1)\n","                        mask_flat = mask.view(-1)\n","\n","                        relevant_logits = logits_flat[mask_flat]\n","                        relevant_labels = labels_flat[mask_flat]\n","\n","                        # Apply focal loss\n","                        #loss = focal_loss(relevant_logits, relevant_labels, alpha=alpha, gamma=gamma)\n","\n","                        # loss = loss / gradient_accumulation_steps\n","                        loss = weighted_cross_entropy(relevant_logits, relevant_labels)\n","                        # # Add L2 regularization on logits to prevent extreme predictions\n","                        logit_reg = 0.001 * (relevant_logits ** 2).mean()\n","                        loss = loss + logit_reg\n","\n","                        loss = loss / gradient_accumulation_steps\n","                        # loss = loss + logit_reg / gradient_accumulation_steps\n","\n","                        # Track predictions for monitoring\n","                        with torch.no_grad():\n","                            preds = relevant_logits.argmax(dim=-1)\n","                            pred_0_count += (preds == token_0).sum().item()\n","                            pred_1_count += (preds == token_1).sum().item()\n","                            # ADD THIS DIAGNOSTIC CODE:\n","                            if batch_idx % 50 == 0:  # Every 50 batches\n","                                # Get unique predicted tokens\n","                                unique_preds, counts = torch.unique(preds, return_counts=True)\n","                                print(f\"\\n  === Token Prediction Diagnostic (Batch {batch_idx}) ===\")\n","                                print(f\"  Expected tokens: {token_0} (' 0'), {token_1} (' 1')\")\n","                                print(f\"  Actually predicted tokens: {dict(zip(unique_preds.tolist(), counts.tolist()))}\")\n","\n","                                # Decode the predicted tokens to see what they are\n","                                print(f\"  Decoded predictions:\")\n","                                for token_id, count in zip(unique_preds.tolist(), counts.tolist()):\n","                                    decoded = tokenizer.decode([token_id])\n","                                    print(f\"    Token {token_id}: '{decoded}' (count: {count})\")\n","\n","                                # Sample a few predictions to see what's happening\n","                                sample_size = min(5, len(preds))\n","                                sample_indices = torch.randperm(len(preds))[:sample_size]\n","                                print(f\"\\n  Sample predictions vs labels:\")\n","                                for idx in sample_indices:\n","                                    pred_token = preds[idx].item()\n","                                    label_token = relevant_labels[idx].item()\n","                                    print(f\"    Predicted: {pred_token} ('{tokenizer.decode([pred_token])}') | Label: {label_token} ('{tokenizer.decode([label_token])}')\")\n","\n","                                # Check logits for tokens 470 and 337\n","                                print(f\"\\n  Logit values for key tokens (first 3 samples):\")\n","                                for i in range(min(3, len(relevant_logits))):\n","                                    logit_470 = relevant_logits[i, token_0].item()\n","                                    logit_337 = relevant_logits[i, token_1].item()\n","                                    print(f\"    Sample {i}: token_470=' 0' logit={logit_470:.3f}, token_337=' 1' logit={logit_337:.3f}\")\n","                            class_0_mask = relevant_labels == token_0\n","                            class_1_mask = relevant_labels == token_1\n","\n","                            if class_0_mask.any():\n","                                class_0_correct += (preds[class_0_mask] == relevant_labels[class_0_mask]).sum().item()\n","                                class_0_total += class_0_mask.sum().item()\n","\n","                            if class_1_mask.any():\n","                                class_1_correct += (preds[class_1_mask] == relevant_labels[class_1_mask]).sum().item()\n","                                class_1_total += class_1_mask.sum().item()\n","                    else:\n","                        continue\n","\n","                scaler.scale(loss).backward()\n","\n","                if (batch_idx + 1) % gradient_accumulation_steps == 0:\n","                    scaler.unscale_(optimizer)\n","\n","                    # Log gradient norm\n","                    if batch_idx % 50 == 0:\n","                        total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","                        print(f\"  Gradient norm: {total_norm:.4f}\")\n","                    else:\n","                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","                    scaler.step(optimizer)\n","                    scaler.update()\n","                    optimizer.zero_grad()\n","                    scheduler.step()\n","\n","            if mask.any() and not torch.isnan(loss):\n","                total_loss += loss.item() * gradient_accumulation_steps\n","                valid_batches += 1\n","\n","                if batch_idx % 10 == 0:\n","                    print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, \"\n","                          f\"Loss: {loss.item() * gradient_accumulation_steps:.4f}\")\n","\n","                    # Print detailed statistics every 50 batches\n","                    if batch_idx > 0 and batch_idx % 50 == 0:\n","                        print(f\"  Class 0 accuracy: {class_0_correct/class_0_total:.2%} ({class_0_correct}/{class_0_total})\")\n","                        if class_1_total > 0:\n","                            print(f\"  Class 1 accuracy: {class_1_correct/class_1_total:.2%} ({class_1_correct}/{class_1_total})\")\n","                        else:\n","                            print(f\"  Class 1 accuracy: No class 1 samples yet\")\n","                        print(f\"  Predictions distribution: {pred_0_count} zeros, {pred_1_count} ones\")\n","\n","                        # Reset prediction counters\n","                        pred_0_count = 0\n","                        pred_1_count = 0\n","\n","                # Periodic validation check\n","                if val_loader is not None and batch_idx > 0 and batch_idx % validation_frequency == 0:\n","                    print(f\"\\n--- Validation Check at Batch {batch_idx} ---\")\n","                    val_metrics = run_validation(model, val_loader, max_batches=100)  # Quick check on 100 batches\n","                    print(f\"Validation Loss: {val_metrics['loss']:.4f}\")\n","                    print(f\"Validation Class 0 accuracy: {val_metrics['class_0_acc']:.2%} \"\n","                          f\"({int(val_metrics['class_0_acc'] * val_metrics['class_0_total'])}/{val_metrics['class_0_total']})\")\n","                    if val_metrics['class_1_total'] > 0:\n","                        print(f\"Validation Class 1 accuracy: {val_metrics['class_1_acc']:.2%} \"\n","                              f\"({int(val_metrics['class_1_acc'] * val_metrics['class_1_total'])}/{val_metrics['class_1_total']})\")\n","                    print(\"--- End Validation Check ---\\n\")\n","\n","        # End of epoch summary\n","        print(f\"\\n{'='*50}\")\n","        print(f\"Epoch {epoch+1} Summary:\")\n","        print(f\"Average Loss: {total_loss/valid_batches:.4f}\")\n","        if class_0_total > 0:\n","            print(f\"Class 0 accuracy: {class_0_correct/class_0_total:.2%} ({class_0_correct}/{class_0_total})\")\n","        if class_1_total > 0:\n","            print(f\"Class 1 accuracy: {class_1_correct/class_1_total:.2%} ({class_1_correct}/{class_1_total})\")\n","        else:\n","            print(\"No class 1 samples in this epoch!\")\n","        print(f\"{'='*50}\\n\")\n","\n","        # Full validation at epoch end\n","        if val_loader is not None:\n","            print(\"Running full validation...\")\n","            val_metrics = run_validation(model, val_loader)  # Full validation\n","            print(f\"Full Validation Results:\")\n","            print(f\"  Loss: {val_metrics['loss']:.4f}\")\n","            print(f\"  Class 0 accuracy: {val_metrics['class_0_acc']:.2%}\")\n","            if val_metrics['class_1_total'] > 0:\n","                print(f\"  Class 1 accuracy: {val_metrics['class_1_acc']:.2%}\")\n","            print()\n","\n","    return model\n","\n","# Updated usage code\n","if __name__ == \"__main__\":\n","    # Setup model and tokenizer\n","    model, tokenizer = setup_model_with_pe_and_lora()\n","\n","    # Use the correct device and dtype setup\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    model = model.to(device)\n","\n","    # Don't convert to half precision manually - let autocast handle it\n","    # model.pe_module = model.pe_module.half()  # Remove this line\n","\n","    print(\"Model setup complete and ready for training!\")"],"metadata":{"id":"-vjqCGzkrLHn","executionInfo":{"status":"ok","timestamp":1756567249203,"user_tz":-120,"elapsed":1311,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"447778fc-5c24-44a3-e3ca-74fe49c824b5"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading EleutherAI/pythia-160m...\n","✓ Total parameters: 162,862,848\n","✓ Trainable parameters: 539,904\n","✓ Trainable %: 0.33%\n","Model setup complete and ready for training!\n"]}]},{"cell_type":"code","source":["import gc\n","\n","# Clear GPU memory\n","torch.cuda.empty_cache()\n","gc.collect()\n","# Setup model and tokenizer\n","model, tokenizer = setup_model_with_pe_and_lora()\n","\n","# Convert model to device\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = model.to(device)\n","\n","\n","# Use smaller batch size to ensure minority class representation\n","train_loader, val_loader, _ = create_dataloaders(\n","    batch_size=10,  # Reduced to ensure we see minority class\n","    tokenizer=tokenizer\n",")\n","\n","# Check if we actually see class 1 in the data\n","print(\"\\nChecking class 1 presence in first 10 batches:\")\n","class_1_count = 0\n","for i, batch in enumerate(train_loader):\n","    if i >= 10:\n","        break\n","    labels = batch['labels']\n","    if (labels == 337).any():\n","        class_1_count += 1\n","print(f\"Found class 1 in {class_1_count}/10 batches\")\n","\n","# Train with focal loss (use the new function)\n","trained_model = train_with_focal_loss(  # Changed from train_with_pe\n","    model,\n","    train_loader,\n","    val_loader,\n","    epochs=3,\n","    learning_rate=1e-4,\n","    device=device,\n","    tokenizer=tokenizer,save_checkpoint_at_batch=2000,checkpoint_path=\"checkpoint_batch_2000.pt\"\n",")"],"metadata":{"id":"l_BbuEEEtfPa","executionInfo":{"status":"error","timestamp":1756567581184,"user_tz":-120,"elapsed":322316,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"da279fdf-f5d1-451f-e5a3-25dc39bce226"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading EleutherAI/pythia-160m...\n","✓ Total parameters: 162,862,848\n","✓ Trainable parameters: 539,904\n","✓ Trainable %: 0.33%\n","\n","Checking class 1 presence in first 10 batches:\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1890492291.py:49: RuntimeWarning: k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.\n","  eigenvalues, eigenvectors = eigsh(laplacian, k=k, which='SM')\n"]},{"output_type":"stream","name":"stdout","text":["Found class 1 in 2/10 batches\n","\n","  === Token Prediction Diagnostic (Batch 0) ===\n","  Expected tokens: 470 (' 0'), 337 (' 1')\n","  Actually predicted tokens: {187: 10}\n","  Decoded predictions:\n","    Token 187: '\n","' (count: 10)\n","\n","  Sample predictions vs labels:\n","    Predicted: 187 ('\n","') | Label: 470 (' 0')\n","    Predicted: 187 ('\n","') | Label: 470 (' 0')\n","    Predicted: 187 ('\n","') | Label: 470 (' 0')\n","    Predicted: 187 ('\n","') | Label: 470 (' 0')\n","    Predicted: 187 ('\n","') | Label: 470 (' 0')\n","\n","  Logit values for key tokens (first 3 samples):\n","    Sample 0: token_470=' 0' logit=830.500, token_337=' 1' logit=831.000\n","    Sample 1: token_470=' 0' logit=832.000, token_337=' 1' logit=832.500\n","    Sample 2: token_470=' 0' logit=831.500, token_337=' 1' logit=831.000\n","Epoch 1/3, Batch 0/3290, Loss: 669.2621\n","Epoch 1/3, Batch 10/3290, Loss: 669.4377\n","Epoch 1/3, Batch 20/3290, Loss: 668.3129\n","Epoch 1/3, Batch 30/3290, Loss: 661.7687\n","Epoch 1/3, Batch 40/3290, Loss: 648.8789\n","\n","  === Token Prediction Diagnostic (Batch 50) ===\n","  Expected tokens: 470 (' 0'), 337 (' 1')\n","  Actually predicted tokens: {36: 1, 337: 2, 470: 7}\n","  Decoded predictions:\n","    Token 36: 'C' (count: 1)\n","    Token 337: ' 1' (count: 2)\n","    Token 470: ' 0' (count: 7)\n","\n","  Sample predictions vs labels:\n","    Predicted: 337 (' 1') | Label: 337 (' 1')\n","    Predicted: 337 (' 1') | Label: 470 (' 0')\n","    Predicted: 36 ('C') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","\n","  Logit values for key tokens (first 3 samples):\n","    Sample 0: token_470=' 0' logit=809.000, token_337=' 1' logit=808.000\n","    Sample 1: token_470=' 0' logit=812.000, token_337=' 1' logit=813.000\n","    Sample 2: token_470=' 0' logit=808.000, token_337=' 1' logit=807.500\n","Epoch 1/3, Batch 50/3290, Loss: 612.3578\n","  Class 0 accuracy: 10.22% (50/489)\n","  Class 1 accuracy: 9.52% (2/21)\n","  Predictions distribution: 50 zeros, 15 ones\n","Epoch 1/3, Batch 60/3290, Loss: 460.1245\n","Epoch 1/3, Batch 70/3290, Loss: 127.1684\n","Epoch 1/3, Batch 80/3290, Loss: 20.2505\n","Epoch 1/3, Batch 90/3290, Loss: 1.8740\n","\n","  === Token Prediction Diagnostic (Batch 100) ===\n","  Expected tokens: 470 (' 0'), 337 (' 1')\n","  Actually predicted tokens: {243: 2, 470: 7, 13076: 1}\n","  Decoded predictions:\n","    Token 243: '�' (count: 2)\n","    Token 470: ' 0' (count: 7)\n","    Token 13076: '��' (count: 1)\n","\n","  Sample predictions vs labels:\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 243 ('�') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 13076 ('��') | Label: 470 (' 0')\n","\n","  Logit values for key tokens (first 3 samples):\n","    Sample 0: token_470=' 0' logit=4.250, token_337=' 1' logit=-8.594\n","    Sample 1: token_470=' 0' logit=72.062, token_337=' 1' logit=59.875\n","    Sample 2: token_470=' 0' logit=49.406, token_337=' 1' logit=40.906\n","Epoch 1/3, Batch 100/3290, Loss: 2.6086\n","  Class 0 accuracy: 36.73% (360/980)\n","  Class 1 accuracy: 16.67% (5/30)\n","  Predictions distribution: 315 zeros, 51 ones\n","\n","--- Validation Check at Batch 100 ---\n","Validation Loss: 1.8985\n","Validation Class 0 accuracy: 75.70% (732/967)\n","Validation Class 1 accuracy: 0.00% (0/33)\n","--- End Validation Check ---\n","\n","Epoch 1/3, Batch 110/3290, Loss: 0.4062\n","Epoch 1/3, Batch 120/3290, Loss: 0.4023\n","Epoch 1/3, Batch 130/3290, Loss: 1.1271\n","Epoch 1/3, Batch 140/3290, Loss: 0.5700\n","\n","  === Token Prediction Diagnostic (Batch 150) ===\n","  Expected tokens: 470 (' 0'), 337 (' 1')\n","  Actually predicted tokens: {337: 1, 470: 9}\n","  Decoded predictions:\n","    Token 337: ' 1' (count: 1)\n","    Token 470: ' 0' (count: 9)\n","\n","  Sample predictions vs labels:\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 337 (' 1') | Label: 337 (' 1')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","\n","  Logit values for key tokens (first 3 samples):\n","    Sample 0: token_470=' 0' logit=45.781, token_337=' 1' logit=34.438\n","    Sample 1: token_470=' 0' logit=46.000, token_337=' 1' logit=35.938\n","    Sample 2: token_470=' 0' logit=47.531, token_337=' 1' logit=38.281\n","Epoch 1/3, Batch 150/3290, Loss: 0.7928\n","  Class 0 accuracy: 55.87% (819/1466)\n","  Class 1 accuracy: 25.00% (11/44)\n","  Predictions distribution: 467 zeros, 25 ones\n","Epoch 1/3, Batch 160/3290, Loss: 0.2021\n","Epoch 1/3, Batch 170/3290, Loss: 0.3050\n","Epoch 1/3, Batch 180/3290, Loss: 0.1908\n","Epoch 1/3, Batch 190/3290, Loss: 0.3825\n","\n","  === Token Prediction Diagnostic (Batch 200) ===\n","  Expected tokens: 470 (' 0'), 337 (' 1')\n","  Actually predicted tokens: {470: 10}\n","  Decoded predictions:\n","    Token 470: ' 0' (count: 10)\n","\n","  Sample predictions vs labels:\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","\n","  Logit values for key tokens (first 3 samples):\n","    Sample 0: token_470=' 0' logit=58.000, token_337=' 1' logit=49.188\n","    Sample 1: token_470=' 0' logit=60.656, token_337=' 1' logit=50.719\n","    Sample 2: token_470=' 0' logit=48.625, token_337=' 1' logit=38.688\n","Epoch 1/3, Batch 200/3290, Loss: 0.1488\n","  Class 0 accuracy: 66.77% (1302/1950)\n","  Class 1 accuracy: 45.00% (27/60)\n","  Predictions distribution: 483 zeros, 17 ones\n","\n","--- Validation Check at Batch 200 ---\n","Validation Loss: 0.0010\n","Validation Class 0 accuracy: 100.00% (967/967)\n","Validation Class 1 accuracy: 100.00% (33/33)\n","--- End Validation Check ---\n","\n","Epoch 1/3, Batch 210/3290, Loss: 0.2542\n","Epoch 1/3, Batch 220/3290, Loss: 0.0974\n","Epoch 1/3, Batch 230/3290, Loss: 0.3827\n","Epoch 1/3, Batch 240/3290, Loss: 0.0995\n","\n","  === Token Prediction Diagnostic (Batch 250) ===\n","  Expected tokens: 470 (' 0'), 337 (' 1')\n","  Actually predicted tokens: {470: 10}\n","  Decoded predictions:\n","    Token 470: ' 0' (count: 10)\n","\n","  Sample predictions vs labels:\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","\n","  Logit values for key tokens (first 3 samples):\n","    Sample 0: token_470=' 0' logit=47.031, token_337=' 1' logit=40.906\n","    Sample 1: token_470=' 0' logit=29.922, token_337=' 1' logit=24.578\n","    Sample 2: token_470=' 0' logit=70.312, token_337=' 1' logit=62.281\n","Epoch 1/3, Batch 250/3290, Loss: 0.1785\n","  Class 0 accuracy: 73.39% (1787/2435)\n","  Class 1 accuracy: 56.00% (42/75)\n","  Predictions distribution: 485 zeros, 15 ones\n","Epoch 1/3, Batch 260/3290, Loss: 0.3048\n","Epoch 1/3, Batch 270/3290, Loss: 0.1765\n","Epoch 1/3, Batch 280/3290, Loss: 0.3256\n","Epoch 1/3, Batch 290/3290, Loss: 0.1469\n","\n","  === Token Prediction Diagnostic (Batch 300) ===\n","  Expected tokens: 470 (' 0'), 337 (' 1')\n","  Actually predicted tokens: {243: 1, 470: 9}\n","  Decoded predictions:\n","    Token 243: '�' (count: 1)\n","    Token 470: ' 0' (count: 9)\n","\n","  Sample predictions vs labels:\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 243 ('�') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","\n","  Logit values for key tokens (first 3 samples):\n","    Sample 0: token_470=' 0' logit=9.594, token_337=' 1' logit=3.129\n","    Sample 1: token_470=' 0' logit=18.516, token_337=' 1' logit=12.336\n","    Sample 2: token_470=' 0' logit=1.369, token_337=' 1' logit=-7.855\n","Epoch 1/3, Batch 300/3290, Loss: 1.0232\n","  Class 0 accuracy: 77.80% (2274/2923)\n","  Class 1 accuracy: 62.07% (54/87)\n","  Predictions distribution: 487 zeros, 12 ones\n","\n","--- Validation Check at Batch 300 ---\n","Validation Loss: 0.1840\n","Validation Class 0 accuracy: 99.59% (963/967)\n","Validation Class 1 accuracy: 72.73% (24/33)\n","--- End Validation Check ---\n","\n","Epoch 1/3, Batch 310/3290, Loss: 0.2329\n","Epoch 1/3, Batch 320/3290, Loss: 0.3116\n","Epoch 1/3, Batch 330/3290, Loss: 0.0604\n","Epoch 1/3, Batch 340/3290, Loss: 0.1175\n","\n","  === Token Prediction Diagnostic (Batch 350) ===\n","  Expected tokens: 470 (' 0'), 337 (' 1')\n","  Actually predicted tokens: {337: 1, 470: 9}\n","  Decoded predictions:\n","    Token 337: ' 1' (count: 1)\n","    Token 470: ' 0' (count: 9)\n","\n","  Sample predictions vs labels:\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 337 (' 1') | Label: 337 (' 1')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","\n","  Logit values for key tokens (first 3 samples):\n","    Sample 0: token_470=' 0' logit=17.938, token_337=' 1' logit=25.047\n","    Sample 1: token_470=' 0' logit=25.797, token_337=' 1' logit=17.047\n","    Sample 2: token_470=' 0' logit=37.125, token_337=' 1' logit=27.703\n","Epoch 1/3, Batch 350/3290, Loss: 0.1371\n","  Class 0 accuracy: 80.77% (2742/3395)\n","  Class 1 accuracy: 70.43% (81/115)\n","  Predictions distribution: 468 zeros, 27 ones\n","Epoch 1/3, Batch 360/3290, Loss: 0.0920\n","Epoch 1/3, Batch 370/3290, Loss: 0.0396\n","Epoch 1/3, Batch 380/3290, Loss: 0.0516\n","Epoch 1/3, Batch 390/3290, Loss: 0.1018\n","\n","  === Token Prediction Diagnostic (Batch 400) ===\n","  Expected tokens: 470 (' 0'), 337 (' 1')\n","  Actually predicted tokens: {470: 10}\n","  Decoded predictions:\n","    Token 470: ' 0' (count: 10)\n","\n","  Sample predictions vs labels:\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","\n","  Logit values for key tokens (first 3 samples):\n","    Sample 0: token_470=' 0' logit=48.125, token_337=' 1' logit=39.562\n","    Sample 1: token_470=' 0' logit=48.594, token_337=' 1' logit=40.750\n","    Sample 2: token_470=' 0' logit=48.625, token_337=' 1' logit=42.156\n","Epoch 1/3, Batch 400/3290, Loss: 0.0698\n","  Class 0 accuracy: 83.17% (3226/3879)\n","  Class 1 accuracy: 74.05% (97/131)\n","  Predictions distribution: 484 zeros, 16 ones\n","\n","--- Validation Check at Batch 400 ---\n","Validation Loss: 0.0016\n","Validation Class 0 accuracy: 100.00% (967/967)\n","Validation Class 1 accuracy: 100.00% (33/33)\n","--- End Validation Check ---\n","\n","Epoch 1/3, Batch 410/3290, Loss: 0.0593\n","Epoch 1/3, Batch 420/3290, Loss: 0.1202\n","Epoch 1/3, Batch 430/3290, Loss: 0.1632\n","Epoch 1/3, Batch 440/3290, Loss: 0.0530\n","\n","  === Token Prediction Diagnostic (Batch 450) ===\n","  Expected tokens: 470 (' 0'), 337 (' 1')\n","  Actually predicted tokens: {470: 10}\n","  Decoded predictions:\n","    Token 470: ' 0' (count: 10)\n","\n","  Sample predictions vs labels:\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","\n","  Logit values for key tokens (first 3 samples):\n","    Sample 0: token_470=' 0' logit=28.578, token_337=' 1' logit=18.156\n","    Sample 1: token_470=' 0' logit=24.516, token_337=' 1' logit=18.031\n","    Sample 2: token_470=' 0' logit=33.031, token_337=' 1' logit=25.922\n","Epoch 1/3, Batch 450/3290, Loss: 0.0779\n","  Class 0 accuracy: 85.04% (3712/4365)\n","  Class 1 accuracy: 76.55% (111/145)\n","  Predictions distribution: 486 zeros, 14 ones\n","Epoch 1/3, Batch 460/3290, Loss: 0.1044\n","Epoch 1/3, Batch 470/3290, Loss: 0.0424\n","Epoch 1/3, Batch 480/3290, Loss: 0.0550\n","Epoch 1/3, Batch 490/3290, Loss: 0.0576\n","\n","  === Token Prediction Diagnostic (Batch 500) ===\n","  Expected tokens: 470 (' 0'), 337 (' 1')\n","  Actually predicted tokens: {337: 1, 470: 9}\n","  Decoded predictions:\n","    Token 337: ' 1' (count: 1)\n","    Token 470: ' 0' (count: 9)\n","\n","  Sample predictions vs labels:\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","    Predicted: 337 (' 1') | Label: 337 (' 1')\n","    Predicted: 470 (' 0') | Label: 470 (' 0')\n","\n","  Logit values for key tokens (first 3 samples):\n","    Sample 0: token_470=' 0' logit=39.812, token_337=' 1' logit=32.125\n","    Sample 1: token_470=' 0' logit=35.125, token_337=' 1' logit=25.734\n","    Sample 2: token_470=' 0' logit=21.797, token_337=' 1' logit=26.172\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-296110432.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Train with focal loss (use the new function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m trained_model = train_with_focal_loss(  # Changed from train_with_pe\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1474454122.py\u001b[0m in \u001b[0;36mtrain_with_focal_loss\u001b[0;34m(model, train_loader, val_loader, epochs, learning_rate, device, tokenizer, save_checkpoint_at_batch, checkpoint_path)\u001b[0m\n\u001b[1;32m    383\u001b[0m                     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m                 \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                 \u001b[0mvalid_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}