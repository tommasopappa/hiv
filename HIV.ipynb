{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPsFff9CoN0Ix9fm5dVtQT+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ab548c52261e434aad3d7e4c83355706":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_832eebafc7484ce8b25002a8370fced0","IPY_MODEL_22bf9b97f8cb4bb4bac266ad53e71af3","IPY_MODEL_812c0638f69a4d3eb872be0866ccf12c"],"layout":"IPY_MODEL_9616b827d2d74372bfdaa71f6c0e6e1d"}},"832eebafc7484ce8b25002a8370fced0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e28dcff068e9415aa350dcb0277fdd99","placeholder":"​","style":"IPY_MODEL_21183786eae64b9a834d3b5e56bf9007","value":"tokenizer_config.json: 100%"}},"22bf9b97f8cb4bb4bac266ad53e71af3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_63c9f80f3c4e423eb3e9d8e8b806c3b7","max":166,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9d6850b2ed364c069b220c092bf16d94","value":166}},"812c0638f69a4d3eb872be0866ccf12c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12f1f8328eba4977a6049b2f7410d97f","placeholder":"​","style":"IPY_MODEL_6d7f4840c1ac4bc883ea7bec071bea93","value":" 166/166 [00:00&lt;00:00, 5.22kB/s]"}},"9616b827d2d74372bfdaa71f6c0e6e1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e28dcff068e9415aa350dcb0277fdd99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21183786eae64b9a834d3b5e56bf9007":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"63c9f80f3c4e423eb3e9d8e8b806c3b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d6850b2ed364c069b220c092bf16d94":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"12f1f8328eba4977a6049b2f7410d97f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d7f4840c1ac4bc883ea7bec071bea93":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d93e5710df2346bb85cd179dc66bd892":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9fa71e7b8d514bbcb949f18a0db56e06","IPY_MODEL_a93c901260c24bedbda31244da1079a7","IPY_MODEL_e77d0abc27924123b1cbac198af2d99c"],"layout":"IPY_MODEL_c9cca902eed84f8d97eec3c98fd330b0"}},"9fa71e7b8d514bbcb949f18a0db56e06":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02ea30ba28654b24947053423b369334","placeholder":"​","style":"IPY_MODEL_9cf6b00d64b949f99db192803a4710bf","value":"config.json: 100%"}},"a93c901260c24bedbda31244da1079a7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8ee369a8b414afea3dc7ae0db2b5c3b","max":501,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2ffa9a7bf0274a779d4b765f512a2ad6","value":501}},"e77d0abc27924123b1cbac198af2d99c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6fe4cf837a0f46b785caf2523b90b278","placeholder":"​","style":"IPY_MODEL_497fadf3fbce48b4868f290c723c49c7","value":" 501/501 [00:00&lt;00:00, 8.65kB/s]"}},"c9cca902eed84f8d97eec3c98fd330b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02ea30ba28654b24947053423b369334":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9cf6b00d64b949f99db192803a4710bf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a8ee369a8b414afea3dc7ae0db2b5c3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ffa9a7bf0274a779d4b765f512a2ad6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6fe4cf837a0f46b785caf2523b90b278":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"497fadf3fbce48b4868f290c723c49c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c51720933555469fb8e416f81ed7b29e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eb488dc60e054fd8b0fac5c2b258abb7","IPY_MODEL_f750d79c4c32473b9c8fe0dbb590f3f1","IPY_MODEL_492555e335de451ab2a3118b32f43af0"],"layout":"IPY_MODEL_4035986f7b5341f09be2610963d11623"}},"eb488dc60e054fd8b0fac5c2b258abb7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a720852377074ccd8e6df826ad8ef99c","placeholder":"​","style":"IPY_MODEL_9c439f3882364cd28bfa25aa763b3e90","value":"vocab.json: "}},"f750d79c4c32473b9c8fe0dbb590f3f1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3fbef546bcbb442cba6345224ff1d7b1","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cc9414a17e214d39bee09d7e6ef60ce0","value":1}},"492555e335de451ab2a3118b32f43af0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b902f98ca55d4254818012797ffa74a8","placeholder":"​","style":"IPY_MODEL_4dd625fb39bb4cf19952b687c451003b","value":" 9.43k/? [00:00&lt;00:00, 238kB/s]"}},"4035986f7b5341f09be2610963d11623":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a720852377074ccd8e6df826ad8ef99c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c439f3882364cd28bfa25aa763b3e90":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3fbef546bcbb442cba6345224ff1d7b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"cc9414a17e214d39bee09d7e6ef60ce0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b902f98ca55d4254818012797ffa74a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4dd625fb39bb4cf19952b687c451003b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc3c63323ac44030a22aac4773ad0183":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ed71fd30066e497286a4870f4e9f8d46","IPY_MODEL_8a26d19c331c4111bc7a0c7cd86884df","IPY_MODEL_1cb7dd128c9841ff86a6b44e476b697f"],"layout":"IPY_MODEL_acd2daa9530a4350b930a3db8985cfc3"}},"ed71fd30066e497286a4870f4e9f8d46":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de24593c664041d2bda6e0f05c210ba8","placeholder":"​","style":"IPY_MODEL_d4dfdb30fe50423b97f70b9d63f02bd0","value":"merges.txt: "}},"8a26d19c331c4111bc7a0c7cd86884df":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e65ed00cacf474881db014289b9a8fa","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4b833554a24b413fb5c855e33263ac78","value":1}},"1cb7dd128c9841ff86a6b44e476b697f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af836433bb114d80912a902a838eb5d7","placeholder":"​","style":"IPY_MODEL_f1d7a34d74014bf7b38ce840290579d6","value":" 3.21k/? [00:00&lt;00:00, 115kB/s]"}},"acd2daa9530a4350b930a3db8985cfc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de24593c664041d2bda6e0f05c210ba8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4dfdb30fe50423b97f70b9d63f02bd0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e65ed00cacf474881db014289b9a8fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"4b833554a24b413fb5c855e33263ac78":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"af836433bb114d80912a902a838eb5d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1d7a34d74014bf7b38ce840290579d6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4634165f8ea4d3d9f78f8a3a1098b49":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_640a91bdc297406a82f615bcb4eda27e","IPY_MODEL_0608ae010bc34a6eb14f0e569bd1a70e","IPY_MODEL_0f1ef3645297488cadfa054a0f9abb5d"],"layout":"IPY_MODEL_1a1affd13d074e639c5b5857f6f0f045"}},"640a91bdc297406a82f615bcb4eda27e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5a8d03009d942718170b0909d70e6b9","placeholder":"​","style":"IPY_MODEL_e7e3f6b5bd8246588470c0fa83162ecf","value":"special_tokens_map.json: 100%"}},"0608ae010bc34a6eb14f0e569bd1a70e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d19809b5cae6424ebb0cb1da24ca6d2b","max":150,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e13a9de2efaa4d1ab99fed8fb49cde6a","value":150}},"0f1ef3645297488cadfa054a0f9abb5d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_234f906da31a4bb58dfa0e0ca087a4eb","placeholder":"​","style":"IPY_MODEL_557b5726fd1e4658b3c348d117f4b76d","value":" 150/150 [00:00&lt;00:00, 2.84kB/s]"}},"1a1affd13d074e639c5b5857f6f0f045":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5a8d03009d942718170b0909d70e6b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7e3f6b5bd8246588470c0fa83162ecf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d19809b5cae6424ebb0cb1da24ca6d2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e13a9de2efaa4d1ab99fed8fb49cde6a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"234f906da31a4bb58dfa0e0ca087a4eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"557b5726fd1e4658b3c348d117f4b76d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# Install only what's needed\n","!pip install torch-geometric -q\n","!pip install rdkit\n","import torch\n","from torch.utils.data import IterableDataset\n","from torch_geometric.datasets import MoleculeNet\n","import pandas as pd\n","import itertools\n","import sys\n","from google.colab import drive\n","import os\n","\n","gdrive_path='/content/gdrive/MyDrive/Project_HIV'\n","\n","# This will mount your google drive under 'MyDrive'\n","drive.mount('/content/gdrive', force_remount=True)\n","# In order to access the files in this notebook we have to navigate to the correct folder\n","os.chdir(gdrive_path)\n","# Check manually if all files are present\n","print(sorted(os.listdir()))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HBpIix3WkoOg","executionInfo":{"status":"ok","timestamp":1756491436594,"user_tz":-120,"elapsed":102856,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"outputId":"d966b163-b868-4a88-db3b-0f4295b88892"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting rdkit\n","  Downloading rdkit-2025.3.5-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rdkit) (2.0.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rdkit) (11.3.0)\n","Downloading rdkit-2025.3.5-cp312-cp312-manylinux_2_28_x86_64.whl (36.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.2/36.2 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: rdkit\n","Successfully installed rdkit-2025.3.5\n","Mounted at /content/gdrive\n","['.git', 'HIV.ipynb', 'README.md', '__pycache__', 'code', 'data_import.py', 'datasets', 'download_tinyllama.sh', 'hiv_dataloader.py', 'hiv_dataloader_with_pe.py', 'hiv_train.jsonl', 'hiv_val.jsonl', 'train_hiv_lora.py']\n"]}]},{"cell_type":"code","source":["# Add code directory to path\n","from torch_geometric.datasets import MoleculeNet\n","# sys.path.insert(0, os.path.join(os.getcwd(), 'code'))\n","\n","# # Import the Colab-friendly version\n","# from data.dataset_colab import PartialHIVDataset\n","\n","class PartialHIVDataset(IterableDataset):\n","    \"\"\"\n","    IterableDataset that loads only a small portion of the HIV dataset.\n","    \"\"\"\n","\n","    def __init__(self, root='/tmp/HIV', max_samples=10):\n","        \"\"\"\n","        Args:\n","            root: Root directory for dataset storage\n","            max_samples: Maximum number of samples to load\n","        \"\"\"\n","        self.root = root\n","        self.max_samples = max_samples\n","        self._dataset = None\n","\n","    def _lazy_load_dataset(self):\n","        \"\"\"Lazy load the dataset only when iteration begins\"\"\"\n","        if self._dataset is None:\n","            print(f\"Initializing HIV dataset (will only load {self.max_samples} samples)...\")\n","            # Import here to ensure it's available\n","\n","            self._dataset = MoleculeNet(root=self.root, name='HIV')\n","            print(f\"Dataset ready! Total size: {len(self._dataset)} molecules\")\n","            print(f\"But we'll only load {self.max_samples} of them.\\n\")\n","\n","    def parse_molecules(self):\n","        \"\"\"\n","        Parse molecules from the dataset, stopping after max_samples.\n","        \"\"\"\n","        self._lazy_load_dataset()\n","\n","        for i in range(min(self.max_samples, len(self._dataset))):\n","            data = self._dataset[i]\n","\n","            # Extract molecule information\n","            mol_info = {\n","                'source': f\"molecule_{i}\",  # Similar to your CustomIterableDataset\n","                'target': data.y.item(),    # HIV activity label\n","                'num_atoms': data.num_nodes,\n","                'num_bonds': data.num_edges // 2,  # Undirected edges\n","                'smiles': getattr(data, 'smiles', 'N/A')\n","            }\n","\n","            yield mol_info\n","\n","    def __iter__(self):\n","        \"\"\"Iterator with worker support\"\"\"\n","        iterator = self.parse_molecules()\n","        worker_info = torch.utils.data.get_worker_info()\n","\n","        if worker_info is not None:\n","            worker_total_num = worker_info.num_workers\n","            worker_id = worker_info.id\n","            return itertools.islice(iterator, worker_id, None, worker_total_num)\n","\n","        return iterator\n","\n","\n","# Your code\n","batch_list = []\n","batch_size = 40\n","\n","# Create dataset\n","dataset = PartialHIVDataset(max_samples=batch_size)\n","iterator = iter(dataset)\n","full_batch_data = list(dataset)\n","\n","df = pd.DataFrame(full_batch_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uCZHJO55qvlC","executionInfo":{"status":"ok","timestamp":1756491522452,"user_tz":-120,"elapsed":85841,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"outputId":"cf5041d5-da03-4e00-8b20-91e880c36f5a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing HIV dataset (will only load 40 samples)...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/HIV.csv\n","Processing...\n","/usr/local/lib/python3.12/dist-packages/torch_geometric/datasets/molecule_net.py:213: UserWarning: Skipping molecule 'O=C1O[Al]23(OC1=O)(OC(=O)C(=O)O2)OC(=O)C(=O)O3' since it resulted in zero atoms\n","  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n","/usr/local/lib/python3.12/dist-packages/torch_geometric/datasets/molecule_net.py:213: UserWarning: Skipping molecule 'Cc1ccc([B-2]2(c3ccc(C)cc3)=NCCO2)cc1' since it resulted in zero atoms\n","  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n","/usr/local/lib/python3.12/dist-packages/torch_geometric/datasets/molecule_net.py:213: UserWarning: Skipping molecule 'Oc1ccc(C2Oc3cc(O)cc4c3C(=[O+][AlH3-3]35([O+]=C6c7c(cc(O)cc7[OH+]3)OC(c3ccc(O)cc3O)C6O)([O+]=C3c6c(cc(O)cc6[OH+]5)OC(c5ccc(O)cc5O)C3O)[OH+]4)C2O)c(O)c1' since it resulted in zero atoms\n","  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n","/usr/local/lib/python3.12/dist-packages/torch_geometric/datasets/molecule_net.py:213: UserWarning: Skipping molecule 'CC1=C2[OH+][AlH3-3]34([O+]=C2C=CN1C)([O+]=C1C=CN(C)C(C)=C1[OH+]3)[O+]=C1C=CN(C)C(C)=C1[OH+]4' since it resulted in zero atoms\n","  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n","/usr/local/lib/python3.12/dist-packages/torch_geometric/datasets/molecule_net.py:213: UserWarning: Skipping molecule 'CC(c1cccs1)=[N+]1[N-]C(N)=[S+][AlH3-]12[OH+]B(c1ccccc1)[OH+]2' since it resulted in zero atoms\n","  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n","/usr/local/lib/python3.12/dist-packages/torch_geometric/datasets/molecule_net.py:213: UserWarning: Skipping molecule 'CC(c1ccccn1)=[N+]1[N-]C(N)=[S+][AlH3-]12[OH+]B(c1ccccc1)[OH+]2' since it resulted in zero atoms\n","  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n","/usr/local/lib/python3.12/dist-packages/torch_geometric/datasets/molecule_net.py:213: UserWarning: Skipping molecule '[Na+].c1ccc([SH+][GeH2+]2[SH+]c3ccccc3[SH+]2)c([SH+][GeH2+]2[SH+]c3ccccc3[SH+]2)c1' since it resulted in zero atoms\n","  warnings.warn(f\"Skipping molecule '{smiles}' since it \"\n","Done!\n"]},{"output_type":"stream","name":"stdout","text":["Dataset ready! Total size: 41120 molecules\n","But we'll only load 40 of them.\n","\n"]}]},{"cell_type":"code","source":["import codecs\n","\n","import numpy as np\n","import torch.nn as nn\n","from transformers import AutoTokenizer\n","\n","smiles_list = []\n","batch_size = 40\n","\n","\n","# Iterate through the dataset and fill it with the SMILES strings\n","for _ in range(batch_size):\n","  mol_info = next(iterator)\n","  smiles_list.append(mol_info['smiles'])\n","\n","\n","\n","# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n","\n","# First, tokenize without padding to find the actual max length\n","tokenized_lengths = []\n","for smiles in smiles_list:\n","    tokens = tokenizer(smiles, padding=False, truncation=False)\n","    tokenized_lengths.append(len(tokens['input_ids']))\n","\n","# Determine max_length from your data\n","max_length = max(tokenized_lengths)\n","print(f\"Maximum sequence length in data: {max_length}\")\n","\n","\n","# Now tokenize with the data-driven max_length\n","encoded = tokenizer(\n","    smiles_list,\n","    padding='max_length',\n","    truncation=True,\n","    max_length=max_length,\n","    return_tensors='pt'\n",")\n","\n","# Create embedding layer\n","embedding = nn.Embedding(\n","    num_embeddings=len(tokenizer),\n","    embedding_dim=512,\n","    padding_idx=tokenizer.pad_token_id\n",")\n","\n","# Get embeddings directly\n","embeddings = embedding(encoded['input_ids'])\n","print(f\"Embeddings shape: {embeddings.shape}\")  # (batch_size, max_length, 512)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":340,"referenced_widgets":["ab548c52261e434aad3d7e4c83355706","832eebafc7484ce8b25002a8370fced0","22bf9b97f8cb4bb4bac266ad53e71af3","812c0638f69a4d3eb872be0866ccf12c","9616b827d2d74372bfdaa71f6c0e6e1d","e28dcff068e9415aa350dcb0277fdd99","21183786eae64b9a834d3b5e56bf9007","63c9f80f3c4e423eb3e9d8e8b806c3b7","9d6850b2ed364c069b220c092bf16d94","12f1f8328eba4977a6049b2f7410d97f","6d7f4840c1ac4bc883ea7bec071bea93","d93e5710df2346bb85cd179dc66bd892","9fa71e7b8d514bbcb949f18a0db56e06","a93c901260c24bedbda31244da1079a7","e77d0abc27924123b1cbac198af2d99c","c9cca902eed84f8d97eec3c98fd330b0","02ea30ba28654b24947053423b369334","9cf6b00d64b949f99db192803a4710bf","a8ee369a8b414afea3dc7ae0db2b5c3b","2ffa9a7bf0274a779d4b765f512a2ad6","6fe4cf837a0f46b785caf2523b90b278","497fadf3fbce48b4868f290c723c49c7","c51720933555469fb8e416f81ed7b29e","eb488dc60e054fd8b0fac5c2b258abb7","f750d79c4c32473b9c8fe0dbb590f3f1","492555e335de451ab2a3118b32f43af0","4035986f7b5341f09be2610963d11623","a720852377074ccd8e6df826ad8ef99c","9c439f3882364cd28bfa25aa763b3e90","3fbef546bcbb442cba6345224ff1d7b1","cc9414a17e214d39bee09d7e6ef60ce0","b902f98ca55d4254818012797ffa74a8","4dd625fb39bb4cf19952b687c451003b","dc3c63323ac44030a22aac4773ad0183","ed71fd30066e497286a4870f4e9f8d46","8a26d19c331c4111bc7a0c7cd86884df","1cb7dd128c9841ff86a6b44e476b697f","acd2daa9530a4350b930a3db8985cfc3","de24593c664041d2bda6e0f05c210ba8","d4dfdb30fe50423b97f70b9d63f02bd0","2e65ed00cacf474881db014289b9a8fa","4b833554a24b413fb5c855e33263ac78","af836433bb114d80912a902a838eb5d7","f1d7a34d74014bf7b38ce840290579d6","f4634165f8ea4d3d9f78f8a3a1098b49","640a91bdc297406a82f615bcb4eda27e","0608ae010bc34a6eb14f0e569bd1a70e","0f1ef3645297488cadfa054a0f9abb5d","1a1affd13d074e639c5b5857f6f0f045","e5a8d03009d942718170b0909d70e6b9","e7e3f6b5bd8246588470c0fa83162ecf","d19809b5cae6424ebb0cb1da24ca6d2b","e13a9de2efaa4d1ab99fed8fb49cde6a","234f906da31a4bb58dfa0e0ca087a4eb","557b5726fd1e4658b3c348d117f4b76d"]},"id":"oRsTzvL0l5bx","executionInfo":{"status":"ok","timestamp":1756491527147,"user_tz":-120,"elapsed":4675,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"outputId":"2c251318-3cf5-4ce1-8695-f3b1eddd66d4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/166 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab548c52261e434aad3d7e4c83355706"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/501 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d93e5710df2346bb85cd179dc66bd892"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c51720933555469fb8e416f81ed7b29e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc3c63323ac44030a22aac4773ad0183"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4634165f8ea4d3d9f78f8a3a1098b49"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Maximum sequence length in data: 61\n","Embeddings shape: torch.Size([40, 61, 512])\n"]}]},{"cell_type":"code","source":["from scipy.sparse.linalg import eigsh\n","from rdkit import Chem\n","def smiles_to_adjacency_matrix(smiles):\n","    \"\"\"Convert SMILES to adjacency matrix.\"\"\"\n","    mol = Chem.MolFromSmiles(smiles)\n","    if mol is None:\n","        return None\n","\n","    n_atoms = mol.GetNumAtoms()\n","    adj_matrix = np.zeros((n_atoms, n_atoms))\n","\n","    for bond in mol.GetBonds():\n","        i = bond.GetBeginAtomIdx()\n","        j = bond.GetEndAtomIdx()\n","        # Undirected graph\n","        adj_matrix[i, j] = 1\n","        adj_matrix[j, i] = 1\n","\n","    return adj_matrix\n","\n","def compute_graph_positional_encoding(adj_matrix, k=30):\n","    \"\"\"\n","    Compute graph positional encodings from eigenvectors of the\n","    symmetrically normalized graph Laplacian.\n","    \"\"\"\n","    n = adj_matrix.shape[0]\n","\n","    # Compute degree matrix\n","    degree = np.sum(adj_matrix, axis=1)\n","    degree[degree == 0] = 1\n","\n","    # D^(-1/2)\n","    d_inv_sqrt = np.diag(1.0 / np.sqrt(degree))\n","\n","    # Symmetrically normalized Laplacian\n","    identity = np.eye(n)\n","    normalized_adj = d_inv_sqrt @ adj_matrix @ d_inv_sqrt\n","    laplacian = identity - normalized_adj\n","\n","    # Compute eigenvectors\n","    if n < k:\n","        # If graph has fewer nodes than k, pad with zeros\n","        eigenvalues, eigenvectors = np.linalg.eigh(laplacian)\n","        # Pad eigenvectors to have k columns\n","        padded_eigenvectors = np.zeros((n, k))\n","        padded_eigenvectors[:, :n] = eigenvectors\n","        return padded_eigenvectors\n","    else:\n","        eigenvalues, eigenvectors = eigsh(laplacian, k=k, which='SM')\n","        return eigenvectors"],"metadata":{"id":"5VXPZypgg5_P","executionInfo":{"status":"ok","timestamp":1756491527168,"user_tz":-120,"elapsed":15,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Process all SMILES and compute their graph PEs\n","graph_pes_list = []\n","max_nodes = 0\n","\n","for smiles in smiles_list:\n","    adj_matrix = smiles_to_adjacency_matrix(smiles)\n","    if adj_matrix is not None:\n","        pe = compute_graph_positional_encoding(adj_matrix, k=30)\n","        graph_pes_list.append(pe)\n","        max_nodes = max(max_nodes, pe.shape[0])\n","    else:\n","        graph_pes_list.append(None)\n","\n","print(f\"Maximum number of atoms in dataset: {max_nodes}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QadoBoVYgrrz","executionInfo":{"status":"ok","timestamp":1756491527286,"user_tz":-120,"elapsed":116,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"outputId":"81b53acf-c644-474c-8f3d-3f79a8d36c9b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum number of atoms in dataset: 42\n"]}]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","from rdkit import Chem\n","import re\n","\n","def generate_random_orthonormal_pe(n_vectors, dim=30):\n","    \"\"\"Generate n_vectors random orthonormal vectors of dimension dim.\"\"\"\n","    if n_vectors == 0:\n","        return np.zeros((0, dim))\n","\n","    # Generate random matrix\n","    random_matrix = np.random.randn(dim, n_vectors)\n","\n","    # Use QR decomposition to get orthonormal vectors\n","    Q, _ = np.linalg.qr(random_matrix)\n","\n","    # Return first n_vectors columns (transposed to have shape (n_vectors, dim))\n","    return Q[:, :n_vectors].T\n","\n","def parse_token_components(token):\n","    \"\"\"\n","    Parse a token to identify atoms and characters.\n","    Returns: (atom_indices, n_characters)\n","    \"\"\"\n","    # Common atom patterns in SMILES\n","    atom_pattern = r'(Cl|Br|Si|Mg|Ca|Fe|Al|Na|Li|[BCNOFPSKHIV])'\n","\n","    # Find all atoms in the token\n","    atoms = re.findall(atom_pattern, token)\n","\n","    # Count non-atom characters\n","    # Remove atoms from token to count remaining characters\n","    remaining = token\n","    for atom in atoms:\n","        remaining = remaining.replace(atom, '', 1)\n","\n","    # Count actual characters (digits, +, -, =, #, etc.)\n","    n_characters = len([c for c in remaining])\n","\n","    return atoms, n_characters\n","\n","def align_graph_pe_to_tokens(smiles, graph_pe, tokenizer, max_length,random_seed=None):\n","    \"\"\"\n","    Align graph PE to tokens, handling pure characters, atoms, and mixed tokens.\n","\n","    Args:\n","        smiles: SMILES string\n","        graph_pe: Graph positional encoding for atoms (n_atoms, embedding_dim)\n","        tokenizer: Tokenizer object\n","        max_length: Maximum sequence length\n","        random_seed: Random seed for reproducible random PEs\n","    \"\"\"\n","    if random_seed is not None:\n","        np.random.seed(random_seed)\n","    # Determine embedding dimension\n","    if graph_pe is not None:\n","        embedding_dim = graph_pe.shape[1]\n","    elif embedding_dim is None:\n","        embedding_dim = 30  # Default fallback\n","\n","    mol = Chem.MolFromSmiles(smiles)\n","    if mol is None or graph_pe is None:\n","        return torch.zeros(max_length, embedding_dim)\n","\n","    # Get tokens\n","    encoding = tokenizer(smiles, padding='max_length', max_length=max_length)\n","    tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n","\n","\n","    # Initialize token PE\n","    token_pe = torch.zeros(max_length, embedding_dim)\n","\n","    # Build atom mapping from SMILES\n","    atom_symbols = [atom.GetSymbol() for atom in mol.GetAtoms()]\n","    atom_count = {symbol: 0 for symbol in set(atom_symbols)}\n","    atom_to_idx = {}\n","\n","    for idx, symbol in enumerate(atom_symbols):\n","        atom_to_idx[(symbol, atom_count[symbol])] = idx\n","        atom_count[symbol] += 1\n","\n","    # Reset atom count for tracking\n","    current_atom_count = {symbol: 0 for symbol in atom_count}\n","\n","    # Process each token\n","    for i, token in enumerate(tokens):\n","        if token in ['<s>', '</s>', '<pad>']:\n","            continue\n","\n","        # Parse token components\n","        atoms_in_token, n_characters = parse_token_components(token)\n","\n","        if len(atoms_in_token) == 0 and n_characters > 0:\n","            # Pure character token - use random orthonormal PE\n","            random_pes = generate_random_orthonormal_pe(n_characters, embedding_dim)\n","            token_pe[i] = torch.tensor(random_pes.sum(axis=0))\n","\n","        elif len(atoms_in_token) > 0 and n_characters == 0:\n","            # Pure atom token(s) - use graph PE\n","            atom_pes = []\n","            for atom_symbol in atoms_in_token:\n","                if atom_symbol in current_atom_count:\n","                    atom_key = (atom_symbol, current_atom_count[atom_symbol])\n","                    if atom_key in atom_to_idx:\n","                        atom_idx = atom_to_idx[atom_key]\n","                        atom_pes.append(graph_pe[atom_idx])\n","                        current_atom_count[atom_symbol] += 1\n","\n","            if atom_pes:\n","                # sum the PEs of all atoms in this token\n","                token_pe[i] = torch.tensor(np.sum(atom_pes, axis=0))\n","\n","        elif len(atoms_in_token) > 0 and n_characters > 0:\n","            # Mixed token - combine atom PE and character PE\n","            # Get atom PEs\n","            atom_pes = []\n","            for atom_symbol in atoms_in_token:\n","                if atom_symbol in current_atom_count:\n","                    atom_key = (atom_symbol, current_atom_count[atom_symbol])\n","                    if atom_key in atom_to_idx:\n","                        atom_idx = atom_to_idx[atom_key]\n","                        atom_pes.append(graph_pe[atom_idx])\n","                        current_atom_count[atom_symbol] += 1\n","\n","            # Get character PEs\n","            random_pes = generate_random_orthonormal_pe(n_characters, embedding_dim)\n","\n","            # Combine: sum of atoms + sum of characters\n","            combined_pe = np.zeros(embedding_dim)\n","            if atom_pes:\n","                combined_pe += np.sum(atom_pes, axis=0)\n","            combined_pe += random_pes.sum(axis=0)\n","\n","            token_pe[i] = torch.tensor(combined_pe)\n","\n","    return token_pe\n","\n"],"metadata":{"id":"_Ndd99hrDjdB","executionInfo":{"status":"ok","timestamp":1756491527381,"user_tz":-120,"elapsed":93,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["token_pes_list = []\n","\n","for smiles, graph_pe in zip(smiles_list, graph_pes_list):\n","    # Align graph PE to tokens using the provided function\n","    token_pe = align_graph_pe_to_tokens(\n","        smiles=smiles,\n","        graph_pe=graph_pe,\n","        tokenizer=tokenizer,\n","        max_length=max_length,\n","        random_seed=42\n","    )\n","    token_pes_list.append(token_pe)\n","\n","# Stack into batch tensor\n","token_pes_batch = torch.stack(token_pes_list)"],"metadata":{"id":"Ahf_qjEtXHz3","executionInfo":{"status":"ok","timestamp":1756491527577,"user_tz":-120,"elapsed":194,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class EmbeddingWithGraphPE(nn.Module):\n","    def __init__(self, embed_dim=512, pe_dim=30):\n","        super().__init__()\n","        # One-layer projection with GeLU from graph PE to embedding dimension\n","        # Using standard Laplacian, so pe_dim is just k\n","        self.pe_projection = nn.Sequential(\n","            nn.Linear(pe_dim, embed_dim),\n","            nn.GELU()\n","        )\n","\n","    def forward(self, embeddings, token_pes):\n","        # token_pes shape: [batch_size, seq_len, pe_dim]\n","        # embeddings shape: [batch_size, seq_len, embed_dim]\n","\n","        # Project token PEs to embedding dimension with GeLU\n","        projected_pes = self.pe_projection(token_pes)\n","\n","        # Add to token embeddings\n","        enhanced_embeddings = embeddings + projected_pes\n","\n","        return enhanced_embeddings\n","\n","# Usage\n","model = EmbeddingWithGraphPE(embed_dim=512, pe_dim=30)\n","# graph_pes_batch shape: [40, 61, 30] (30 eigenvectors from standard Laplacian)\n","# embeddings shape: [40, 61, 512]\n","enhanced_embeddings = model(embeddings, token_pes_batch)\n","# Output shape: [40, 61, 512]"],"metadata":{"id":"i0zuJ4bRvlJT","executionInfo":{"status":"ok","timestamp":1756491527675,"user_tz":-120,"elapsed":88,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["enhanced_embeddings.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LYwFIBucX0hF","executionInfo":{"status":"ok","timestamp":1756491527691,"user_tz":-120,"elapsed":7,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"outputId":"80d01024-d3c6-4466-f4e1-617c54586408"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([40, 61, 512])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["import json\n","from data_import import check_class_balance, stratified_train_val_split, convert_to_litgpt_format, save_to_jsonl\n","\n","# Create dataset\n","batch_size = 1000  # Adjust based on your needs\n","dataset = PartialHIVDataset(max_samples=batch_size)\n","\n","# Convert to DataFrame\n","full_batch_data = list(dataset)\n","df = pd.DataFrame(full_batch_data)\n","\n","\n","# Check balance in the original DataFrame\n","original_balance = check_class_balance(df, \"Original Dataset\")\n","\n","# Convert to LitGPT format for different task types\n","classification_data = convert_to_litgpt_format(df, task_type=\"classification\")\n","\n","# Check balance in the converted data\n","converted_balance = check_class_balance(classification_data, \"Converted Dataset\")\n","\n","# Create stratified train/validation split\n","train_data, val_data = stratified_train_val_split(classification_data, train_ratio=0.8)\n","\n","# Check balance in train and validation sets\n","train_balance = check_class_balance(train_data, \"Training Set\")\n","val_balance = check_class_balance(val_data, \"Validation Set\")\n","\n","# Calculate difference from original distribution\n","train_diff_0 = abs(train_balance['ratio_0'] - original_balance['ratio_0'])\n","train_diff_1 = abs(train_balance['ratio_1'] - original_balance['ratio_1'])\n","val_diff_0 = abs(val_balance['ratio_0'] - original_balance['ratio_0'])\n","val_diff_1 = abs(val_balance['ratio_1'] - original_balance['ratio_1'])\n","\n","\n","# Save to JSONL files\n","save_to_jsonl(train_data, \"hiv_train.jsonl\")\n","save_to_jsonl(val_data, \"hiv_val.jsonl\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0i30OS9CSDBm","executionInfo":{"status":"ok","timestamp":1756491529809,"user_tz":-120,"elapsed":2116,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"outputId":"2ac90faa-8adf-4cad-e037-4823161007b3"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing HIV dataset (will only load 1000 samples)...\n","Dataset ready! Total size: 41120 molecules\n","But we'll only load 1000 of them.\n","\n","\n","Original Dataset Class Balance:\n","Total samples: 1000\n","Class 0.0: 971 samples (97.10%)\n","Class 1.0: 29 samples (2.90%)\n","\n","Converted Dataset Class Balance:\n","Total samples: 1000\n","Class 0: 971 samples (97.10%)\n","Class 1: 29 samples (2.90%)\n","\n","Training Set Class Balance:\n","Total samples: 799\n","Class 0: 776 samples (97.12%)\n","Class 1: 23 samples (2.88%)\n","\n","Validation Set Class Balance:\n","Total samples: 201\n","Class 0: 195 samples (97.01%)\n","Class 1: 6 samples (2.99%)\n","Saved 799 entries to hiv_train.jsonl\n","Saved 201 entries to hiv_val.jsonl\n"]}]},{"cell_type":"code","source":["import json\n","import torch\n","from utils import calculate_max_lengths\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer\n","\n","def load_jsonl(filepath):\n","    \"\"\"Load data from JSONL file\"\"\"\n","    data = []\n","    with open(filepath, 'r') as f:\n","        for line in f:\n","            data.append(json.loads(line))\n","    return data\n","\n","class HIVDataset(Dataset):\n","    def __init__(self, jsonl_filepath, tokenizer, max_smiles_length=100, max_full_length=256):\n","        self.data = load_jsonl(jsonl_filepath)\n","        self.tokenizer = tokenizer\n","        self.max_smiles_length = max_smiles_length\n","        self.max_full_length = max_full_length\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        item = self.data[idx]\n","\n","        # Extract SMILES\n","        smiles = item['input'].replace(\"SMILES: \", \"\")\n","\n","        # Create full prompt\n","        full_prompt = f\"{item['instruction']}\\n{item['input']}\\nAnswer:\"\n","        # Compute graph PE with actual SMILES length\n","        smiles_tokens = self.tokenizer(smiles, add_special_tokens=False)\n","        actual_smiles_length = len(smiles_tokens['input_ids'])\n","        # Get prompt length for label masking\n","        prompt_encoding = self.tokenizer(full_prompt, add_special_tokens=True, return_tensors='pt')\n","        prompt_length = prompt_encoding['input_ids'].shape[1]\n","\n","        # Full sequence for training\n","        target_text = f\"{full_prompt} {item['output']}\"\n","        full_encoding = self.tokenizer(\n","            target_text,\n","            padding='max_length',\n","            max_length=self.max_full_length,\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","\n","        # Compute graph PE for SMILES only\n","        adj_matrix = smiles_to_adjacency_matrix(smiles)\n","        if adj_matrix is not None:\n","            graph_pe = compute_graph_positional_encoding(adj_matrix, k=30)\n","            smiles_token_pe = align_graph_pe_to_tokens(\n","                smiles, graph_pe, self.tokenizer, actual_smiles_length, random_seed=42\n","            )\n","        else:\n","            smiles_token_pe = torch.zeros(self.max_smiles_length, 30)\n","\n","        # Create full-length PE tensor\n","        full_token_pe = torch.zeros(self.max_full_length, 30)\n","\n","        # Find where SMILES tokens are and place PEs there\n","        # This is approximate - you might need better alignment\n","        instruction_text = f\"{item['instruction']}\\nSMILES: \"\n","        instruction_tokens = self.tokenizer(instruction_text, add_special_tokens=True)\n","        smiles_start_idx = len(instruction_tokens['input_ids'])\n","\n","        # Copy SMILES PEs to correct position\n","        pe_end_idx = min(smiles_start_idx + actual_smiles_length, self.max_full_length)\n","\n","        full_token_pe[smiles_start_idx:pe_end_idx] = smiles_token_pe\n","\n","        # Create labels with prompt masked\n","        labels = full_encoding['input_ids'][0].clone()\n","        labels[:prompt_length] = -100  # Mask prompt tokens\n","\n","        return {\n","            'input_ids': full_encoding['input_ids'][0],\n","            'graph_pes': full_token_pe,\n","            'labels': labels,\n","        }\n","\n","# First calculate max lengths\n","max_smiles_length, max_full_length = calculate_max_lengths('hiv_train.jsonl')\n","\n","# Then create dataloaders with those lengths\n","def create_dataloaders(train_path='hiv_train.jsonl', val_path='hiv_val.jsonl',\n","                      batch_size=8, tokenizer=None, max_smiles_length=max_smiles_length, max_full_length=max_full_length):\n","    if tokenizer is None:\n","        tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-160m\")\n","        if tokenizer.pad_token is None:\n","            tokenizer.pad_token = tokenizer.eos_token\n","\n","    train_dataset = HIVDataset(train_path, tokenizer, max_smiles_length, max_full_length)\n","    val_dataset = HIVDataset(val_path, tokenizer, max_smiles_length, max_full_length)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","    return train_loader, val_loader, tokenizer\n","\n","# Usage\n","train_loader, val_loader, tokenizer = create_dataloaders(batch_size=8)\n","\n","# Test one batch\n","sample_batch = next(iter(train_loader))\n","print(f\"Input IDs shape: {sample_batch['input_ids'].shape}\")\n","print(f\"Graph PEs shape: {sample_batch['graph_pes'].shape}\")\n","print(f\"Labels shape: {sample_batch['labels'].shape}\")"],"metadata":{"id":"41yISTqnPEyP","executionInfo":{"status":"ok","timestamp":1756495963268,"user_tz":-120,"elapsed":769,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a1bb4145-7fd7-4bed-d6cf-2ab70b0dafe3"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Recommended max_smiles_length: 130\n","Recommended max_length: 171\n","Input IDs shape: torch.Size([8, 171])\n","Graph PEs shape: torch.Size([8, 171, 30])\n","Labels shape: torch.Size([8, 171])\n"]}]},{"cell_type":"code","source":["# Look at first example in batch\n","i = 0\n","print(f\"Input text (decoded): {tokenizer.decode(sample_batch['input_ids'][i])}\")\n","print(f\"\\nNon-zero PE positions: {torch.nonzero(sample_batch['graph_pes'][i].sum(dim=1)).squeeze().tolist()}\")\n","print(f\"\\nLabel tokens that aren't -100: {sample_batch['labels'][i][sample_batch['labels'][i] != -100].tolist()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CycDQXtCRHqC","executionInfo":{"status":"ok","timestamp":1756495967276,"user_tz":-120,"elapsed":28,"user":{"displayName":"Tommaso Pappagallo","userId":"11586722758477823355"}},"outputId":"de07f8f6-0260-4043-826d-30c777a3c4ae"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Input text (decoded): Classify the following molecule based on its HIV activity. Respond with '1' if the molecule shows HIV activity, or '0' if it does not.\n","SMILES: CC(C)(C)CC1=CC(=C2C=C(CC(C)(C)C)C(=O)C(CC(C)(C)C)=C2)C=C(CC(C)(C)C)C1=O\n","Answer: 0<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n","\n","Non-zero PE positions: [38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97]\n","\n","Label tokens that aren't -100: [470, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"]}]}]}